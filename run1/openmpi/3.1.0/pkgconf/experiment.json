[
    {
        "binaries": {
            "spliced": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps"
            ],
            "original": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-top"
            ]
        },
        "predictions": {
            "spack-test": [
                {
                    "message": "==> Spack test 3hutjcsrefi232cdtlufvumx3qkhe4rc\n==> Testing package openmpi-3.1.0-qcuulae\n==> Error: TestFailure: 18 tests failed.\n\n\nCommand exited with status 1:\n    './hello_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:33191] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     589    -------------------------------------------------------------------\n            -------\n     590    *** An error occurred in MPI_Init\n     591    *** on a NULL communicator\n     592    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     593    ***    and potentially your MPI job)\n     594    [borax27:33191] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 595    FAILED: Command exited with status 1:\n     596        './hello_c'\n     597    -------------------------------------------------------------------\n            -------\n     598    The application appears to have been direct launched using \"srun\",\n     599    but OMPI was not built with SLURM support. This usually happens\n     600    when OMPI was not configured --with-slurm and we weren't able\n     601    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:33192] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     662    -------------------------------------------------------------------\n            -------\n     663    *** An error occurred in MPI_Init\n     664    *** on a NULL communicator\n     665    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     666    ***    and potentially your MPI job)\n     667    [borax27:33192] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 668    FAILED: Command exited with status 1:\n     669        './hello_mpifh'\n     670    -------------------------------------------------------------------\n            -------\n     671    The application appears to have been direct launched using \"srun\",\n     672    but OMPI was not built with SLURM support. This usually happens\n     673    when OMPI was not configured --with-slurm and we weren't able\n     674    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 33270, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        33270\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     741    of its peer processes in the job will be killed properly.  You shou\n            ld\n     742    double check that everything has shut down cleanly.\n     743    \n     744    Local host: borax27\n     745    PID:        33270\n     746    -------------------------------------------------------------------\n            -------\n  >> 747    FAILED: Command exited with status 255:\n     748        './hello_oshmem'\n     749    -------------------------------------------------------------------\n            -------\n     750    The application appears to have been direct launched using \"srun\",\n     751    but OMPI was not built with SLURM support. This usually happens\n     752    when OMPI was not configured --with-slurm and we weren't able\n     753    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemcxx'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 33346, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        33346\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     826    of its peer processes in the job will be killed properly.  You shou\n            ld\n     827    double check that everything has shut down cleanly.\n     828    \n     829    Local host: borax27\n     830    PID:        33346\n     831    -------------------------------------------------------------------\n            -------\n  >> 832    FAILED: Command exited with status 255:\n     833        './hello_oshmemcxx'\n     834    -------------------------------------------------------------------\n            -------\n     835    The application appears to have been direct launched using \"srun\",\n     836    but OMPI was not built with SLURM support. This usually happens\n     837    when OMPI was not configured --with-slurm and we weren't able\n     838    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 33422, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        33422\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     911    of its peer processes in the job will be killed properly.  You shou\n            ld\n     912    double check that everything has shut down cleanly.\n     913    \n     914    Local host: borax27\n     915    PID:        33422\n     916    -------------------------------------------------------------------\n            -------\n  >> 917    FAILED: Command exited with status 255:\n     918        './hello_oshmemfh'\n     919    -------------------------------------------------------------------\n            -------\n     920    The application appears to have been direct launched using \"srun\",\n     921    but OMPI was not built with SLURM support. This usually happens\n     922    when OMPI was not configured --with-slurm and we weren't able\n     923    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:33498] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     990    -------------------------------------------------------------------\n            -------\n     991    *** An error occurred in MPI_Init\n     992    *** on a NULL communicator\n     993    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     994    ***    and potentially your MPI job)\n     995    [borax27:33498] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 996    FAILED: Command exited with status 1:\n     997        './hello_usempi'\n     998    -------------------------------------------------------------------\n            -------\n     999    The application appears to have been direct launched using \"srun\",\n     1000   but OMPI was not built with SLURM support. This usually happens\n     1001   when OMPI was not configured --with-slurm and we weren't able\n     1002   to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:33575] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1063    ------------------------------------------------------------------\n             --------\n     1064    *** An error occurred in MPI_Init\n     1065    *** on a NULL communicator\n     1066    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1067    ***    and potentially your MPI job)\n     1068    [borax27:33575] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1069    FAILED: Command exited with status 1:\n     1070        './hello_usempif08'\n     1071    ------------------------------------------------------------------\n             --------\n     1072    The application appears to have been direct launched using \"srun\",\n     1073    but OMPI was not built with SLURM support. This usually happens\n     1074    when OMPI was not configured --with-slurm and we weren't able\n     1075    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_circular_shift'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 33651, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        33651\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1142    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1143    double check that everything has shut down cleanly.\n     1144    \n     1145    Local host: borax27\n     1146    PID:        33651\n     1147    ------------------------------------------------------------------\n             --------\n  >> 1148    FAILED: Command exited with status 255:\n     1149        './oshmem_circular_shift'\n     1150    ------------------------------------------------------------------\n             --------\n     1151    The application appears to have been direct launched using \"srun\",\n     1152    but OMPI was not built with SLURM support. This usually happens\n     1153    when OMPI was not configured --with-slurm and we weren't able\n     1154    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_max_reduction'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 33727, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        33727\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1227    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1228    double check that everything has shut down cleanly.\n     1229    \n     1230    Local host: borax27\n     1231    PID:        33727\n     1232    ------------------------------------------------------------------\n             --------\n  >> 1233    FAILED: Command exited with status 255:\n     1234        './oshmem_max_reduction'\n     1235    ------------------------------------------------------------------\n             --------\n     1236    The application appears to have been direct launched using \"srun\",\n     1237    but OMPI was not built with SLURM support. This usually happens\n     1238    when OMPI was not configured --with-slurm and we weren't able\n     1239    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_shmalloc'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 33803, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        33803\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1312    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1313    double check that everything has shut down cleanly.\n     1314    \n     1315    Local host: borax27\n     1316    PID:        33803\n     1317    ------------------------------------------------------------------\n             --------\n  >> 1318    FAILED: Command exited with status 255:\n     1319        './oshmem_shmalloc'\n     1320    ------------------------------------------------------------------\n             --------\n     1321    The application appears to have been direct launched using \"srun\",\n     1322    but OMPI was not built with SLURM support. This usually happens\n     1323    when OMPI was not configured --with-slurm and we weren't able\n     1324    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_strided_puts'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 33880, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        33880\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1397    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1398    double check that everything has shut down cleanly.\n     1399    \n     1400    Local host: borax27\n     1401    PID:        33880\n     1402    ------------------------------------------------------------------\n             --------\n  >> 1403    FAILED: Command exited with status 255:\n     1404        './oshmem_strided_puts'\n     1405    ------------------------------------------------------------------\n             --------\n     1406    The application appears to have been direct launched using \"srun\",\n     1407    but OMPI was not built with SLURM support. This usually happens\n     1408    when OMPI was not configured --with-slurm and we weren't able\n     1409    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_symmetric_data'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 33956, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        33956\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1482    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1483    double check that everything has shut down cleanly.\n     1484    \n     1485    Local host: borax27\n     1486    PID:        33956\n     1487    ------------------------------------------------------------------\n             --------\n  >> 1488    FAILED: Command exited with status 255:\n     1489        './oshmem_symmetric_data'\n     1490    ------------------------------------------------------------------\n             --------\n     1491    The application appears to have been direct launched using \"srun\",\n     1492    but OMPI was not built with SLURM support. This usually happens\n     1493    when OMPI was not configured --with-slurm and we weren't able\n     1494    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:34032] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1561    ------------------------------------------------------------------\n             --------\n     1562    *** An error occurred in MPI_Init\n     1563    *** on a NULL communicator\n     1564    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1565    ***    and potentially your MPI job)\n     1566    [borax27:34032] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1567    FAILED: Command exited with status 1:\n     1568        './ring_c'\n     1569    ------------------------------------------------------------------\n             --------\n     1570    The application appears to have been direct launched using \"srun\",\n     1571    but OMPI was not built with SLURM support. This usually happens\n     1572    when OMPI was not configured --with-slurm and we weren't able\n     1573    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:34108] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1634    ------------------------------------------------------------------\n             --------\n     1635    *** An error occurred in MPI_Init\n     1636    *** on a NULL communicator\n     1637    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1638    ***    and potentially your MPI job)\n     1639    [borax27:34108] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1640    FAILED: Command exited with status 1:\n     1641        './ring_mpifh'\n     1642    ------------------------------------------------------------------\n             --------\n     1643    The application appears to have been direct launched using \"srun\",\n     1644    but OMPI was not built with SLURM support. This usually happens\n     1645    when OMPI was not configured --with-slurm and we weren't able\n     1646    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 34184, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        34184\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1713    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1714    double check that everything has shut down cleanly.\n     1715    \n     1716    Local host: borax27\n     1717    PID:        34184\n     1718    ------------------------------------------------------------------\n             --------\n  >> 1719    FAILED: Command exited with status 255:\n     1720        './ring_oshmem'\n     1721    ------------------------------------------------------------------\n             --------\n     1722    The application appears to have been direct launched using \"srun\",\n     1723    but OMPI was not built with SLURM support. This usually happens\n     1724    when OMPI was not configured --with-slurm and we weren't able\n     1725    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 34261, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        34261\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1798    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1799    double check that everything has shut down cleanly.\n     1800    \n     1801    Local host: borax27\n     1802    PID:        34261\n     1803    ------------------------------------------------------------------\n             --------\n  >> 1804    FAILED: Command exited with status 255:\n     1805        './ring_oshmemfh'\n     1806    ------------------------------------------------------------------\n             --------\n     1807    The application appears to have been direct launched using \"srun\",\n     1808    but OMPI was not built with SLURM support. This usually happens\n     1809    when OMPI was not configured --with-slurm and we weren't able\n     1810    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:34337] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1877    ------------------------------------------------------------------\n             --------\n     1878    *** An error occurred in MPI_Init\n     1879    *** on a NULL communicator\n     1880    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1881    ***    and potentially your MPI job)\n     1882    [borax27:34337] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1883    FAILED: Command exited with status 1:\n     1884        './ring_usempi'\n     1885    ------------------------------------------------------------------\n             --------\n     1886    The application appears to have been direct launched using \"srun\",\n     1887    but OMPI was not built with SLURM support. This usually happens\n     1888    when OMPI was not configured --with-slurm and we weren't able\n     1889    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:34413] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1950    ------------------------------------------------------------------\n             --------\n     1951    *** An error occurred in MPI_Init\n     1952    *** on a NULL communicator\n     1953    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1954    ***    and potentially your MPI job)\n     1955    [borax27:34413] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1956    FAILED: Command exited with status 1:\n     1957        './ring_usempif08'\n     1958    ------------------------------------------------------------------\n             --------\n     1959    The application appears to have been direct launched using \"srun\",\n     1960    but OMPI was not built with SLURM support. This usually happens\n     1961    when OMPI was not configured --with-slurm and we weren't able\n     1962    to discover a SLURM installation in the usual places.\n\n\n\n/p/vast1/build/spack/lib/spack/spack/build_environment.py:1061, in _setup_pkg_and_run:\n       1058        tb_string = traceback.format_exc()\n       1059\n       1060        # build up some context from the offending package so we can\n  >>   1061        # show that, too.\n       1062        package_context = get_package_context(tb)\n       1063\n       1064        logfile = None\n\nSee test log for details:\n  /g/g0/sochat1/.spack/test/3hutjcsrefi232cdtlufvumx3qkhe4rc/openmpi-3.1.0-qcuulae-test-out.txt\n\n==> Error: 1 test(s) in the suite failed.\n\n======================== 1 failed, 0 passed of 1 specs =========================\n",
                    "return_code": 1,
                    "prediction": false,
                    "command": "/p/vast1/build/spack/bin/spack test run /qcuula"
                }
            ],
            "symbolator": [
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ],
            "libabigail": [
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ]
        },
        "libs": {
            "spliced": [
                {
                    "dep": "pkgconf@1.8.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ],
            "original": [
                {
                    "dep": "pkgconf@1.8.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ]
        },
        "experiment": "experiment",
        "result": "splice-success",
        "success": true,
        "splice": "pkgconf@1.8.0",
        "package": "openmpi@3.1.0"
    },
    {
        "binaries": {
            "spliced": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps"
            ],
            "original": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-top"
            ]
        },
        "predictions": {
            "spack-test": [
                {
                    "message": "==> Spack test 3hutjcsrefi232cdtlufvumx3qkhe4rc\n==> Testing package openmpi-3.1.0-qcuulae\n==> Error: TestFailure: 18 tests failed.\n\n\nCommand exited with status 1:\n    './hello_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:34794] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     592    -------------------------------------------------------------------\n            -------\n     593    *** An error occurred in MPI_Init\n     594    *** on a NULL communicator\n     595    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     596    ***    and potentially your MPI job)\n     597    [borax27:34794] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 598    FAILED: Command exited with status 1:\n     599        './hello_c'\n     600    -------------------------------------------------------------------\n            -------\n     601    The application appears to have been direct launched using \"srun\",\n     602    but OMPI was not built with SLURM support. This usually happens\n     603    when OMPI was not configured --with-slurm and we weren't able\n     604    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:34795] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     665    -------------------------------------------------------------------\n            -------\n     666    *** An error occurred in MPI_Init\n     667    *** on a NULL communicator\n     668    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     669    ***    and potentially your MPI job)\n     670    [borax27:34795] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 671    FAILED: Command exited with status 1:\n     672        './hello_mpifh'\n     673    -------------------------------------------------------------------\n            -------\n     674    The application appears to have been direct launched using \"srun\",\n     675    but OMPI was not built with SLURM support. This usually happens\n     676    when OMPI was not configured --with-slurm and we weren't able\n     677    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 34872, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        34872\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     744    of its peer processes in the job will be killed properly.  You shou\n            ld\n     745    double check that everything has shut down cleanly.\n     746    \n     747    Local host: borax27\n     748    PID:        34872\n     749    -------------------------------------------------------------------\n            -------\n  >> 750    FAILED: Command exited with status 255:\n     751        './hello_oshmem'\n     752    -------------------------------------------------------------------\n            -------\n     753    The application appears to have been direct launched using \"srun\",\n     754    but OMPI was not built with SLURM support. This usually happens\n     755    when OMPI was not configured --with-slurm and we weren't able\n     756    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemcxx'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 34948, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        34948\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     829    of its peer processes in the job will be killed properly.  You shou\n            ld\n     830    double check that everything has shut down cleanly.\n     831    \n     832    Local host: borax27\n     833    PID:        34948\n     834    -------------------------------------------------------------------\n            -------\n  >> 835    FAILED: Command exited with status 255:\n     836        './hello_oshmemcxx'\n     837    -------------------------------------------------------------------\n            -------\n     838    The application appears to have been direct launched using \"srun\",\n     839    but OMPI was not built with SLURM support. This usually happens\n     840    when OMPI was not configured --with-slurm and we weren't able\n     841    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 35025, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        35025\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     914    of its peer processes in the job will be killed properly.  You shou\n            ld\n     915    double check that everything has shut down cleanly.\n     916    \n     917    Local host: borax27\n     918    PID:        35025\n     919    -------------------------------------------------------------------\n            -------\n  >> 920    FAILED: Command exited with status 255:\n     921        './hello_oshmemfh'\n     922    -------------------------------------------------------------------\n            -------\n     923    The application appears to have been direct launched using \"srun\",\n     924    but OMPI was not built with SLURM support. This usually happens\n     925    when OMPI was not configured --with-slurm and we weren't able\n     926    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:35101] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     993    -------------------------------------------------------------------\n            -------\n     994    *** An error occurred in MPI_Init\n     995    *** on a NULL communicator\n     996    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     997    ***    and potentially your MPI job)\n     998    [borax27:35101] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 999    FAILED: Command exited with status 1:\n     1000       './hello_usempi'\n     1001   -------------------------------------------------------------------\n            -------\n     1002   The application appears to have been direct launched using \"srun\",\n     1003   but OMPI was not built with SLURM support. This usually happens\n     1004   when OMPI was not configured --with-slurm and we weren't able\n     1005   to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:35177] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1066    ------------------------------------------------------------------\n             --------\n     1067    *** An error occurred in MPI_Init\n     1068    *** on a NULL communicator\n     1069    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1070    ***    and potentially your MPI job)\n     1071    [borax27:35177] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1072    FAILED: Command exited with status 1:\n     1073        './hello_usempif08'\n     1074    ------------------------------------------------------------------\n             --------\n     1075    The application appears to have been direct launched using \"srun\",\n     1076    but OMPI was not built with SLURM support. This usually happens\n     1077    when OMPI was not configured --with-slurm and we weren't able\n     1078    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_circular_shift'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 35253, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        35253\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1145    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1146    double check that everything has shut down cleanly.\n     1147    \n     1148    Local host: borax27\n     1149    PID:        35253\n     1150    ------------------------------------------------------------------\n             --------\n  >> 1151    FAILED: Command exited with status 255:\n     1152        './oshmem_circular_shift'\n     1153    ------------------------------------------------------------------\n             --------\n     1154    The application appears to have been direct launched using \"srun\",\n     1155    but OMPI was not built with SLURM support. This usually happens\n     1156    when OMPI was not configured --with-slurm and we weren't able\n     1157    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_max_reduction'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 35330, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        35330\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1230    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1231    double check that everything has shut down cleanly.\n     1232    \n     1233    Local host: borax27\n     1234    PID:        35330\n     1235    ------------------------------------------------------------------\n             --------\n  >> 1236    FAILED: Command exited with status 255:\n     1237        './oshmem_max_reduction'\n     1238    ------------------------------------------------------------------\n             --------\n     1239    The application appears to have been direct launched using \"srun\",\n     1240    but OMPI was not built with SLURM support. This usually happens\n     1241    when OMPI was not configured --with-slurm and we weren't able\n     1242    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_shmalloc'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 35406, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        35406\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1315    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1316    double check that everything has shut down cleanly.\n     1317    \n     1318    Local host: borax27\n     1319    PID:        35406\n     1320    ------------------------------------------------------------------\n             --------\n  >> 1321    FAILED: Command exited with status 255:\n     1322        './oshmem_shmalloc'\n     1323    ------------------------------------------------------------------\n             --------\n     1324    The application appears to have been direct launched using \"srun\",\n     1325    but OMPI was not built with SLURM support. This usually happens\n     1326    when OMPI was not configured --with-slurm and we weren't able\n     1327    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_strided_puts'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 35482, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        35482\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1400    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1401    double check that everything has shut down cleanly.\n     1402    \n     1403    Local host: borax27\n     1404    PID:        35482\n     1405    ------------------------------------------------------------------\n             --------\n  >> 1406    FAILED: Command exited with status 255:\n     1407        './oshmem_strided_puts'\n     1408    ------------------------------------------------------------------\n             --------\n     1409    The application appears to have been direct launched using \"srun\",\n     1410    but OMPI was not built with SLURM support. This usually happens\n     1411    when OMPI was not configured --with-slurm and we weren't able\n     1412    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_symmetric_data'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 35558, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        35558\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1485    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1486    double check that everything has shut down cleanly.\n     1487    \n     1488    Local host: borax27\n     1489    PID:        35558\n     1490    ------------------------------------------------------------------\n             --------\n  >> 1491    FAILED: Command exited with status 255:\n     1492        './oshmem_symmetric_data'\n     1493    ------------------------------------------------------------------\n             --------\n     1494    The application appears to have been direct launched using \"srun\",\n     1495    but OMPI was not built with SLURM support. This usually happens\n     1496    when OMPI was not configured --with-slurm and we weren't able\n     1497    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:35635] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1564    ------------------------------------------------------------------\n             --------\n     1565    *** An error occurred in MPI_Init\n     1566    *** on a NULL communicator\n     1567    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1568    ***    and potentially your MPI job)\n     1569    [borax27:35635] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1570    FAILED: Command exited with status 1:\n     1571        './ring_c'\n     1572    ------------------------------------------------------------------\n             --------\n     1573    The application appears to have been direct launched using \"srun\",\n     1574    but OMPI was not built with SLURM support. This usually happens\n     1575    when OMPI was not configured --with-slurm and we weren't able\n     1576    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:35711] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1637    ------------------------------------------------------------------\n             --------\n     1638    *** An error occurred in MPI_Init\n     1639    *** on a NULL communicator\n     1640    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1641    ***    and potentially your MPI job)\n     1642    [borax27:35711] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1643    FAILED: Command exited with status 1:\n     1644        './ring_mpifh'\n     1645    ------------------------------------------------------------------\n             --------\n     1646    The application appears to have been direct launched using \"srun\",\n     1647    but OMPI was not built with SLURM support. This usually happens\n     1648    when OMPI was not configured --with-slurm and we weren't able\n     1649    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 35787, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        35787\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1716    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1717    double check that everything has shut down cleanly.\n     1718    \n     1719    Local host: borax27\n     1720    PID:        35787\n     1721    ------------------------------------------------------------------\n             --------\n  >> 1722    FAILED: Command exited with status 255:\n     1723        './ring_oshmem'\n     1724    ------------------------------------------------------------------\n             --------\n     1725    The application appears to have been direct launched using \"srun\",\n     1726    but OMPI was not built with SLURM support. This usually happens\n     1727    when OMPI was not configured --with-slurm and we weren't able\n     1728    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 35863, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        35863\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1801    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1802    double check that everything has shut down cleanly.\n     1803    \n     1804    Local host: borax27\n     1805    PID:        35863\n     1806    ------------------------------------------------------------------\n             --------\n  >> 1807    FAILED: Command exited with status 255:\n     1808        './ring_oshmemfh'\n     1809    ------------------------------------------------------------------\n             --------\n     1810    The application appears to have been direct launched using \"srun\",\n     1811    but OMPI was not built with SLURM support. This usually happens\n     1812    when OMPI was not configured --with-slurm and we weren't able\n     1813    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:35940] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1880    ------------------------------------------------------------------\n             --------\n     1881    *** An error occurred in MPI_Init\n     1882    *** on a NULL communicator\n     1883    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1884    ***    and potentially your MPI job)\n     1885    [borax27:35940] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1886    FAILED: Command exited with status 1:\n     1887        './ring_usempi'\n     1888    ------------------------------------------------------------------\n             --------\n     1889    The application appears to have been direct launched using \"srun\",\n     1890    but OMPI was not built with SLURM support. This usually happens\n     1891    when OMPI was not configured --with-slurm and we weren't able\n     1892    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:36016] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1953    ------------------------------------------------------------------\n             --------\n     1954    *** An error occurred in MPI_Init\n     1955    *** on a NULL communicator\n     1956    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1957    ***    and potentially your MPI job)\n     1958    [borax27:36016] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1959    FAILED: Command exited with status 1:\n     1960        './ring_usempif08'\n     1961    ------------------------------------------------------------------\n             --------\n     1962    The application appears to have been direct launched using \"srun\",\n     1963    but OMPI was not built with SLURM support. This usually happens\n     1964    when OMPI was not configured --with-slurm and we weren't able\n     1965    to discover a SLURM installation in the usual places.\n\n\n\n/p/vast1/build/spack/lib/spack/spack/build_environment.py:1061, in _setup_pkg_and_run:\n       1058        tb_string = traceback.format_exc()\n       1059\n       1060        # build up some context from the offending package so we can\n  >>   1061        # show that, too.\n       1062        package_context = get_package_context(tb)\n       1063\n       1064        logfile = None\n\nSee test log for details:\n  /g/g0/sochat1/.spack/test/3hutjcsrefi232cdtlufvumx3qkhe4rc/openmpi-3.1.0-qcuulae-test-out.txt\n\n==> Error: 1 test(s) in the suite failed.\n\n======================== 1 failed, 0 passed of 1 specs =========================\n",
                    "return_code": 1,
                    "prediction": false,
                    "command": "/p/vast1/build/spack/bin/spack test run /qcuula"
                }
            ],
            "symbolator": [
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ],
            "libabigail": [
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ]
        },
        "libs": {
            "spliced": [
                {
                    "dep": "pkgconf@1.7.4%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ],
            "original": [
                {
                    "dep": "pkgconf@1.8.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ]
        },
        "experiment": "experiment",
        "result": "splice-success",
        "success": true,
        "splice": "pkgconf@1.7.4",
        "package": "openmpi@3.1.0"
    },
    {
        "binaries": {
            "spliced": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps"
            ],
            "original": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-top"
            ]
        },
        "predictions": {
            "spack-test": [
                {
                    "message": "==> Spack test 3hutjcsrefi232cdtlufvumx3qkhe4rc\n==> Testing package openmpi-3.1.0-qcuulae\n==> Error: TestFailure: 18 tests failed.\n\n\nCommand exited with status 1:\n    './hello_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:36397] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     592    -------------------------------------------------------------------\n            -------\n     593    *** An error occurred in MPI_Init\n     594    *** on a NULL communicator\n     595    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     596    ***    and potentially your MPI job)\n     597    [borax27:36397] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 598    FAILED: Command exited with status 1:\n     599        './hello_c'\n     600    -------------------------------------------------------------------\n            -------\n     601    The application appears to have been direct launched using \"srun\",\n     602    but OMPI was not built with SLURM support. This usually happens\n     603    when OMPI was not configured --with-slurm and we weren't able\n     604    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:36398] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     665    -------------------------------------------------------------------\n            -------\n     666    *** An error occurred in MPI_Init\n     667    *** on a NULL communicator\n     668    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     669    ***    and potentially your MPI job)\n     670    [borax27:36398] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 671    FAILED: Command exited with status 1:\n     672        './hello_mpifh'\n     673    -------------------------------------------------------------------\n            -------\n     674    The application appears to have been direct launched using \"srun\",\n     675    but OMPI was not built with SLURM support. This usually happens\n     676    when OMPI was not configured --with-slurm and we weren't able\n     677    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 36399, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        36399\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     744    of its peer processes in the job will be killed properly.  You shou\n            ld\n     745    double check that everything has shut down cleanly.\n     746    \n     747    Local host: borax27\n     748    PID:        36399\n     749    -------------------------------------------------------------------\n            -------\n  >> 750    FAILED: Command exited with status 255:\n     751        './hello_oshmem'\n     752    -------------------------------------------------------------------\n            -------\n     753    The application appears to have been direct launched using \"srun\",\n     754    but OMPI was not built with SLURM support. This usually happens\n     755    when OMPI was not configured --with-slurm and we weren't able\n     756    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemcxx'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 36475, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        36475\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     829    of its peer processes in the job will be killed properly.  You shou\n            ld\n     830    double check that everything has shut down cleanly.\n     831    \n     832    Local host: borax27\n     833    PID:        36475\n     834    -------------------------------------------------------------------\n            -------\n  >> 835    FAILED: Command exited with status 255:\n     836        './hello_oshmemcxx'\n     837    -------------------------------------------------------------------\n            -------\n     838    The application appears to have been direct launched using \"srun\",\n     839    but OMPI was not built with SLURM support. This usually happens\n     840    when OMPI was not configured --with-slurm and we weren't able\n     841    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 36551, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        36551\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     914    of its peer processes in the job will be killed properly.  You shou\n            ld\n     915    double check that everything has shut down cleanly.\n     916    \n     917    Local host: borax27\n     918    PID:        36551\n     919    -------------------------------------------------------------------\n            -------\n  >> 920    FAILED: Command exited with status 255:\n     921        './hello_oshmemfh'\n     922    -------------------------------------------------------------------\n            -------\n     923    The application appears to have been direct launched using \"srun\",\n     924    but OMPI was not built with SLURM support. This usually happens\n     925    when OMPI was not configured --with-slurm and we weren't able\n     926    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:36628] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     993    -------------------------------------------------------------------\n            -------\n     994    *** An error occurred in MPI_Init\n     995    *** on a NULL communicator\n     996    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     997    ***    and potentially your MPI job)\n     998    [borax27:36628] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 999    FAILED: Command exited with status 1:\n     1000       './hello_usempi'\n     1001   -------------------------------------------------------------------\n            -------\n     1002   The application appears to have been direct launched using \"srun\",\n     1003   but OMPI was not built with SLURM support. This usually happens\n     1004   when OMPI was not configured --with-slurm and we weren't able\n     1005   to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:36704] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1066    ------------------------------------------------------------------\n             --------\n     1067    *** An error occurred in MPI_Init\n     1068    *** on a NULL communicator\n     1069    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1070    ***    and potentially your MPI job)\n     1071    [borax27:36704] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1072    FAILED: Command exited with status 1:\n     1073        './hello_usempif08'\n     1074    ------------------------------------------------------------------\n             --------\n     1075    The application appears to have been direct launched using \"srun\",\n     1076    but OMPI was not built with SLURM support. This usually happens\n     1077    when OMPI was not configured --with-slurm and we weren't able\n     1078    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_circular_shift'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 36780, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        36780\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1145    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1146    double check that everything has shut down cleanly.\n     1147    \n     1148    Local host: borax27\n     1149    PID:        36780\n     1150    ------------------------------------------------------------------\n             --------\n  >> 1151    FAILED: Command exited with status 255:\n     1152        './oshmem_circular_shift'\n     1153    ------------------------------------------------------------------\n             --------\n     1154    The application appears to have been direct launched using \"srun\",\n     1155    but OMPI was not built with SLURM support. This usually happens\n     1156    when OMPI was not configured --with-slurm and we weren't able\n     1157    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_max_reduction'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 36858, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        36858\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1230    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1231    double check that everything has shut down cleanly.\n     1232    \n     1233    Local host: borax27\n     1234    PID:        36858\n     1235    ------------------------------------------------------------------\n             --------\n  >> 1236    FAILED: Command exited with status 255:\n     1237        './oshmem_max_reduction'\n     1238    ------------------------------------------------------------------\n             --------\n     1239    The application appears to have been direct launched using \"srun\",\n     1240    but OMPI was not built with SLURM support. This usually happens\n     1241    when OMPI was not configured --with-slurm and we weren't able\n     1242    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_shmalloc'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 36935, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        36935\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1315    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1316    double check that everything has shut down cleanly.\n     1317    \n     1318    Local host: borax27\n     1319    PID:        36935\n     1320    ------------------------------------------------------------------\n             --------\n  >> 1321    FAILED: Command exited with status 255:\n     1322        './oshmem_shmalloc'\n     1323    ------------------------------------------------------------------\n             --------\n     1324    The application appears to have been direct launched using \"srun\",\n     1325    but OMPI was not built with SLURM support. This usually happens\n     1326    when OMPI was not configured --with-slurm and we weren't able\n     1327    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_strided_puts'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 37011, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        37011\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1400    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1401    double check that everything has shut down cleanly.\n     1402    \n     1403    Local host: borax27\n     1404    PID:        37011\n     1405    ------------------------------------------------------------------\n             --------\n  >> 1406    FAILED: Command exited with status 255:\n     1407        './oshmem_strided_puts'\n     1408    ------------------------------------------------------------------\n             --------\n     1409    The application appears to have been direct launched using \"srun\",\n     1410    but OMPI was not built with SLURM support. This usually happens\n     1411    when OMPI was not configured --with-slurm and we weren't able\n     1412    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_symmetric_data'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 37087, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        37087\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1485    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1486    double check that everything has shut down cleanly.\n     1487    \n     1488    Local host: borax27\n     1489    PID:        37087\n     1490    ------------------------------------------------------------------\n             --------\n  >> 1491    FAILED: Command exited with status 255:\n     1492        './oshmem_symmetric_data'\n     1493    ------------------------------------------------------------------\n             --------\n     1494    The application appears to have been direct launched using \"srun\",\n     1495    but OMPI was not built with SLURM support. This usually happens\n     1496    when OMPI was not configured --with-slurm and we weren't able\n     1497    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:37163] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1564    ------------------------------------------------------------------\n             --------\n     1565    *** An error occurred in MPI_Init\n     1566    *** on a NULL communicator\n     1567    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1568    ***    and potentially your MPI job)\n     1569    [borax27:37163] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1570    FAILED: Command exited with status 1:\n     1571        './ring_c'\n     1572    ------------------------------------------------------------------\n             --------\n     1573    The application appears to have been direct launched using \"srun\",\n     1574    but OMPI was not built with SLURM support. This usually happens\n     1575    when OMPI was not configured --with-slurm and we weren't able\n     1576    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:37240] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1637    ------------------------------------------------------------------\n             --------\n     1638    *** An error occurred in MPI_Init\n     1639    *** on a NULL communicator\n     1640    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1641    ***    and potentially your MPI job)\n     1642    [borax27:37240] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1643    FAILED: Command exited with status 1:\n     1644        './ring_mpifh'\n     1645    ------------------------------------------------------------------\n             --------\n     1646    The application appears to have been direct launched using \"srun\",\n     1647    but OMPI was not built with SLURM support. This usually happens\n     1648    when OMPI was not configured --with-slurm and we weren't able\n     1649    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 37316, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        37316\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1716    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1717    double check that everything has shut down cleanly.\n     1718    \n     1719    Local host: borax27\n     1720    PID:        37316\n     1721    ------------------------------------------------------------------\n             --------\n  >> 1722    FAILED: Command exited with status 255:\n     1723        './ring_oshmem'\n     1724    ------------------------------------------------------------------\n             --------\n     1725    The application appears to have been direct launched using \"srun\",\n     1726    but OMPI was not built with SLURM support. This usually happens\n     1727    when OMPI was not configured --with-slurm and we weren't able\n     1728    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 37392, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        37392\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1801    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1802    double check that everything has shut down cleanly.\n     1803    \n     1804    Local host: borax27\n     1805    PID:        37392\n     1806    ------------------------------------------------------------------\n             --------\n  >> 1807    FAILED: Command exited with status 255:\n     1808        './ring_oshmemfh'\n     1809    ------------------------------------------------------------------\n             --------\n     1810    The application appears to have been direct launched using \"srun\",\n     1811    but OMPI was not built with SLURM support. This usually happens\n     1812    when OMPI was not configured --with-slurm and we weren't able\n     1813    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:37468] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1880    ------------------------------------------------------------------\n             --------\n     1881    *** An error occurred in MPI_Init\n     1882    *** on a NULL communicator\n     1883    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1884    ***    and potentially your MPI job)\n     1885    [borax27:37468] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1886    FAILED: Command exited with status 1:\n     1887        './ring_usempi'\n     1888    ------------------------------------------------------------------\n             --------\n     1889    The application appears to have been direct launched using \"srun\",\n     1890    but OMPI was not built with SLURM support. This usually happens\n     1891    when OMPI was not configured --with-slurm and we weren't able\n     1892    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:37545] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1953    ------------------------------------------------------------------\n             --------\n     1954    *** An error occurred in MPI_Init\n     1955    *** on a NULL communicator\n     1956    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1957    ***    and potentially your MPI job)\n     1958    [borax27:37545] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1959    FAILED: Command exited with status 1:\n     1960        './ring_usempif08'\n     1961    ------------------------------------------------------------------\n             --------\n     1962    The application appears to have been direct launched using \"srun\",\n     1963    but OMPI was not built with SLURM support. This usually happens\n     1964    when OMPI was not configured --with-slurm and we weren't able\n     1965    to discover a SLURM installation in the usual places.\n\n\n\n/p/vast1/build/spack/lib/spack/spack/build_environment.py:1061, in _setup_pkg_and_run:\n       1058        tb_string = traceback.format_exc()\n       1059\n       1060        # build up some context from the offending package so we can\n  >>   1061        # show that, too.\n       1062        package_context = get_package_context(tb)\n       1063\n       1064        logfile = None\n\nSee test log for details:\n  /g/g0/sochat1/.spack/test/3hutjcsrefi232cdtlufvumx3qkhe4rc/openmpi-3.1.0-qcuulae-test-out.txt\n\n==> Error: 1 test(s) in the suite failed.\n\n======================== 1 failed, 0 passed of 1 specs =========================\n",
                    "return_code": 1,
                    "prediction": false,
                    "command": "/p/vast1/build/spack/bin/spack test run /qcuula"
                }
            ],
            "symbolator": [
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ],
            "libabigail": [
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ]
        },
        "libs": {
            "spliced": [
                {
                    "dep": "pkgconf@1.7.3%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ],
            "original": [
                {
                    "dep": "pkgconf@1.8.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ]
        },
        "experiment": "experiment",
        "result": "splice-success",
        "success": true,
        "splice": "pkgconf@1.7.3",
        "package": "openmpi@3.1.0"
    },
    {
        "binaries": {
            "spliced": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps"
            ],
            "original": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-top"
            ]
        },
        "predictions": {
            "spack-test": [
                {
                    "message": "==> Spack test 3hutjcsrefi232cdtlufvumx3qkhe4rc\n==> Testing package openmpi-3.1.0-qcuulae\n==> Error: TestFailure: 18 tests failed.\n\n\nCommand exited with status 1:\n    './hello_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:37931] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     592    -------------------------------------------------------------------\n            -------\n     593    *** An error occurred in MPI_Init\n     594    *** on a NULL communicator\n     595    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     596    ***    and potentially your MPI job)\n     597    [borax27:37931] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 598    FAILED: Command exited with status 1:\n     599        './hello_c'\n     600    -------------------------------------------------------------------\n            -------\n     601    The application appears to have been direct launched using \"srun\",\n     602    but OMPI was not built with SLURM support. This usually happens\n     603    when OMPI was not configured --with-slurm and we weren't able\n     604    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:37932] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     665    -------------------------------------------------------------------\n            -------\n     666    *** An error occurred in MPI_Init\n     667    *** on a NULL communicator\n     668    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     669    ***    and potentially your MPI job)\n     670    [borax27:37932] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 671    FAILED: Command exited with status 1:\n     672        './hello_mpifh'\n     673    -------------------------------------------------------------------\n            -------\n     674    The application appears to have been direct launched using \"srun\",\n     675    but OMPI was not built with SLURM support. This usually happens\n     676    when OMPI was not configured --with-slurm and we weren't able\n     677    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 38008, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        38008\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     744    of its peer processes in the job will be killed properly.  You shou\n            ld\n     745    double check that everything has shut down cleanly.\n     746    \n     747    Local host: borax27\n     748    PID:        38008\n     749    -------------------------------------------------------------------\n            -------\n  >> 750    FAILED: Command exited with status 255:\n     751        './hello_oshmem'\n     752    -------------------------------------------------------------------\n            -------\n     753    The application appears to have been direct launched using \"srun\",\n     754    but OMPI was not built with SLURM support. This usually happens\n     755    when OMPI was not configured --with-slurm and we weren't able\n     756    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemcxx'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 38087, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        38087\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     829    of its peer processes in the job will be killed properly.  You shou\n            ld\n     830    double check that everything has shut down cleanly.\n     831    \n     832    Local host: borax27\n     833    PID:        38087\n     834    -------------------------------------------------------------------\n            -------\n  >> 835    FAILED: Command exited with status 255:\n     836        './hello_oshmemcxx'\n     837    -------------------------------------------------------------------\n            -------\n     838    The application appears to have been direct launched using \"srun\",\n     839    but OMPI was not built with SLURM support. This usually happens\n     840    when OMPI was not configured --with-slurm and we weren't able\n     841    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 38164, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        38164\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     914    of its peer processes in the job will be killed properly.  You shou\n            ld\n     915    double check that everything has shut down cleanly.\n     916    \n     917    Local host: borax27\n     918    PID:        38164\n     919    -------------------------------------------------------------------\n            -------\n  >> 920    FAILED: Command exited with status 255:\n     921        './hello_oshmemfh'\n     922    -------------------------------------------------------------------\n            -------\n     923    The application appears to have been direct launched using \"srun\",\n     924    but OMPI was not built with SLURM support. This usually happens\n     925    when OMPI was not configured --with-slurm and we weren't able\n     926    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:38240] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     993    -------------------------------------------------------------------\n            -------\n     994    *** An error occurred in MPI_Init\n     995    *** on a NULL communicator\n     996    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     997    ***    and potentially your MPI job)\n     998    [borax27:38240] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 999    FAILED: Command exited with status 1:\n     1000       './hello_usempi'\n     1001   -------------------------------------------------------------------\n            -------\n     1002   The application appears to have been direct launched using \"srun\",\n     1003   but OMPI was not built with SLURM support. This usually happens\n     1004   when OMPI was not configured --with-slurm and we weren't able\n     1005   to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:38316] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1066    ------------------------------------------------------------------\n             --------\n     1067    *** An error occurred in MPI_Init\n     1068    *** on a NULL communicator\n     1069    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1070    ***    and potentially your MPI job)\n     1071    [borax27:38316] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1072    FAILED: Command exited with status 1:\n     1073        './hello_usempif08'\n     1074    ------------------------------------------------------------------\n             --------\n     1075    The application appears to have been direct launched using \"srun\",\n     1076    but OMPI was not built with SLURM support. This usually happens\n     1077    when OMPI was not configured --with-slurm and we weren't able\n     1078    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_circular_shift'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 38394, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        38394\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1145    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1146    double check that everything has shut down cleanly.\n     1147    \n     1148    Local host: borax27\n     1149    PID:        38394\n     1150    ------------------------------------------------------------------\n             --------\n  >> 1151    FAILED: Command exited with status 255:\n     1152        './oshmem_circular_shift'\n     1153    ------------------------------------------------------------------\n             --------\n     1154    The application appears to have been direct launched using \"srun\",\n     1155    but OMPI was not built with SLURM support. This usually happens\n     1156    when OMPI was not configured --with-slurm and we weren't able\n     1157    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_max_reduction'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 38470, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        38470\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1230    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1231    double check that everything has shut down cleanly.\n     1232    \n     1233    Local host: borax27\n     1234    PID:        38470\n     1235    ------------------------------------------------------------------\n             --------\n  >> 1236    FAILED: Command exited with status 255:\n     1237        './oshmem_max_reduction'\n     1238    ------------------------------------------------------------------\n             --------\n     1239    The application appears to have been direct launched using \"srun\",\n     1240    but OMPI was not built with SLURM support. This usually happens\n     1241    when OMPI was not configured --with-slurm and we weren't able\n     1242    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_shmalloc'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 38546, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        38546\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1315    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1316    double check that everything has shut down cleanly.\n     1317    \n     1318    Local host: borax27\n     1319    PID:        38546\n     1320    ------------------------------------------------------------------\n             --------\n  >> 1321    FAILED: Command exited with status 255:\n     1322        './oshmem_shmalloc'\n     1323    ------------------------------------------------------------------\n             --------\n     1324    The application appears to have been direct launched using \"srun\",\n     1325    but OMPI was not built with SLURM support. This usually happens\n     1326    when OMPI was not configured --with-slurm and we weren't able\n     1327    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_strided_puts'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 38622, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        38622\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1400    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1401    double check that everything has shut down cleanly.\n     1402    \n     1403    Local host: borax27\n     1404    PID:        38622\n     1405    ------------------------------------------------------------------\n             --------\n  >> 1406    FAILED: Command exited with status 255:\n     1407        './oshmem_strided_puts'\n     1408    ------------------------------------------------------------------\n             --------\n     1409    The application appears to have been direct launched using \"srun\",\n     1410    but OMPI was not built with SLURM support. This usually happens\n     1411    when OMPI was not configured --with-slurm and we weren't able\n     1412    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_symmetric_data'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 38699, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        38699\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1485    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1486    double check that everything has shut down cleanly.\n     1487    \n     1488    Local host: borax27\n     1489    PID:        38699\n     1490    ------------------------------------------------------------------\n             --------\n  >> 1491    FAILED: Command exited with status 255:\n     1492        './oshmem_symmetric_data'\n     1493    ------------------------------------------------------------------\n             --------\n     1494    The application appears to have been direct launched using \"srun\",\n     1495    but OMPI was not built with SLURM support. This usually happens\n     1496    when OMPI was not configured --with-slurm and we weren't able\n     1497    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:38775] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1564    ------------------------------------------------------------------\n             --------\n     1565    *** An error occurred in MPI_Init\n     1566    *** on a NULL communicator\n     1567    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1568    ***    and potentially your MPI job)\n     1569    [borax27:38775] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1570    FAILED: Command exited with status 1:\n     1571        './ring_c'\n     1572    ------------------------------------------------------------------\n             --------\n     1573    The application appears to have been direct launched using \"srun\",\n     1574    but OMPI was not built with SLURM support. This usually happens\n     1575    when OMPI was not configured --with-slurm and we weren't able\n     1576    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:38851] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1637    ------------------------------------------------------------------\n             --------\n     1638    *** An error occurred in MPI_Init\n     1639    *** on a NULL communicator\n     1640    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1641    ***    and potentially your MPI job)\n     1642    [borax27:38851] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1643    FAILED: Command exited with status 1:\n     1644        './ring_mpifh'\n     1645    ------------------------------------------------------------------\n             --------\n     1646    The application appears to have been direct launched using \"srun\",\n     1647    but OMPI was not built with SLURM support. This usually happens\n     1648    when OMPI was not configured --with-slurm and we weren't able\n     1649    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 38927, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        38927\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1716    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1717    double check that everything has shut down cleanly.\n     1718    \n     1719    Local host: borax27\n     1720    PID:        38927\n     1721    ------------------------------------------------------------------\n             --------\n  >> 1722    FAILED: Command exited with status 255:\n     1723        './ring_oshmem'\n     1724    ------------------------------------------------------------------\n             --------\n     1725    The application appears to have been direct launched using \"srun\",\n     1726    but OMPI was not built with SLURM support. This usually happens\n     1727    when OMPI was not configured --with-slurm and we weren't able\n     1728    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 39004, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        39004\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1801    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1802    double check that everything has shut down cleanly.\n     1803    \n     1804    Local host: borax27\n     1805    PID:        39004\n     1806    ------------------------------------------------------------------\n             --------\n  >> 1807    FAILED: Command exited with status 255:\n     1808        './ring_oshmemfh'\n     1809    ------------------------------------------------------------------\n             --------\n     1810    The application appears to have been direct launched using \"srun\",\n     1811    but OMPI was not built with SLURM support. This usually happens\n     1812    when OMPI was not configured --with-slurm and we weren't able\n     1813    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:39080] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1880    ------------------------------------------------------------------\n             --------\n     1881    *** An error occurred in MPI_Init\n     1882    *** on a NULL communicator\n     1883    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1884    ***    and potentially your MPI job)\n     1885    [borax27:39080] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1886    FAILED: Command exited with status 1:\n     1887        './ring_usempi'\n     1888    ------------------------------------------------------------------\n             --------\n     1889    The application appears to have been direct launched using \"srun\",\n     1890    but OMPI was not built with SLURM support. This usually happens\n     1891    when OMPI was not configured --with-slurm and we weren't able\n     1892    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:39156] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1953    ------------------------------------------------------------------\n             --------\n     1954    *** An error occurred in MPI_Init\n     1955    *** on a NULL communicator\n     1956    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1957    ***    and potentially your MPI job)\n     1958    [borax27:39156] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1959    FAILED: Command exited with status 1:\n     1960        './ring_usempif08'\n     1961    ------------------------------------------------------------------\n             --------\n     1962    The application appears to have been direct launched using \"srun\",\n     1963    but OMPI was not built with SLURM support. This usually happens\n     1964    when OMPI was not configured --with-slurm and we weren't able\n     1965    to discover a SLURM installation in the usual places.\n\n\n\n/p/vast1/build/spack/lib/spack/spack/build_environment.py:1061, in _setup_pkg_and_run:\n       1058        tb_string = traceback.format_exc()\n       1059\n       1060        # build up some context from the offending package so we can\n  >>   1061        # show that, too.\n       1062        package_context = get_package_context(tb)\n       1063\n       1064        logfile = None\n\nSee test log for details:\n  /g/g0/sochat1/.spack/test/3hutjcsrefi232cdtlufvumx3qkhe4rc/openmpi-3.1.0-qcuulae-test-out.txt\n\n==> Error: 1 test(s) in the suite failed.\n\n======================== 1 failed, 0 passed of 1 specs =========================\n",
                    "return_code": 1,
                    "prediction": false,
                    "command": "/p/vast1/build/spack/bin/spack test run /qcuula"
                }
            ],
            "symbolator": [
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ],
            "libabigail": [
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ]
        },
        "libs": {
            "spliced": [
                {
                    "dep": "pkgconf@1.6.3%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ],
            "original": [
                {
                    "dep": "pkgconf@1.8.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ]
        },
        "experiment": "experiment",
        "result": "splice-success",
        "success": true,
        "splice": "pkgconf@1.6.3",
        "package": "openmpi@3.1.0"
    },
    {
        "binaries": {
            "spliced": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps"
            ],
            "original": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-top"
            ]
        },
        "predictions": {
            "spack-test": [
                {
                    "message": "==> Spack test 3hutjcsrefi232cdtlufvumx3qkhe4rc\n==> Testing package openmpi-3.1.0-qcuulae\n==> Error: TestFailure: 18 tests failed.\n\n\nCommand exited with status 1:\n    './hello_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:39537] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     592    -------------------------------------------------------------------\n            -------\n     593    *** An error occurred in MPI_Init\n     594    *** on a NULL communicator\n     595    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     596    ***    and potentially your MPI job)\n     597    [borax27:39537] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 598    FAILED: Command exited with status 1:\n     599        './hello_c'\n     600    -------------------------------------------------------------------\n            -------\n     601    The application appears to have been direct launched using \"srun\",\n     602    but OMPI was not built with SLURM support. This usually happens\n     603    when OMPI was not configured --with-slurm and we weren't able\n     604    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:39538] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     665    -------------------------------------------------------------------\n            -------\n     666    *** An error occurred in MPI_Init\n     667    *** on a NULL communicator\n     668    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     669    ***    and potentially your MPI job)\n     670    [borax27:39538] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 671    FAILED: Command exited with status 1:\n     672        './hello_mpifh'\n     673    -------------------------------------------------------------------\n            -------\n     674    The application appears to have been direct launched using \"srun\",\n     675    but OMPI was not built with SLURM support. This usually happens\n     676    when OMPI was not configured --with-slurm and we weren't able\n     677    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 39539, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        39539\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     744    of its peer processes in the job will be killed properly.  You shou\n            ld\n     745    double check that everything has shut down cleanly.\n     746    \n     747    Local host: borax27\n     748    PID:        39539\n     749    -------------------------------------------------------------------\n            -------\n  >> 750    FAILED: Command exited with status 255:\n     751        './hello_oshmem'\n     752    -------------------------------------------------------------------\n            -------\n     753    The application appears to have been direct launched using \"srun\",\n     754    but OMPI was not built with SLURM support. This usually happens\n     755    when OMPI was not configured --with-slurm and we weren't able\n     756    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemcxx'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 39615, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        39615\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     829    of its peer processes in the job will be killed properly.  You shou\n            ld\n     830    double check that everything has shut down cleanly.\n     831    \n     832    Local host: borax27\n     833    PID:        39615\n     834    -------------------------------------------------------------------\n            -------\n  >> 835    FAILED: Command exited with status 255:\n     836        './hello_oshmemcxx'\n     837    -------------------------------------------------------------------\n            -------\n     838    The application appears to have been direct launched using \"srun\",\n     839    but OMPI was not built with SLURM support. This usually happens\n     840    when OMPI was not configured --with-slurm and we weren't able\n     841    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 39692, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        39692\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     914    of its peer processes in the job will be killed properly.  You shou\n            ld\n     915    double check that everything has shut down cleanly.\n     916    \n     917    Local host: borax27\n     918    PID:        39692\n     919    -------------------------------------------------------------------\n            -------\n  >> 920    FAILED: Command exited with status 255:\n     921        './hello_oshmemfh'\n     922    -------------------------------------------------------------------\n            -------\n     923    The application appears to have been direct launched using \"srun\",\n     924    but OMPI was not built with SLURM support. This usually happens\n     925    when OMPI was not configured --with-slurm and we weren't able\n     926    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:39768] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     993    -------------------------------------------------------------------\n            -------\n     994    *** An error occurred in MPI_Init\n     995    *** on a NULL communicator\n     996    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     997    ***    and potentially your MPI job)\n     998    [borax27:39768] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 999    FAILED: Command exited with status 1:\n     1000       './hello_usempi'\n     1001   -------------------------------------------------------------------\n            -------\n     1002   The application appears to have been direct launched using \"srun\",\n     1003   but OMPI was not built with SLURM support. This usually happens\n     1004   when OMPI was not configured --with-slurm and we weren't able\n     1005   to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:39844] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1066    ------------------------------------------------------------------\n             --------\n     1067    *** An error occurred in MPI_Init\n     1068    *** on a NULL communicator\n     1069    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1070    ***    and potentially your MPI job)\n     1071    [borax27:39844] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1072    FAILED: Command exited with status 1:\n     1073        './hello_usempif08'\n     1074    ------------------------------------------------------------------\n             --------\n     1075    The application appears to have been direct launched using \"srun\",\n     1076    but OMPI was not built with SLURM support. This usually happens\n     1077    when OMPI was not configured --with-slurm and we weren't able\n     1078    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_circular_shift'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 39920, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        39920\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1145    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1146    double check that everything has shut down cleanly.\n     1147    \n     1148    Local host: borax27\n     1149    PID:        39920\n     1150    ------------------------------------------------------------------\n             --------\n  >> 1151    FAILED: Command exited with status 255:\n     1152        './oshmem_circular_shift'\n     1153    ------------------------------------------------------------------\n             --------\n     1154    The application appears to have been direct launched using \"srun\",\n     1155    but OMPI was not built with SLURM support. This usually happens\n     1156    when OMPI was not configured --with-slurm and we weren't able\n     1157    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_max_reduction'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 39997, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        39997\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1230    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1231    double check that everything has shut down cleanly.\n     1232    \n     1233    Local host: borax27\n     1234    PID:        39997\n     1235    ------------------------------------------------------------------\n             --------\n  >> 1236    FAILED: Command exited with status 255:\n     1237        './oshmem_max_reduction'\n     1238    ------------------------------------------------------------------\n             --------\n     1239    The application appears to have been direct launched using \"srun\",\n     1240    but OMPI was not built with SLURM support. This usually happens\n     1241    when OMPI was not configured --with-slurm and we weren't able\n     1242    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_shmalloc'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 40073, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        40073\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1315    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1316    double check that everything has shut down cleanly.\n     1317    \n     1318    Local host: borax27\n     1319    PID:        40073\n     1320    ------------------------------------------------------------------\n             --------\n  >> 1321    FAILED: Command exited with status 255:\n     1322        './oshmem_shmalloc'\n     1323    ------------------------------------------------------------------\n             --------\n     1324    The application appears to have been direct launched using \"srun\",\n     1325    but OMPI was not built with SLURM support. This usually happens\n     1326    when OMPI was not configured --with-slurm and we weren't able\n     1327    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_strided_puts'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 40151, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        40151\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1400    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1401    double check that everything has shut down cleanly.\n     1402    \n     1403    Local host: borax27\n     1404    PID:        40151\n     1405    ------------------------------------------------------------------\n             --------\n  >> 1406    FAILED: Command exited with status 255:\n     1407        './oshmem_strided_puts'\n     1408    ------------------------------------------------------------------\n             --------\n     1409    The application appears to have been direct launched using \"srun\",\n     1410    but OMPI was not built with SLURM support. This usually happens\n     1411    when OMPI was not configured --with-slurm and we weren't able\n     1412    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_symmetric_data'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 40228, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        40228\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1485    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1486    double check that everything has shut down cleanly.\n     1487    \n     1488    Local host: borax27\n     1489    PID:        40228\n     1490    ------------------------------------------------------------------\n             --------\n  >> 1491    FAILED: Command exited with status 255:\n     1492        './oshmem_symmetric_data'\n     1493    ------------------------------------------------------------------\n             --------\n     1494    The application appears to have been direct launched using \"srun\",\n     1495    but OMPI was not built with SLURM support. This usually happens\n     1496    when OMPI was not configured --with-slurm and we weren't able\n     1497    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:40305] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1564    ------------------------------------------------------------------\n             --------\n     1565    *** An error occurred in MPI_Init\n     1566    *** on a NULL communicator\n     1567    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1568    ***    and potentially your MPI job)\n     1569    [borax27:40305] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1570    FAILED: Command exited with status 1:\n     1571        './ring_c'\n     1572    ------------------------------------------------------------------\n             --------\n     1573    The application appears to have been direct launched using \"srun\",\n     1574    but OMPI was not built with SLURM support. This usually happens\n     1575    when OMPI was not configured --with-slurm and we weren't able\n     1576    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:40381] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1637    ------------------------------------------------------------------\n             --------\n     1638    *** An error occurred in MPI_Init\n     1639    *** on a NULL communicator\n     1640    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1641    ***    and potentially your MPI job)\n     1642    [borax27:40381] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1643    FAILED: Command exited with status 1:\n     1644        './ring_mpifh'\n     1645    ------------------------------------------------------------------\n             --------\n     1646    The application appears to have been direct launched using \"srun\",\n     1647    but OMPI was not built with SLURM support. This usually happens\n     1648    when OMPI was not configured --with-slurm and we weren't able\n     1649    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 40457, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        40457\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1716    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1717    double check that everything has shut down cleanly.\n     1718    \n     1719    Local host: borax27\n     1720    PID:        40457\n     1721    ------------------------------------------------------------------\n             --------\n  >> 1722    FAILED: Command exited with status 255:\n     1723        './ring_oshmem'\n     1724    ------------------------------------------------------------------\n             --------\n     1725    The application appears to have been direct launched using \"srun\",\n     1726    but OMPI was not built with SLURM support. This usually happens\n     1727    when OMPI was not configured --with-slurm and we weren't able\n     1728    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 40533, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        40533\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1801    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1802    double check that everything has shut down cleanly.\n     1803    \n     1804    Local host: borax27\n     1805    PID:        40533\n     1806    ------------------------------------------------------------------\n             --------\n  >> 1807    FAILED: Command exited with status 255:\n     1808        './ring_oshmemfh'\n     1809    ------------------------------------------------------------------\n             --------\n     1810    The application appears to have been direct launched using \"srun\",\n     1811    but OMPI was not built with SLURM support. This usually happens\n     1812    when OMPI was not configured --with-slurm and we weren't able\n     1813    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:40610] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1880    ------------------------------------------------------------------\n             --------\n     1881    *** An error occurred in MPI_Init\n     1882    *** on a NULL communicator\n     1883    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1884    ***    and potentially your MPI job)\n     1885    [borax27:40610] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1886    FAILED: Command exited with status 1:\n     1887        './ring_usempi'\n     1888    ------------------------------------------------------------------\n             --------\n     1889    The application appears to have been direct launched using \"srun\",\n     1890    but OMPI was not built with SLURM support. This usually happens\n     1891    when OMPI was not configured --with-slurm and we weren't able\n     1892    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:40686] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1953    ------------------------------------------------------------------\n             --------\n     1954    *** An error occurred in MPI_Init\n     1955    *** on a NULL communicator\n     1956    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1957    ***    and potentially your MPI job)\n     1958    [borax27:40686] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1959    FAILED: Command exited with status 1:\n     1960        './ring_usempif08'\n     1961    ------------------------------------------------------------------\n             --------\n     1962    The application appears to have been direct launched using \"srun\",\n     1963    but OMPI was not built with SLURM support. This usually happens\n     1964    when OMPI was not configured --with-slurm and we weren't able\n     1965    to discover a SLURM installation in the usual places.\n\n\n\n/p/vast1/build/spack/lib/spack/spack/build_environment.py:1061, in _setup_pkg_and_run:\n       1058        tb_string = traceback.format_exc()\n       1059\n       1060        # build up some context from the offending package so we can\n  >>   1061        # show that, too.\n       1062        package_context = get_package_context(tb)\n       1063\n       1064        logfile = None\n\nSee test log for details:\n  /g/g0/sochat1/.spack/test/3hutjcsrefi232cdtlufvumx3qkhe4rc/openmpi-3.1.0-qcuulae-test-out.txt\n\n==> Error: 1 test(s) in the suite failed.\n\n======================== 1 failed, 0 passed of 1 specs =========================\n",
                    "return_code": 1,
                    "prediction": false,
                    "command": "/p/vast1/build/spack/bin/spack test run /qcuula"
                }
            ],
            "symbolator": [
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ],
            "libabigail": [
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ]
        },
        "libs": {
            "spliced": [
                {
                    "dep": "pkgconf@1.6.1%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ],
            "original": [
                {
                    "dep": "pkgconf@1.8.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ]
        },
        "experiment": "experiment",
        "result": "splice-success",
        "success": true,
        "splice": "pkgconf@1.6.1",
        "package": "openmpi@3.1.0"
    },
    {
        "binaries": {
            "spliced": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps"
            ],
            "original": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-top"
            ]
        },
        "predictions": {
            "spack-test": [
                {
                    "message": "==> Spack test 3hutjcsrefi232cdtlufvumx3qkhe4rc\n==> Testing package openmpi-3.1.0-qcuulae\n==> Error: TestFailure: 18 tests failed.\n\n\nCommand exited with status 1:\n    './hello_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:41068] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     592    -------------------------------------------------------------------\n            -------\n     593    *** An error occurred in MPI_Init\n     594    *** on a NULL communicator\n     595    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     596    ***    and potentially your MPI job)\n     597    [borax27:41068] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 598    FAILED: Command exited with status 1:\n     599        './hello_c'\n     600    -------------------------------------------------------------------\n            -------\n     601    The application appears to have been direct launched using \"srun\",\n     602    but OMPI was not built with SLURM support. This usually happens\n     603    when OMPI was not configured --with-slurm and we weren't able\n     604    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:41069] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     665    -------------------------------------------------------------------\n            -------\n     666    *** An error occurred in MPI_Init\n     667    *** on a NULL communicator\n     668    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     669    ***    and potentially your MPI job)\n     670    [borax27:41069] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 671    FAILED: Command exited with status 1:\n     672        './hello_mpifh'\n     673    -------------------------------------------------------------------\n            -------\n     674    The application appears to have been direct launched using \"srun\",\n     675    but OMPI was not built with SLURM support. This usually happens\n     676    when OMPI was not configured --with-slurm and we weren't able\n     677    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 41070, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        41070\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     744    of its peer processes in the job will be killed properly.  You shou\n            ld\n     745    double check that everything has shut down cleanly.\n     746    \n     747    Local host: borax27\n     748    PID:        41070\n     749    -------------------------------------------------------------------\n            -------\n  >> 750    FAILED: Command exited with status 255:\n     751        './hello_oshmem'\n     752    -------------------------------------------------------------------\n            -------\n     753    The application appears to have been direct launched using \"srun\",\n     754    but OMPI was not built with SLURM support. This usually happens\n     755    when OMPI was not configured --with-slurm and we weren't able\n     756    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemcxx'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 41146, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        41146\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     829    of its peer processes in the job will be killed properly.  You shou\n            ld\n     830    double check that everything has shut down cleanly.\n     831    \n     832    Local host: borax27\n     833    PID:        41146\n     834    -------------------------------------------------------------------\n            -------\n  >> 835    FAILED: Command exited with status 255:\n     836        './hello_oshmemcxx'\n     837    -------------------------------------------------------------------\n            -------\n     838    The application appears to have been direct launched using \"srun\",\n     839    but OMPI was not built with SLURM support. This usually happens\n     840    when OMPI was not configured --with-slurm and we weren't able\n     841    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 41223, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        41223\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     914    of its peer processes in the job will be killed properly.  You shou\n            ld\n     915    double check that everything has shut down cleanly.\n     916    \n     917    Local host: borax27\n     918    PID:        41223\n     919    -------------------------------------------------------------------\n            -------\n  >> 920    FAILED: Command exited with status 255:\n     921        './hello_oshmemfh'\n     922    -------------------------------------------------------------------\n            -------\n     923    The application appears to have been direct launched using \"srun\",\n     924    but OMPI was not built with SLURM support. This usually happens\n     925    when OMPI was not configured --with-slurm and we weren't able\n     926    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:41299] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     993    -------------------------------------------------------------------\n            -------\n     994    *** An error occurred in MPI_Init\n     995    *** on a NULL communicator\n     996    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     997    ***    and potentially your MPI job)\n     998    [borax27:41299] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 999    FAILED: Command exited with status 1:\n     1000       './hello_usempi'\n     1001   -------------------------------------------------------------------\n            -------\n     1002   The application appears to have been direct launched using \"srun\",\n     1003   but OMPI was not built with SLURM support. This usually happens\n     1004   when OMPI was not configured --with-slurm and we weren't able\n     1005   to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:41375] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1066    ------------------------------------------------------------------\n             --------\n     1067    *** An error occurred in MPI_Init\n     1068    *** on a NULL communicator\n     1069    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1070    ***    and potentially your MPI job)\n     1071    [borax27:41375] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1072    FAILED: Command exited with status 1:\n     1073        './hello_usempif08'\n     1074    ------------------------------------------------------------------\n             --------\n     1075    The application appears to have been direct launched using \"srun\",\n     1076    but OMPI was not built with SLURM support. This usually happens\n     1077    when OMPI was not configured --with-slurm and we weren't able\n     1078    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_circular_shift'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 41451, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        41451\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1145    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1146    double check that everything has shut down cleanly.\n     1147    \n     1148    Local host: borax27\n     1149    PID:        41451\n     1150    ------------------------------------------------------------------\n             --------\n  >> 1151    FAILED: Command exited with status 255:\n     1152        './oshmem_circular_shift'\n     1153    ------------------------------------------------------------------\n             --------\n     1154    The application appears to have been direct launched using \"srun\",\n     1155    but OMPI was not built with SLURM support. This usually happens\n     1156    when OMPI was not configured --with-slurm and we weren't able\n     1157    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_max_reduction'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 41528, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        41528\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1230    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1231    double check that everything has shut down cleanly.\n     1232    \n     1233    Local host: borax27\n     1234    PID:        41528\n     1235    ------------------------------------------------------------------\n             --------\n  >> 1236    FAILED: Command exited with status 255:\n     1237        './oshmem_max_reduction'\n     1238    ------------------------------------------------------------------\n             --------\n     1239    The application appears to have been direct launched using \"srun\",\n     1240    but OMPI was not built with SLURM support. This usually happens\n     1241    when OMPI was not configured --with-slurm and we weren't able\n     1242    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_shmalloc'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 41604, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        41604\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1315    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1316    double check that everything has shut down cleanly.\n     1317    \n     1318    Local host: borax27\n     1319    PID:        41604\n     1320    ------------------------------------------------------------------\n             --------\n  >> 1321    FAILED: Command exited with status 255:\n     1322        './oshmem_shmalloc'\n     1323    ------------------------------------------------------------------\n             --------\n     1324    The application appears to have been direct launched using \"srun\",\n     1325    but OMPI was not built with SLURM support. This usually happens\n     1326    when OMPI was not configured --with-slurm and we weren't able\n     1327    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_strided_puts'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 41680, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        41680\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1400    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1401    double check that everything has shut down cleanly.\n     1402    \n     1403    Local host: borax27\n     1404    PID:        41680\n     1405    ------------------------------------------------------------------\n             --------\n  >> 1406    FAILED: Command exited with status 255:\n     1407        './oshmem_strided_puts'\n     1408    ------------------------------------------------------------------\n             --------\n     1409    The application appears to have been direct launched using \"srun\",\n     1410    but OMPI was not built with SLURM support. This usually happens\n     1411    when OMPI was not configured --with-slurm and we weren't able\n     1412    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_symmetric_data'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 41756, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        41756\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1485    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1486    double check that everything has shut down cleanly.\n     1487    \n     1488    Local host: borax27\n     1489    PID:        41756\n     1490    ------------------------------------------------------------------\n             --------\n  >> 1491    FAILED: Command exited with status 255:\n     1492        './oshmem_symmetric_data'\n     1493    ------------------------------------------------------------------\n             --------\n     1494    The application appears to have been direct launched using \"srun\",\n     1495    but OMPI was not built with SLURM support. This usually happens\n     1496    when OMPI was not configured --with-slurm and we weren't able\n     1497    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:41833] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1564    ------------------------------------------------------------------\n             --------\n     1565    *** An error occurred in MPI_Init\n     1566    *** on a NULL communicator\n     1567    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1568    ***    and potentially your MPI job)\n     1569    [borax27:41833] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1570    FAILED: Command exited with status 1:\n     1571        './ring_c'\n     1572    ------------------------------------------------------------------\n             --------\n     1573    The application appears to have been direct launched using \"srun\",\n     1574    but OMPI was not built with SLURM support. This usually happens\n     1575    when OMPI was not configured --with-slurm and we weren't able\n     1576    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:41909] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1637    ------------------------------------------------------------------\n             --------\n     1638    *** An error occurred in MPI_Init\n     1639    *** on a NULL communicator\n     1640    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1641    ***    and potentially your MPI job)\n     1642    [borax27:41909] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1643    FAILED: Command exited with status 1:\n     1644        './ring_mpifh'\n     1645    ------------------------------------------------------------------\n             --------\n     1646    The application appears to have been direct launched using \"srun\",\n     1647    but OMPI was not built with SLURM support. This usually happens\n     1648    when OMPI was not configured --with-slurm and we weren't able\n     1649    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 41985, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        41985\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1716    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1717    double check that everything has shut down cleanly.\n     1718    \n     1719    Local host: borax27\n     1720    PID:        41985\n     1721    ------------------------------------------------------------------\n             --------\n  >> 1722    FAILED: Command exited with status 255:\n     1723        './ring_oshmem'\n     1724    ------------------------------------------------------------------\n             --------\n     1725    The application appears to have been direct launched using \"srun\",\n     1726    but OMPI was not built with SLURM support. This usually happens\n     1727    when OMPI was not configured --with-slurm and we weren't able\n     1728    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 42061, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        42061\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1801    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1802    double check that everything has shut down cleanly.\n     1803    \n     1804    Local host: borax27\n     1805    PID:        42061\n     1806    ------------------------------------------------------------------\n             --------\n  >> 1807    FAILED: Command exited with status 255:\n     1808        './ring_oshmemfh'\n     1809    ------------------------------------------------------------------\n             --------\n     1810    The application appears to have been direct launched using \"srun\",\n     1811    but OMPI was not built with SLURM support. This usually happens\n     1812    when OMPI was not configured --with-slurm and we weren't able\n     1813    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:42138] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1880    ------------------------------------------------------------------\n             --------\n     1881    *** An error occurred in MPI_Init\n     1882    *** on a NULL communicator\n     1883    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1884    ***    and potentially your MPI job)\n     1885    [borax27:42138] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1886    FAILED: Command exited with status 1:\n     1887        './ring_usempi'\n     1888    ------------------------------------------------------------------\n             --------\n     1889    The application appears to have been direct launched using \"srun\",\n     1890    but OMPI was not built with SLURM support. This usually happens\n     1891    when OMPI was not configured --with-slurm and we weren't able\n     1892    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:42214] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1953    ------------------------------------------------------------------\n             --------\n     1954    *** An error occurred in MPI_Init\n     1955    *** on a NULL communicator\n     1956    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1957    ***    and potentially your MPI job)\n     1958    [borax27:42214] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1959    FAILED: Command exited with status 1:\n     1960        './ring_usempif08'\n     1961    ------------------------------------------------------------------\n             --------\n     1962    The application appears to have been direct launched using \"srun\",\n     1963    but OMPI was not built with SLURM support. This usually happens\n     1964    when OMPI was not configured --with-slurm and we weren't able\n     1965    to discover a SLURM installation in the usual places.\n\n\n\n/p/vast1/build/spack/lib/spack/spack/build_environment.py:1061, in _setup_pkg_and_run:\n       1058        tb_string = traceback.format_exc()\n       1059\n       1060        # build up some context from the offending package so we can\n  >>   1061        # show that, too.\n       1062        package_context = get_package_context(tb)\n       1063\n       1064        logfile = None\n\nSee test log for details:\n  /g/g0/sochat1/.spack/test/3hutjcsrefi232cdtlufvumx3qkhe4rc/openmpi-3.1.0-qcuulae-test-out.txt\n\n==> Error: 1 test(s) in the suite failed.\n\n======================== 1 failed, 0 passed of 1 specs =========================\n",
                    "return_code": 1,
                    "prediction": false,
                    "command": "/p/vast1/build/spack/bin/spack test run /qcuula"
                }
            ],
            "symbolator": [
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ],
            "libabigail": [
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ]
        },
        "libs": {
            "spliced": [
                {
                    "dep": "pkgconf@1.6.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ],
            "original": [
                {
                    "dep": "pkgconf@1.8.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ]
        },
        "experiment": "experiment",
        "result": "splice-success",
        "success": true,
        "splice": "pkgconf@1.6.0",
        "package": "openmpi@3.1.0"
    },
    {
        "binaries": {
            "spliced": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps"
            ],
            "original": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-top"
            ]
        },
        "predictions": {
            "spack-test": [
                {
                    "message": "==> Spack test 3hutjcsrefi232cdtlufvumx3qkhe4rc\n==> Testing package openmpi-3.1.0-qcuulae\n==> Error: TestFailure: 18 tests failed.\n\n\nCommand exited with status 1:\n    './hello_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:42596] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     592    -------------------------------------------------------------------\n            -------\n     593    *** An error occurred in MPI_Init\n     594    *** on a NULL communicator\n     595    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     596    ***    and potentially your MPI job)\n     597    [borax27:42596] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 598    FAILED: Command exited with status 1:\n     599        './hello_c'\n     600    -------------------------------------------------------------------\n            -------\n     601    The application appears to have been direct launched using \"srun\",\n     602    but OMPI was not built with SLURM support. This usually happens\n     603    when OMPI was not configured --with-slurm and we weren't able\n     604    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:42597] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     665    -------------------------------------------------------------------\n            -------\n     666    *** An error occurred in MPI_Init\n     667    *** on a NULL communicator\n     668    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     669    ***    and potentially your MPI job)\n     670    [borax27:42597] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 671    FAILED: Command exited with status 1:\n     672        './hello_mpifh'\n     673    -------------------------------------------------------------------\n            -------\n     674    The application appears to have been direct launched using \"srun\",\n     675    but OMPI was not built with SLURM support. This usually happens\n     676    when OMPI was not configured --with-slurm and we weren't able\n     677    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 42598, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        42598\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     744    of its peer processes in the job will be killed properly.  You shou\n            ld\n     745    double check that everything has shut down cleanly.\n     746    \n     747    Local host: borax27\n     748    PID:        42598\n     749    -------------------------------------------------------------------\n            -------\n  >> 750    FAILED: Command exited with status 255:\n     751        './hello_oshmem'\n     752    -------------------------------------------------------------------\n            -------\n     753    The application appears to have been direct launched using \"srun\",\n     754    but OMPI was not built with SLURM support. This usually happens\n     755    when OMPI was not configured --with-slurm and we weren't able\n     756    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemcxx'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 42675, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        42675\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     829    of its peer processes in the job will be killed properly.  You shou\n            ld\n     830    double check that everything has shut down cleanly.\n     831    \n     832    Local host: borax27\n     833    PID:        42675\n     834    -------------------------------------------------------------------\n            -------\n  >> 835    FAILED: Command exited with status 255:\n     836        './hello_oshmemcxx'\n     837    -------------------------------------------------------------------\n            -------\n     838    The application appears to have been direct launched using \"srun\",\n     839    but OMPI was not built with SLURM support. This usually happens\n     840    when OMPI was not configured --with-slurm and we weren't able\n     841    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 42751, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        42751\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     914    of its peer processes in the job will be killed properly.  You shou\n            ld\n     915    double check that everything has shut down cleanly.\n     916    \n     917    Local host: borax27\n     918    PID:        42751\n     919    -------------------------------------------------------------------\n            -------\n  >> 920    FAILED: Command exited with status 255:\n     921        './hello_oshmemfh'\n     922    -------------------------------------------------------------------\n            -------\n     923    The application appears to have been direct launched using \"srun\",\n     924    but OMPI was not built with SLURM support. This usually happens\n     925    when OMPI was not configured --with-slurm and we weren't able\n     926    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:42827] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     993    -------------------------------------------------------------------\n            -------\n     994    *** An error occurred in MPI_Init\n     995    *** on a NULL communicator\n     996    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     997    ***    and potentially your MPI job)\n     998    [borax27:42827] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 999    FAILED: Command exited with status 1:\n     1000       './hello_usempi'\n     1001   -------------------------------------------------------------------\n            -------\n     1002   The application appears to have been direct launched using \"srun\",\n     1003   but OMPI was not built with SLURM support. This usually happens\n     1004   when OMPI was not configured --with-slurm and we weren't able\n     1005   to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:42903] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1066    ------------------------------------------------------------------\n             --------\n     1067    *** An error occurred in MPI_Init\n     1068    *** on a NULL communicator\n     1069    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1070    ***    and potentially your MPI job)\n     1071    [borax27:42903] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1072    FAILED: Command exited with status 1:\n     1073        './hello_usempif08'\n     1074    ------------------------------------------------------------------\n             --------\n     1075    The application appears to have been direct launched using \"srun\",\n     1076    but OMPI was not built with SLURM support. This usually happens\n     1077    when OMPI was not configured --with-slurm and we weren't able\n     1078    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_circular_shift'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 42979, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        42979\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1145    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1146    double check that everything has shut down cleanly.\n     1147    \n     1148    Local host: borax27\n     1149    PID:        42979\n     1150    ------------------------------------------------------------------\n             --------\n  >> 1151    FAILED: Command exited with status 255:\n     1152        './oshmem_circular_shift'\n     1153    ------------------------------------------------------------------\n             --------\n     1154    The application appears to have been direct launched using \"srun\",\n     1155    but OMPI was not built with SLURM support. This usually happens\n     1156    when OMPI was not configured --with-slurm and we weren't able\n     1157    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_max_reduction'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 43056, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        43056\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1230    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1231    double check that everything has shut down cleanly.\n     1232    \n     1233    Local host: borax27\n     1234    PID:        43056\n     1235    ------------------------------------------------------------------\n             --------\n  >> 1236    FAILED: Command exited with status 255:\n     1237        './oshmem_max_reduction'\n     1238    ------------------------------------------------------------------\n             --------\n     1239    The application appears to have been direct launched using \"srun\",\n     1240    but OMPI was not built with SLURM support. This usually happens\n     1241    when OMPI was not configured --with-slurm and we weren't able\n     1242    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_shmalloc'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 43133, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        43133\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1315    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1316    double check that everything has shut down cleanly.\n     1317    \n     1318    Local host: borax27\n     1319    PID:        43133\n     1320    ------------------------------------------------------------------\n             --------\n  >> 1321    FAILED: Command exited with status 255:\n     1322        './oshmem_shmalloc'\n     1323    ------------------------------------------------------------------\n             --------\n     1324    The application appears to have been direct launched using \"srun\",\n     1325    but OMPI was not built with SLURM support. This usually happens\n     1326    when OMPI was not configured --with-slurm and we weren't able\n     1327    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_strided_puts'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 43209, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        43209\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1400    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1401    double check that everything has shut down cleanly.\n     1402    \n     1403    Local host: borax27\n     1404    PID:        43209\n     1405    ------------------------------------------------------------------\n             --------\n  >> 1406    FAILED: Command exited with status 255:\n     1407        './oshmem_strided_puts'\n     1408    ------------------------------------------------------------------\n             --------\n     1409    The application appears to have been direct launched using \"srun\",\n     1410    but OMPI was not built with SLURM support. This usually happens\n     1411    when OMPI was not configured --with-slurm and we weren't able\n     1412    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_symmetric_data'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 43285, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        43285\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1485    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1486    double check that everything has shut down cleanly.\n     1487    \n     1488    Local host: borax27\n     1489    PID:        43285\n     1490    ------------------------------------------------------------------\n             --------\n  >> 1491    FAILED: Command exited with status 255:\n     1492        './oshmem_symmetric_data'\n     1493    ------------------------------------------------------------------\n             --------\n     1494    The application appears to have been direct launched using \"srun\",\n     1495    but OMPI was not built with SLURM support. This usually happens\n     1496    when OMPI was not configured --with-slurm and we weren't able\n     1497    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:43363] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1564    ------------------------------------------------------------------\n             --------\n     1565    *** An error occurred in MPI_Init\n     1566    *** on a NULL communicator\n     1567    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1568    ***    and potentially your MPI job)\n     1569    [borax27:43363] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1570    FAILED: Command exited with status 1:\n     1571        './ring_c'\n     1572    ------------------------------------------------------------------\n             --------\n     1573    The application appears to have been direct launched using \"srun\",\n     1574    but OMPI was not built with SLURM support. This usually happens\n     1575    when OMPI was not configured --with-slurm and we weren't able\n     1576    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:43439] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1637    ------------------------------------------------------------------\n             --------\n     1638    *** An error occurred in MPI_Init\n     1639    *** on a NULL communicator\n     1640    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1641    ***    and potentially your MPI job)\n     1642    [borax27:43439] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1643    FAILED: Command exited with status 1:\n     1644        './ring_mpifh'\n     1645    ------------------------------------------------------------------\n             --------\n     1646    The application appears to have been direct launched using \"srun\",\n     1647    but OMPI was not built with SLURM support. This usually happens\n     1648    when OMPI was not configured --with-slurm and we weren't able\n     1649    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 43515, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        43515\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1716    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1717    double check that everything has shut down cleanly.\n     1718    \n     1719    Local host: borax27\n     1720    PID:        43515\n     1721    ------------------------------------------------------------------\n             --------\n  >> 1722    FAILED: Command exited with status 255:\n     1723        './ring_oshmem'\n     1724    ------------------------------------------------------------------\n             --------\n     1725    The application appears to have been direct launched using \"srun\",\n     1726    but OMPI was not built with SLURM support. This usually happens\n     1727    when OMPI was not configured --with-slurm and we weren't able\n     1728    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 43591, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        43591\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1801    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1802    double check that everything has shut down cleanly.\n     1803    \n     1804    Local host: borax27\n     1805    PID:        43591\n     1806    ------------------------------------------------------------------\n             --------\n  >> 1807    FAILED: Command exited with status 255:\n     1808        './ring_oshmemfh'\n     1809    ------------------------------------------------------------------\n             --------\n     1810    The application appears to have been direct launched using \"srun\",\n     1811    but OMPI was not built with SLURM support. This usually happens\n     1812    when OMPI was not configured --with-slurm and we weren't able\n     1813    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:43668] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1880    ------------------------------------------------------------------\n             --------\n     1881    *** An error occurred in MPI_Init\n     1882    *** on a NULL communicator\n     1883    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1884    ***    and potentially your MPI job)\n     1885    [borax27:43668] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1886    FAILED: Command exited with status 1:\n     1887        './ring_usempi'\n     1888    ------------------------------------------------------------------\n             --------\n     1889    The application appears to have been direct launched using \"srun\",\n     1890    but OMPI was not built with SLURM support. This usually happens\n     1891    when OMPI was not configured --with-slurm and we weren't able\n     1892    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:43744] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1953    ------------------------------------------------------------------\n             --------\n     1954    *** An error occurred in MPI_Init\n     1955    *** on a NULL communicator\n     1956    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1957    ***    and potentially your MPI job)\n     1958    [borax27:43744] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1959    FAILED: Command exited with status 1:\n     1960        './ring_usempif08'\n     1961    ------------------------------------------------------------------\n             --------\n     1962    The application appears to have been direct launched using \"srun\",\n     1963    but OMPI was not built with SLURM support. This usually happens\n     1964    when OMPI was not configured --with-slurm and we weren't able\n     1965    to discover a SLURM installation in the usual places.\n\n\n\n/p/vast1/build/spack/lib/spack/spack/build_environment.py:1061, in _setup_pkg_and_run:\n       1058        tb_string = traceback.format_exc()\n       1059\n       1060        # build up some context from the offending package so we can\n  >>   1061        # show that, too.\n       1062        package_context = get_package_context(tb)\n       1063\n       1064        logfile = None\n\nSee test log for details:\n  /g/g0/sochat1/.spack/test/3hutjcsrefi232cdtlufvumx3qkhe4rc/openmpi-3.1.0-qcuulae-test-out.txt\n\n==> Error: 1 test(s) in the suite failed.\n\n======================== 1 failed, 0 passed of 1 specs =========================\n",
                    "return_code": 1,
                    "prediction": false,
                    "command": "/p/vast1/build/spack/bin/spack test run /qcuula"
                }
            ],
            "symbolator": [
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ],
            "libabigail": [
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ]
        },
        "libs": {
            "spliced": [
                {
                    "dep": "pkgconf@1.5.4%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ],
            "original": [
                {
                    "dep": "pkgconf@1.8.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ]
        },
        "experiment": "experiment",
        "result": "splice-success",
        "success": true,
        "splice": "pkgconf@1.5.4",
        "package": "openmpi@3.1.0"
    },
    {
        "binaries": {
            "spliced": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps"
            ],
            "original": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-top"
            ]
        },
        "predictions": {
            "spack-test": [
                {
                    "message": "==> Spack test 3hutjcsrefi232cdtlufvumx3qkhe4rc\n==> Testing package openmpi-3.1.0-qcuulae\n==> Error: TestFailure: 18 tests failed.\n\n\nCommand exited with status 1:\n    './hello_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:44126] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     592    -------------------------------------------------------------------\n            -------\n     593    *** An error occurred in MPI_Init\n     594    *** on a NULL communicator\n     595    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     596    ***    and potentially your MPI job)\n     597    [borax27:44126] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 598    FAILED: Command exited with status 1:\n     599        './hello_c'\n     600    -------------------------------------------------------------------\n            -------\n     601    The application appears to have been direct launched using \"srun\",\n     602    but OMPI was not built with SLURM support. This usually happens\n     603    when OMPI was not configured --with-slurm and we weren't able\n     604    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:44127] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     665    -------------------------------------------------------------------\n            -------\n     666    *** An error occurred in MPI_Init\n     667    *** on a NULL communicator\n     668    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     669    ***    and potentially your MPI job)\n     670    [borax27:44127] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 671    FAILED: Command exited with status 1:\n     672        './hello_mpifh'\n     673    -------------------------------------------------------------------\n            -------\n     674    The application appears to have been direct launched using \"srun\",\n     675    but OMPI was not built with SLURM support. This usually happens\n     676    when OMPI was not configured --with-slurm and we weren't able\n     677    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 44128, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        44128\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     744    of its peer processes in the job will be killed properly.  You shou\n            ld\n     745    double check that everything has shut down cleanly.\n     746    \n     747    Local host: borax27\n     748    PID:        44128\n     749    -------------------------------------------------------------------\n            -------\n  >> 750    FAILED: Command exited with status 255:\n     751        './hello_oshmem'\n     752    -------------------------------------------------------------------\n            -------\n     753    The application appears to have been direct launched using \"srun\",\n     754    but OMPI was not built with SLURM support. This usually happens\n     755    when OMPI was not configured --with-slurm and we weren't able\n     756    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemcxx'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 44204, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        44204\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     829    of its peer processes in the job will be killed properly.  You shou\n            ld\n     830    double check that everything has shut down cleanly.\n     831    \n     832    Local host: borax27\n     833    PID:        44204\n     834    -------------------------------------------------------------------\n            -------\n  >> 835    FAILED: Command exited with status 255:\n     836        './hello_oshmemcxx'\n     837    -------------------------------------------------------------------\n            -------\n     838    The application appears to have been direct launched using \"srun\",\n     839    but OMPI was not built with SLURM support. This usually happens\n     840    when OMPI was not configured --with-slurm and we weren't able\n     841    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 44280, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        44280\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     914    of its peer processes in the job will be killed properly.  You shou\n            ld\n     915    double check that everything has shut down cleanly.\n     916    \n     917    Local host: borax27\n     918    PID:        44280\n     919    -------------------------------------------------------------------\n            -------\n  >> 920    FAILED: Command exited with status 255:\n     921        './hello_oshmemfh'\n     922    -------------------------------------------------------------------\n            -------\n     923    The application appears to have been direct launched using \"srun\",\n     924    but OMPI was not built with SLURM support. This usually happens\n     925    when OMPI was not configured --with-slurm and we weren't able\n     926    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:44356] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     993    -------------------------------------------------------------------\n            -------\n     994    *** An error occurred in MPI_Init\n     995    *** on a NULL communicator\n     996    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     997    ***    and potentially your MPI job)\n     998    [borax27:44356] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 999    FAILED: Command exited with status 1:\n     1000       './hello_usempi'\n     1001   -------------------------------------------------------------------\n            -------\n     1002   The application appears to have been direct launched using \"srun\",\n     1003   but OMPI was not built with SLURM support. This usually happens\n     1004   when OMPI was not configured --with-slurm and we weren't able\n     1005   to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:44433] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1066    ------------------------------------------------------------------\n             --------\n     1067    *** An error occurred in MPI_Init\n     1068    *** on a NULL communicator\n     1069    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1070    ***    and potentially your MPI job)\n     1071    [borax27:44433] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1072    FAILED: Command exited with status 1:\n     1073        './hello_usempif08'\n     1074    ------------------------------------------------------------------\n             --------\n     1075    The application appears to have been direct launched using \"srun\",\n     1076    but OMPI was not built with SLURM support. This usually happens\n     1077    when OMPI was not configured --with-slurm and we weren't able\n     1078    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_circular_shift'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 44511, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        44511\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1145    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1146    double check that everything has shut down cleanly.\n     1147    \n     1148    Local host: borax27\n     1149    PID:        44511\n     1150    ------------------------------------------------------------------\n             --------\n  >> 1151    FAILED: Command exited with status 255:\n     1152        './oshmem_circular_shift'\n     1153    ------------------------------------------------------------------\n             --------\n     1154    The application appears to have been direct launched using \"srun\",\n     1155    but OMPI was not built with SLURM support. This usually happens\n     1156    when OMPI was not configured --with-slurm and we weren't able\n     1157    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_max_reduction'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 44587, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        44587\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1230    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1231    double check that everything has shut down cleanly.\n     1232    \n     1233    Local host: borax27\n     1234    PID:        44587\n     1235    ------------------------------------------------------------------\n             --------\n  >> 1236    FAILED: Command exited with status 255:\n     1237        './oshmem_max_reduction'\n     1238    ------------------------------------------------------------------\n             --------\n     1239    The application appears to have been direct launched using \"srun\",\n     1240    but OMPI was not built with SLURM support. This usually happens\n     1241    when OMPI was not configured --with-slurm and we weren't able\n     1242    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_shmalloc'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 44663, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        44663\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1315    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1316    double check that everything has shut down cleanly.\n     1317    \n     1318    Local host: borax27\n     1319    PID:        44663\n     1320    ------------------------------------------------------------------\n             --------\n  >> 1321    FAILED: Command exited with status 255:\n     1322        './oshmem_shmalloc'\n     1323    ------------------------------------------------------------------\n             --------\n     1324    The application appears to have been direct launched using \"srun\",\n     1325    but OMPI was not built with SLURM support. This usually happens\n     1326    when OMPI was not configured --with-slurm and we weren't able\n     1327    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_strided_puts'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 44741, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        44741\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1400    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1401    double check that everything has shut down cleanly.\n     1402    \n     1403    Local host: borax27\n     1404    PID:        44741\n     1405    ------------------------------------------------------------------\n             --------\n  >> 1406    FAILED: Command exited with status 255:\n     1407        './oshmem_strided_puts'\n     1408    ------------------------------------------------------------------\n             --------\n     1409    The application appears to have been direct launched using \"srun\",\n     1410    but OMPI was not built with SLURM support. This usually happens\n     1411    when OMPI was not configured --with-slurm and we weren't able\n     1412    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_symmetric_data'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 44817, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        44817\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1485    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1486    double check that everything has shut down cleanly.\n     1487    \n     1488    Local host: borax27\n     1489    PID:        44817\n     1490    ------------------------------------------------------------------\n             --------\n  >> 1491    FAILED: Command exited with status 255:\n     1492        './oshmem_symmetric_data'\n     1493    ------------------------------------------------------------------\n             --------\n     1494    The application appears to have been direct launched using \"srun\",\n     1495    but OMPI was not built with SLURM support. This usually happens\n     1496    when OMPI was not configured --with-slurm and we weren't able\n     1497    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:44893] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1564    ------------------------------------------------------------------\n             --------\n     1565    *** An error occurred in MPI_Init\n     1566    *** on a NULL communicator\n     1567    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1568    ***    and potentially your MPI job)\n     1569    [borax27:44893] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1570    FAILED: Command exited with status 1:\n     1571        './ring_c'\n     1572    ------------------------------------------------------------------\n             --------\n     1573    The application appears to have been direct launched using \"srun\",\n     1574    but OMPI was not built with SLURM support. This usually happens\n     1575    when OMPI was not configured --with-slurm and we weren't able\n     1576    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:44969] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1637    ------------------------------------------------------------------\n             --------\n     1638    *** An error occurred in MPI_Init\n     1639    *** on a NULL communicator\n     1640    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1641    ***    and potentially your MPI job)\n     1642    [borax27:44969] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1643    FAILED: Command exited with status 1:\n     1644        './ring_mpifh'\n     1645    ------------------------------------------------------------------\n             --------\n     1646    The application appears to have been direct launched using \"srun\",\n     1647    but OMPI was not built with SLURM support. This usually happens\n     1648    when OMPI was not configured --with-slurm and we weren't able\n     1649    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 45046, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        45046\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1716    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1717    double check that everything has shut down cleanly.\n     1718    \n     1719    Local host: borax27\n     1720    PID:        45046\n     1721    ------------------------------------------------------------------\n             --------\n  >> 1722    FAILED: Command exited with status 255:\n     1723        './ring_oshmem'\n     1724    ------------------------------------------------------------------\n             --------\n     1725    The application appears to have been direct launched using \"srun\",\n     1726    but OMPI was not built with SLURM support. This usually happens\n     1727    when OMPI was not configured --with-slurm and we weren't able\n     1728    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 45122, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        45122\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1801    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1802    double check that everything has shut down cleanly.\n     1803    \n     1804    Local host: borax27\n     1805    PID:        45122\n     1806    ------------------------------------------------------------------\n             --------\n  >> 1807    FAILED: Command exited with status 255:\n     1808        './ring_oshmemfh'\n     1809    ------------------------------------------------------------------\n             --------\n     1810    The application appears to have been direct launched using \"srun\",\n     1811    but OMPI was not built with SLURM support. This usually happens\n     1812    when OMPI was not configured --with-slurm and we weren't able\n     1813    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:45199] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1880    ------------------------------------------------------------------\n             --------\n     1881    *** An error occurred in MPI_Init\n     1882    *** on a NULL communicator\n     1883    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1884    ***    and potentially your MPI job)\n     1885    [borax27:45199] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1886    FAILED: Command exited with status 1:\n     1887        './ring_usempi'\n     1888    ------------------------------------------------------------------\n             --------\n     1889    The application appears to have been direct launched using \"srun\",\n     1890    but OMPI was not built with SLURM support. This usually happens\n     1891    when OMPI was not configured --with-slurm and we weren't able\n     1892    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:45275] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1953    ------------------------------------------------------------------\n             --------\n     1954    *** An error occurred in MPI_Init\n     1955    *** on a NULL communicator\n     1956    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1957    ***    and potentially your MPI job)\n     1958    [borax27:45275] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1959    FAILED: Command exited with status 1:\n     1960        './ring_usempif08'\n     1961    ------------------------------------------------------------------\n             --------\n     1962    The application appears to have been direct launched using \"srun\",\n     1963    but OMPI was not built with SLURM support. This usually happens\n     1964    when OMPI was not configured --with-slurm and we weren't able\n     1965    to discover a SLURM installation in the usual places.\n\n\n\n/p/vast1/build/spack/lib/spack/spack/build_environment.py:1061, in _setup_pkg_and_run:\n       1058        tb_string = traceback.format_exc()\n       1059\n       1060        # build up some context from the offending package so we can\n  >>   1061        # show that, too.\n       1062        package_context = get_package_context(tb)\n       1063\n       1064        logfile = None\n\nSee test log for details:\n  /g/g0/sochat1/.spack/test/3hutjcsrefi232cdtlufvumx3qkhe4rc/openmpi-3.1.0-qcuulae-test-out.txt\n\n==> Error: 1 test(s) in the suite failed.\n\n======================== 1 failed, 0 passed of 1 specs =========================\n",
                    "return_code": 1,
                    "prediction": false,
                    "command": "/p/vast1/build/spack/bin/spack test run /qcuula"
                }
            ],
            "symbolator": [
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ],
            "libabigail": [
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ]
        },
        "libs": {
            "spliced": [
                {
                    "dep": "pkgconf@1.4.2%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ],
            "original": [
                {
                    "dep": "pkgconf@1.8.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ]
        },
        "experiment": "experiment",
        "result": "splice-success",
        "success": true,
        "splice": "pkgconf@1.4.2",
        "package": "openmpi@3.1.0"
    },
    {
        "binaries": {
            "spliced": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps"
            ],
            "original": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-top"
            ]
        },
        "predictions": {
            "spack-test": [
                {
                    "message": "==> Spack test 3hutjcsrefi232cdtlufvumx3qkhe4rc\n==> Testing package openmpi-3.1.0-qcuulae\n==> Error: TestFailure: 18 tests failed.\n\n\nCommand exited with status 1:\n    './hello_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:45656] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     592    -------------------------------------------------------------------\n            -------\n     593    *** An error occurred in MPI_Init\n     594    *** on a NULL communicator\n     595    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     596    ***    and potentially your MPI job)\n     597    [borax27:45656] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 598    FAILED: Command exited with status 1:\n     599        './hello_c'\n     600    -------------------------------------------------------------------\n            -------\n     601    The application appears to have been direct launched using \"srun\",\n     602    but OMPI was not built with SLURM support. This usually happens\n     603    when OMPI was not configured --with-slurm and we weren't able\n     604    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:45657] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     665    -------------------------------------------------------------------\n            -------\n     666    *** An error occurred in MPI_Init\n     667    *** on a NULL communicator\n     668    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     669    ***    and potentially your MPI job)\n     670    [borax27:45657] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 671    FAILED: Command exited with status 1:\n     672        './hello_mpifh'\n     673    -------------------------------------------------------------------\n            -------\n     674    The application appears to have been direct launched using \"srun\",\n     675    but OMPI was not built with SLURM support. This usually happens\n     676    when OMPI was not configured --with-slurm and we weren't able\n     677    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 45658, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        45658\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     744    of its peer processes in the job will be killed properly.  You shou\n            ld\n     745    double check that everything has shut down cleanly.\n     746    \n     747    Local host: borax27\n     748    PID:        45658\n     749    -------------------------------------------------------------------\n            -------\n  >> 750    FAILED: Command exited with status 255:\n     751        './hello_oshmem'\n     752    -------------------------------------------------------------------\n            -------\n     753    The application appears to have been direct launched using \"srun\",\n     754    but OMPI was not built with SLURM support. This usually happens\n     755    when OMPI was not configured --with-slurm and we weren't able\n     756    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemcxx'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 45735, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        45735\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     829    of its peer processes in the job will be killed properly.  You shou\n            ld\n     830    double check that everything has shut down cleanly.\n     831    \n     832    Local host: borax27\n     833    PID:        45735\n     834    -------------------------------------------------------------------\n            -------\n  >> 835    FAILED: Command exited with status 255:\n     836        './hello_oshmemcxx'\n     837    -------------------------------------------------------------------\n            -------\n     838    The application appears to have been direct launched using \"srun\",\n     839    but OMPI was not built with SLURM support. This usually happens\n     840    when OMPI was not configured --with-slurm and we weren't able\n     841    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 45811, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        45811\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     914    of its peer processes in the job will be killed properly.  You shou\n            ld\n     915    double check that everything has shut down cleanly.\n     916    \n     917    Local host: borax27\n     918    PID:        45811\n     919    -------------------------------------------------------------------\n            -------\n  >> 920    FAILED: Command exited with status 255:\n     921        './hello_oshmemfh'\n     922    -------------------------------------------------------------------\n            -------\n     923    The application appears to have been direct launched using \"srun\",\n     924    but OMPI was not built with SLURM support. This usually happens\n     925    when OMPI was not configured --with-slurm and we weren't able\n     926    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:45887] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     993    -------------------------------------------------------------------\n            -------\n     994    *** An error occurred in MPI_Init\n     995    *** on a NULL communicator\n     996    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     997    ***    and potentially your MPI job)\n     998    [borax27:45887] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 999    FAILED: Command exited with status 1:\n     1000       './hello_usempi'\n     1001   -------------------------------------------------------------------\n            -------\n     1002   The application appears to have been direct launched using \"srun\",\n     1003   but OMPI was not built with SLURM support. This usually happens\n     1004   when OMPI was not configured --with-slurm and we weren't able\n     1005   to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:45963] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1066    ------------------------------------------------------------------\n             --------\n     1067    *** An error occurred in MPI_Init\n     1068    *** on a NULL communicator\n     1069    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1070    ***    and potentially your MPI job)\n     1071    [borax27:45963] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1072    FAILED: Command exited with status 1:\n     1073        './hello_usempif08'\n     1074    ------------------------------------------------------------------\n             --------\n     1075    The application appears to have been direct launched using \"srun\",\n     1076    but OMPI was not built with SLURM support. This usually happens\n     1077    when OMPI was not configured --with-slurm and we weren't able\n     1078    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_circular_shift'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 46040, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        46040\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1145    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1146    double check that everything has shut down cleanly.\n     1147    \n     1148    Local host: borax27\n     1149    PID:        46040\n     1150    ------------------------------------------------------------------\n             --------\n  >> 1151    FAILED: Command exited with status 255:\n     1152        './oshmem_circular_shift'\n     1153    ------------------------------------------------------------------\n             --------\n     1154    The application appears to have been direct launched using \"srun\",\n     1155    but OMPI was not built with SLURM support. This usually happens\n     1156    when OMPI was not configured --with-slurm and we weren't able\n     1157    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_max_reduction'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 46116, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        46116\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1230    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1231    double check that everything has shut down cleanly.\n     1232    \n     1233    Local host: borax27\n     1234    PID:        46116\n     1235    ------------------------------------------------------------------\n             --------\n  >> 1236    FAILED: Command exited with status 255:\n     1237        './oshmem_max_reduction'\n     1238    ------------------------------------------------------------------\n             --------\n     1239    The application appears to have been direct launched using \"srun\",\n     1240    but OMPI was not built with SLURM support. This usually happens\n     1241    when OMPI was not configured --with-slurm and we weren't able\n     1242    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_shmalloc'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 46192, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        46192\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1315    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1316    double check that everything has shut down cleanly.\n     1317    \n     1318    Local host: borax27\n     1319    PID:        46192\n     1320    ------------------------------------------------------------------\n             --------\n  >> 1321    FAILED: Command exited with status 255:\n     1322        './oshmem_shmalloc'\n     1323    ------------------------------------------------------------------\n             --------\n     1324    The application appears to have been direct launched using \"srun\",\n     1325    but OMPI was not built with SLURM support. This usually happens\n     1326    when OMPI was not configured --with-slurm and we weren't able\n     1327    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_strided_puts'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 46268, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        46268\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1400    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1401    double check that everything has shut down cleanly.\n     1402    \n     1403    Local host: borax27\n     1404    PID:        46268\n     1405    ------------------------------------------------------------------\n             --------\n  >> 1406    FAILED: Command exited with status 255:\n     1407        './oshmem_strided_puts'\n     1408    ------------------------------------------------------------------\n             --------\n     1409    The application appears to have been direct launched using \"srun\",\n     1410    but OMPI was not built with SLURM support. This usually happens\n     1411    when OMPI was not configured --with-slurm and we weren't able\n     1412    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_symmetric_data'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 46345, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        46345\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1485    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1486    double check that everything has shut down cleanly.\n     1487    \n     1488    Local host: borax27\n     1489    PID:        46345\n     1490    ------------------------------------------------------------------\n             --------\n  >> 1491    FAILED: Command exited with status 255:\n     1492        './oshmem_symmetric_data'\n     1493    ------------------------------------------------------------------\n             --------\n     1494    The application appears to have been direct launched using \"srun\",\n     1495    but OMPI was not built with SLURM support. This usually happens\n     1496    when OMPI was not configured --with-slurm and we weren't able\n     1497    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:46421] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1564    ------------------------------------------------------------------\n             --------\n     1565    *** An error occurred in MPI_Init\n     1566    *** on a NULL communicator\n     1567    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1568    ***    and potentially your MPI job)\n     1569    [borax27:46421] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1570    FAILED: Command exited with status 1:\n     1571        './ring_c'\n     1572    ------------------------------------------------------------------\n             --------\n     1573    The application appears to have been direct launched using \"srun\",\n     1574    but OMPI was not built with SLURM support. This usually happens\n     1575    when OMPI was not configured --with-slurm and we weren't able\n     1576    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:46497] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1637    ------------------------------------------------------------------\n             --------\n     1638    *** An error occurred in MPI_Init\n     1639    *** on a NULL communicator\n     1640    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1641    ***    and potentially your MPI job)\n     1642    [borax27:46497] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1643    FAILED: Command exited with status 1:\n     1644        './ring_mpifh'\n     1645    ------------------------------------------------------------------\n             --------\n     1646    The application appears to have been direct launched using \"srun\",\n     1647    but OMPI was not built with SLURM support. This usually happens\n     1648    when OMPI was not configured --with-slurm and we weren't able\n     1649    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 46573, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        46573\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1716    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1717    double check that everything has shut down cleanly.\n     1718    \n     1719    Local host: borax27\n     1720    PID:        46573\n     1721    ------------------------------------------------------------------\n             --------\n  >> 1722    FAILED: Command exited with status 255:\n     1723        './ring_oshmem'\n     1724    ------------------------------------------------------------------\n             --------\n     1725    The application appears to have been direct launched using \"srun\",\n     1726    but OMPI was not built with SLURM support. This usually happens\n     1727    when OMPI was not configured --with-slurm and we weren't able\n     1728    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 46650, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        46650\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1801    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1802    double check that everything has shut down cleanly.\n     1803    \n     1804    Local host: borax27\n     1805    PID:        46650\n     1806    ------------------------------------------------------------------\n             --------\n  >> 1807    FAILED: Command exited with status 255:\n     1808        './ring_oshmemfh'\n     1809    ------------------------------------------------------------------\n             --------\n     1810    The application appears to have been direct launched using \"srun\",\n     1811    but OMPI was not built with SLURM support. This usually happens\n     1812    when OMPI was not configured --with-slurm and we weren't able\n     1813    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:46726] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1880    ------------------------------------------------------------------\n             --------\n     1881    *** An error occurred in MPI_Init\n     1882    *** on a NULL communicator\n     1883    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1884    ***    and potentially your MPI job)\n     1885    [borax27:46726] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1886    FAILED: Command exited with status 1:\n     1887        './ring_usempi'\n     1888    ------------------------------------------------------------------\n             --------\n     1889    The application appears to have been direct launched using \"srun\",\n     1890    but OMPI was not built with SLURM support. This usually happens\n     1891    when OMPI was not configured --with-slurm and we weren't able\n     1892    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:46802] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1953    ------------------------------------------------------------------\n             --------\n     1954    *** An error occurred in MPI_Init\n     1955    *** on a NULL communicator\n     1956    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1957    ***    and potentially your MPI job)\n     1958    [borax27:46802] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1959    FAILED: Command exited with status 1:\n     1960        './ring_usempif08'\n     1961    ------------------------------------------------------------------\n             --------\n     1962    The application appears to have been direct launched using \"srun\",\n     1963    but OMPI was not built with SLURM support. This usually happens\n     1964    when OMPI was not configured --with-slurm and we weren't able\n     1965    to discover a SLURM installation in the usual places.\n\n\n\n/p/vast1/build/spack/lib/spack/spack/build_environment.py:1061, in _setup_pkg_and_run:\n       1058        tb_string = traceback.format_exc()\n       1059\n       1060        # build up some context from the offending package so we can\n  >>   1061        # show that, too.\n       1062        package_context = get_package_context(tb)\n       1063\n       1064        logfile = None\n\nSee test log for details:\n  /g/g0/sochat1/.spack/test/3hutjcsrefi232cdtlufvumx3qkhe4rc/openmpi-3.1.0-qcuulae-test-out.txt\n\n==> Error: 1 test(s) in the suite failed.\n\n======================== 1 failed, 0 passed of 1 specs =========================\n",
                    "return_code": 1,
                    "prediction": false,
                    "command": "/p/vast1/build/spack/bin/spack test run /qcuula"
                }
            ],
            "symbolator": [
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ],
            "libabigail": [
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ]
        },
        "libs": {
            "spliced": [
                {
                    "dep": "pkgconf@1.4.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ],
            "original": [
                {
                    "dep": "pkgconf@1.8.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ]
        },
        "experiment": "experiment",
        "result": "splice-success",
        "success": true,
        "splice": "pkgconf@1.4.0",
        "package": "openmpi@3.1.0"
    },
    {
        "binaries": {
            "spliced": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps"
            ],
            "original": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-top"
            ]
        },
        "predictions": {
            "spack-test": [
                {
                    "message": "==> Spack test 3hutjcsrefi232cdtlufvumx3qkhe4rc\n==> Testing package openmpi-3.1.0-qcuulae\n==> Error: TestFailure: 18 tests failed.\n\n\nCommand exited with status 1:\n    './hello_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:47184] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     592    -------------------------------------------------------------------\n            -------\n     593    *** An error occurred in MPI_Init\n     594    *** on a NULL communicator\n     595    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     596    ***    and potentially your MPI job)\n     597    [borax27:47184] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 598    FAILED: Command exited with status 1:\n     599        './hello_c'\n     600    -------------------------------------------------------------------\n            -------\n     601    The application appears to have been direct launched using \"srun\",\n     602    but OMPI was not built with SLURM support. This usually happens\n     603    when OMPI was not configured --with-slurm and we weren't able\n     604    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:47185] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     665    -------------------------------------------------------------------\n            -------\n     666    *** An error occurred in MPI_Init\n     667    *** on a NULL communicator\n     668    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     669    ***    and potentially your MPI job)\n     670    [borax27:47185] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 671    FAILED: Command exited with status 1:\n     672        './hello_mpifh'\n     673    -------------------------------------------------------------------\n            -------\n     674    The application appears to have been direct launched using \"srun\",\n     675    but OMPI was not built with SLURM support. This usually happens\n     676    when OMPI was not configured --with-slurm and we weren't able\n     677    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 47263, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        47263\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     744    of its peer processes in the job will be killed properly.  You shou\n            ld\n     745    double check that everything has shut down cleanly.\n     746    \n     747    Local host: borax27\n     748    PID:        47263\n     749    -------------------------------------------------------------------\n            -------\n  >> 750    FAILED: Command exited with status 255:\n     751        './hello_oshmem'\n     752    -------------------------------------------------------------------\n            -------\n     753    The application appears to have been direct launched using \"srun\",\n     754    but OMPI was not built with SLURM support. This usually happens\n     755    when OMPI was not configured --with-slurm and we weren't able\n     756    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemcxx'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 47339, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        47339\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     829    of its peer processes in the job will be killed properly.  You shou\n            ld\n     830    double check that everything has shut down cleanly.\n     831    \n     832    Local host: borax27\n     833    PID:        47339\n     834    -------------------------------------------------------------------\n            -------\n  >> 835    FAILED: Command exited with status 255:\n     836        './hello_oshmemcxx'\n     837    -------------------------------------------------------------------\n            -------\n     838    The application appears to have been direct launched using \"srun\",\n     839    but OMPI was not built with SLURM support. This usually happens\n     840    when OMPI was not configured --with-slurm and we weren't able\n     841    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 47415, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        47415\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     914    of its peer processes in the job will be killed properly.  You shou\n            ld\n     915    double check that everything has shut down cleanly.\n     916    \n     917    Local host: borax27\n     918    PID:        47415\n     919    -------------------------------------------------------------------\n            -------\n  >> 920    FAILED: Command exited with status 255:\n     921        './hello_oshmemfh'\n     922    -------------------------------------------------------------------\n            -------\n     923    The application appears to have been direct launched using \"srun\",\n     924    but OMPI was not built with SLURM support. This usually happens\n     925    when OMPI was not configured --with-slurm and we weren't able\n     926    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:47491] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     993    -------------------------------------------------------------------\n            -------\n     994    *** An error occurred in MPI_Init\n     995    *** on a NULL communicator\n     996    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     997    ***    and potentially your MPI job)\n     998    [borax27:47491] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 999    FAILED: Command exited with status 1:\n     1000       './hello_usempi'\n     1001   -------------------------------------------------------------------\n            -------\n     1002   The application appears to have been direct launched using \"srun\",\n     1003   but OMPI was not built with SLURM support. This usually happens\n     1004   when OMPI was not configured --with-slurm and we weren't able\n     1005   to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:47568] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1066    ------------------------------------------------------------------\n             --------\n     1067    *** An error occurred in MPI_Init\n     1068    *** on a NULL communicator\n     1069    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1070    ***    and potentially your MPI job)\n     1071    [borax27:47568] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1072    FAILED: Command exited with status 1:\n     1073        './hello_usempif08'\n     1074    ------------------------------------------------------------------\n             --------\n     1075    The application appears to have been direct launched using \"srun\",\n     1076    but OMPI was not built with SLURM support. This usually happens\n     1077    when OMPI was not configured --with-slurm and we weren't able\n     1078    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_circular_shift'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 47644, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        47644\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1145    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1146    double check that everything has shut down cleanly.\n     1147    \n     1148    Local host: borax27\n     1149    PID:        47644\n     1150    ------------------------------------------------------------------\n             --------\n  >> 1151    FAILED: Command exited with status 255:\n     1152        './oshmem_circular_shift'\n     1153    ------------------------------------------------------------------\n             --------\n     1154    The application appears to have been direct launched using \"srun\",\n     1155    but OMPI was not built with SLURM support. This usually happens\n     1156    when OMPI was not configured --with-slurm and we weren't able\n     1157    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_max_reduction'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 47720, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        47720\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1230    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1231    double check that everything has shut down cleanly.\n     1232    \n     1233    Local host: borax27\n     1234    PID:        47720\n     1235    ------------------------------------------------------------------\n             --------\n  >> 1236    FAILED: Command exited with status 255:\n     1237        './oshmem_max_reduction'\n     1238    ------------------------------------------------------------------\n             --------\n     1239    The application appears to have been direct launched using \"srun\",\n     1240    but OMPI was not built with SLURM support. This usually happens\n     1241    when OMPI was not configured --with-slurm and we weren't able\n     1242    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_shmalloc'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 47796, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        47796\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1315    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1316    double check that everything has shut down cleanly.\n     1317    \n     1318    Local host: borax27\n     1319    PID:        47796\n     1320    ------------------------------------------------------------------\n             --------\n  >> 1321    FAILED: Command exited with status 255:\n     1322        './oshmem_shmalloc'\n     1323    ------------------------------------------------------------------\n             --------\n     1324    The application appears to have been direct launched using \"srun\",\n     1325    but OMPI was not built with SLURM support. This usually happens\n     1326    when OMPI was not configured --with-slurm and we weren't able\n     1327    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_strided_puts'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 47875, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        47875\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1400    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1401    double check that everything has shut down cleanly.\n     1402    \n     1403    Local host: borax27\n     1404    PID:        47875\n     1405    ------------------------------------------------------------------\n             --------\n  >> 1406    FAILED: Command exited with status 255:\n     1407        './oshmem_strided_puts'\n     1408    ------------------------------------------------------------------\n             --------\n     1409    The application appears to have been direct launched using \"srun\",\n     1410    but OMPI was not built with SLURM support. This usually happens\n     1411    when OMPI was not configured --with-slurm and we weren't able\n     1412    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_symmetric_data'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 47951, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        47951\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1485    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1486    double check that everything has shut down cleanly.\n     1487    \n     1488    Local host: borax27\n     1489    PID:        47951\n     1490    ------------------------------------------------------------------\n             --------\n  >> 1491    FAILED: Command exited with status 255:\n     1492        './oshmem_symmetric_data'\n     1493    ------------------------------------------------------------------\n             --------\n     1494    The application appears to have been direct launched using \"srun\",\n     1495    but OMPI was not built with SLURM support. This usually happens\n     1496    when OMPI was not configured --with-slurm and we weren't able\n     1497    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:48027] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1564    ------------------------------------------------------------------\n             --------\n     1565    *** An error occurred in MPI_Init\n     1566    *** on a NULL communicator\n     1567    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1568    ***    and potentially your MPI job)\n     1569    [borax27:48027] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1570    FAILED: Command exited with status 1:\n     1571        './ring_c'\n     1572    ------------------------------------------------------------------\n             --------\n     1573    The application appears to have been direct launched using \"srun\",\n     1574    but OMPI was not built with SLURM support. This usually happens\n     1575    when OMPI was not configured --with-slurm and we weren't able\n     1576    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:48105] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1637    ------------------------------------------------------------------\n             --------\n     1638    *** An error occurred in MPI_Init\n     1639    *** on a NULL communicator\n     1640    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1641    ***    and potentially your MPI job)\n     1642    [borax27:48105] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1643    FAILED: Command exited with status 1:\n     1644        './ring_mpifh'\n     1645    ------------------------------------------------------------------\n             --------\n     1646    The application appears to have been direct launched using \"srun\",\n     1647    but OMPI was not built with SLURM support. This usually happens\n     1648    when OMPI was not configured --with-slurm and we weren't able\n     1649    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 48184, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        48184\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1716    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1717    double check that everything has shut down cleanly.\n     1718    \n     1719    Local host: borax27\n     1720    PID:        48184\n     1721    ------------------------------------------------------------------\n             --------\n  >> 1722    FAILED: Command exited with status 255:\n     1723        './ring_oshmem'\n     1724    ------------------------------------------------------------------\n             --------\n     1725    The application appears to have been direct launched using \"srun\",\n     1726    but OMPI was not built with SLURM support. This usually happens\n     1727    when OMPI was not configured --with-slurm and we weren't able\n     1728    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 48263, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        48263\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1801    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1802    double check that everything has shut down cleanly.\n     1803    \n     1804    Local host: borax27\n     1805    PID:        48263\n     1806    ------------------------------------------------------------------\n             --------\n  >> 1807    FAILED: Command exited with status 255:\n     1808        './ring_oshmemfh'\n     1809    ------------------------------------------------------------------\n             --------\n     1810    The application appears to have been direct launched using \"srun\",\n     1811    but OMPI was not built with SLURM support. This usually happens\n     1812    when OMPI was not configured --with-slurm and we weren't able\n     1813    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:48340] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1880    ------------------------------------------------------------------\n             --------\n     1881    *** An error occurred in MPI_Init\n     1882    *** on a NULL communicator\n     1883    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1884    ***    and potentially your MPI job)\n     1885    [borax27:48340] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1886    FAILED: Command exited with status 1:\n     1887        './ring_usempi'\n     1888    ------------------------------------------------------------------\n             --------\n     1889    The application appears to have been direct launched using \"srun\",\n     1890    but OMPI was not built with SLURM support. This usually happens\n     1891    when OMPI was not configured --with-slurm and we weren't able\n     1892    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:48416] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1953    ------------------------------------------------------------------\n             --------\n     1954    *** An error occurred in MPI_Init\n     1955    *** on a NULL communicator\n     1956    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1957    ***    and potentially your MPI job)\n     1958    [borax27:48416] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1959    FAILED: Command exited with status 1:\n     1960        './ring_usempif08'\n     1961    ------------------------------------------------------------------\n             --------\n     1962    The application appears to have been direct launched using \"srun\",\n     1963    but OMPI was not built with SLURM support. This usually happens\n     1964    when OMPI was not configured --with-slurm and we weren't able\n     1965    to discover a SLURM installation in the usual places.\n\n\n\n/p/vast1/build/spack/lib/spack/spack/build_environment.py:1061, in _setup_pkg_and_run:\n       1058        tb_string = traceback.format_exc()\n       1059\n       1060        # build up some context from the offending package so we can\n  >>   1061        # show that, too.\n       1062        package_context = get_package_context(tb)\n       1063\n       1064        logfile = None\n\nSee test log for details:\n  /g/g0/sochat1/.spack/test/3hutjcsrefi232cdtlufvumx3qkhe4rc/openmpi-3.1.0-qcuulae-test-out.txt\n\n==> Error: 1 test(s) in the suite failed.\n\n======================== 1 failed, 0 passed of 1 specs =========================\n",
                    "return_code": 1,
                    "prediction": false,
                    "command": "/p/vast1/build/spack/bin/spack test run /qcuula"
                }
            ],
            "symbolator": [
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                }
            ],
            "libabigail": [
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                }
            ]
        },
        "libs": {
            "spliced": [
                {
                    "dep": "pkgconf@1.3.10%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0"
                    ]
                }
            ],
            "original": [
                {
                    "dep": "pkgconf@1.8.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ]
        },
        "experiment": "experiment",
        "result": "splice-success",
        "success": true,
        "splice": "pkgconf@1.3.10",
        "package": "openmpi@3.1.0"
    },
    {
        "binaries": {
            "spliced": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps"
            ],
            "original": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-a326oktiaylrxchsd62y2czgo47ug6fr/bin/orte-top"
            ]
        },
        "predictions": {
            "spack-test": [
                {
                    "message": "==> Spack test 3hutjcsrefi232cdtlufvumx3qkhe4rc\n==> Testing package openmpi-3.1.0-qcuulae\n==> Error: TestFailure: 18 tests failed.\n\n\nCommand exited with status 1:\n    './hello_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:48800] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     592    -------------------------------------------------------------------\n            -------\n     593    *** An error occurred in MPI_Init\n     594    *** on a NULL communicator\n     595    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     596    ***    and potentially your MPI job)\n     597    [borax27:48800] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 598    FAILED: Command exited with status 1:\n     599        './hello_c'\n     600    -------------------------------------------------------------------\n            -------\n     601    The application appears to have been direct launched using \"srun\",\n     602    but OMPI was not built with SLURM support. This usually happens\n     603    when OMPI was not configured --with-slurm and we weren't able\n     604    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:48801] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     665    -------------------------------------------------------------------\n            -------\n     666    *** An error occurred in MPI_Init\n     667    *** on a NULL communicator\n     668    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     669    ***    and potentially your MPI job)\n     670    [borax27:48801] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 671    FAILED: Command exited with status 1:\n     672        './hello_mpifh'\n     673    -------------------------------------------------------------------\n            -------\n     674    The application appears to have been direct launched using \"srun\",\n     675    but OMPI was not built with SLURM support. This usually happens\n     676    when OMPI was not configured --with-slurm and we weren't able\n     677    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 48802, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        48802\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     744    of its peer processes in the job will be killed properly.  You shou\n            ld\n     745    double check that everything has shut down cleanly.\n     746    \n     747    Local host: borax27\n     748    PID:        48802\n     749    -------------------------------------------------------------------\n            -------\n  >> 750    FAILED: Command exited with status 255:\n     751        './hello_oshmem'\n     752    -------------------------------------------------------------------\n            -------\n     753    The application appears to have been direct launched using \"srun\",\n     754    but OMPI was not built with SLURM support. This usually happens\n     755    when OMPI was not configured --with-slurm and we weren't able\n     756    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemcxx'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 48879, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        48879\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     829    of its peer processes in the job will be killed properly.  You shou\n            ld\n     830    double check that everything has shut down cleanly.\n     831    \n     832    Local host: borax27\n     833    PID:        48879\n     834    -------------------------------------------------------------------\n            -------\n  >> 835    FAILED: Command exited with status 255:\n     836        './hello_oshmemcxx'\n     837    -------------------------------------------------------------------\n            -------\n     838    The application appears to have been direct launched using \"srun\",\n     839    but OMPI was not built with SLURM support. This usually happens\n     840    when OMPI was not configured --with-slurm and we weren't able\n     841    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 48955, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        48955\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     914    of its peer processes in the job will be killed properly.  You shou\n            ld\n     915    double check that everything has shut down cleanly.\n     916    \n     917    Local host: borax27\n     918    PID:        48955\n     919    -------------------------------------------------------------------\n            -------\n  >> 920    FAILED: Command exited with status 255:\n     921        './hello_oshmemfh'\n     922    -------------------------------------------------------------------\n            -------\n     923    The application appears to have been direct launched using \"srun\",\n     924    but OMPI was not built with SLURM support. This usually happens\n     925    when OMPI was not configured --with-slurm and we weren't able\n     926    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:49031] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     993    -------------------------------------------------------------------\n            -------\n     994    *** An error occurred in MPI_Init\n     995    *** on a NULL communicator\n     996    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     997    ***    and potentially your MPI job)\n     998    [borax27:49031] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 999    FAILED: Command exited with status 1:\n     1000       './hello_usempi'\n     1001   -------------------------------------------------------------------\n            -------\n     1002   The application appears to have been direct launched using \"srun\",\n     1003   but OMPI was not built with SLURM support. This usually happens\n     1004   when OMPI was not configured --with-slurm and we weren't able\n     1005   to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:49108] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1066    ------------------------------------------------------------------\n             --------\n     1067    *** An error occurred in MPI_Init\n     1068    *** on a NULL communicator\n     1069    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1070    ***    and potentially your MPI job)\n     1071    [borax27:49108] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1072    FAILED: Command exited with status 1:\n     1073        './hello_usempif08'\n     1074    ------------------------------------------------------------------\n             --------\n     1075    The application appears to have been direct launched using \"srun\",\n     1076    but OMPI was not built with SLURM support. This usually happens\n     1077    when OMPI was not configured --with-slurm and we weren't able\n     1078    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_circular_shift'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 49185, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        49185\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1145    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1146    double check that everything has shut down cleanly.\n     1147    \n     1148    Local host: borax27\n     1149    PID:        49185\n     1150    ------------------------------------------------------------------\n             --------\n  >> 1151    FAILED: Command exited with status 255:\n     1152        './oshmem_circular_shift'\n     1153    ------------------------------------------------------------------\n             --------\n     1154    The application appears to have been direct launched using \"srun\",\n     1155    but OMPI was not built with SLURM support. This usually happens\n     1156    when OMPI was not configured --with-slurm and we weren't able\n     1157    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_max_reduction'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 49263, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        49263\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1230    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1231    double check that everything has shut down cleanly.\n     1232    \n     1233    Local host: borax27\n     1234    PID:        49263\n     1235    ------------------------------------------------------------------\n             --------\n  >> 1236    FAILED: Command exited with status 255:\n     1237        './oshmem_max_reduction'\n     1238    ------------------------------------------------------------------\n             --------\n     1239    The application appears to have been direct launched using \"srun\",\n     1240    but OMPI was not built with SLURM support. This usually happens\n     1241    when OMPI was not configured --with-slurm and we weren't able\n     1242    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_shmalloc'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 49340, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        49340\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1315    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1316    double check that everything has shut down cleanly.\n     1317    \n     1318    Local host: borax27\n     1319    PID:        49340\n     1320    ------------------------------------------------------------------\n             --------\n  >> 1321    FAILED: Command exited with status 255:\n     1322        './oshmem_shmalloc'\n     1323    ------------------------------------------------------------------\n             --------\n     1324    The application appears to have been direct launched using \"srun\",\n     1325    but OMPI was not built with SLURM support. This usually happens\n     1326    when OMPI was not configured --with-slurm and we weren't able\n     1327    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_strided_puts'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 49418, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        49418\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1400    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1401    double check that everything has shut down cleanly.\n     1402    \n     1403    Local host: borax27\n     1404    PID:        49418\n     1405    ------------------------------------------------------------------\n             --------\n  >> 1406    FAILED: Command exited with status 255:\n     1407        './oshmem_strided_puts'\n     1408    ------------------------------------------------------------------\n             --------\n     1409    The application appears to have been direct launched using \"srun\",\n     1410    but OMPI was not built with SLURM support. This usually happens\n     1411    when OMPI was not configured --with-slurm and we weren't able\n     1412    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_symmetric_data'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 49494, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        49494\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1485    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1486    double check that everything has shut down cleanly.\n     1487    \n     1488    Local host: borax27\n     1489    PID:        49494\n     1490    ------------------------------------------------------------------\n             --------\n  >> 1491    FAILED: Command exited with status 255:\n     1492        './oshmem_symmetric_data'\n     1493    ------------------------------------------------------------------\n             --------\n     1494    The application appears to have been direct launched using \"srun\",\n     1495    but OMPI was not built with SLURM support. This usually happens\n     1496    when OMPI was not configured --with-slurm and we weren't able\n     1497    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:49571] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1564    ------------------------------------------------------------------\n             --------\n     1565    *** An error occurred in MPI_Init\n     1566    *** on a NULL communicator\n     1567    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1568    ***    and potentially your MPI job)\n     1569    [borax27:49571] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1570    FAILED: Command exited with status 1:\n     1571        './ring_c'\n     1572    ------------------------------------------------------------------\n             --------\n     1573    The application appears to have been direct launched using \"srun\",\n     1574    but OMPI was not built with SLURM support. This usually happens\n     1575    when OMPI was not configured --with-slurm and we weren't able\n     1576    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:49647] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1637    ------------------------------------------------------------------\n             --------\n     1638    *** An error occurred in MPI_Init\n     1639    *** on a NULL communicator\n     1640    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1641    ***    and potentially your MPI job)\n     1642    [borax27:49647] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1643    FAILED: Command exited with status 1:\n     1644        './ring_mpifh'\n     1645    ------------------------------------------------------------------\n             --------\n     1646    The application appears to have been direct launched using \"srun\",\n     1647    but OMPI was not built with SLURM support. This usually happens\n     1648    when OMPI was not configured --with-slurm and we weren't able\n     1649    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 49723, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        49723\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1716    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1717    double check that everything has shut down cleanly.\n     1718    \n     1719    Local host: borax27\n     1720    PID:        49723\n     1721    ------------------------------------------------------------------\n             --------\n  >> 1722    FAILED: Command exited with status 255:\n     1723        './ring_oshmem'\n     1724    ------------------------------------------------------------------\n             --------\n     1725    The application appears to have been direct launched using \"srun\",\n     1726    but OMPI was not built with SLURM support. This usually happens\n     1727    when OMPI was not configured --with-slurm and we weren't able\n     1728    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 49799, host=borax27) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax27\nPID:        49799\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1801    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1802    double check that everything has shut down cleanly.\n     1803    \n     1804    Local host: borax27\n     1805    PID:        49799\n     1806    ------------------------------------------------------------------\n             --------\n  >> 1807    FAILED: Command exited with status 255:\n     1808        './ring_oshmemfh'\n     1809    ------------------------------------------------------------------\n             --------\n     1810    The application appears to have been direct launched using \"srun\",\n     1811    but OMPI was not built with SLURM support. This usually happens\n     1812    when OMPI was not configured --with-slurm and we weren't able\n     1813    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:49877] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1880    ------------------------------------------------------------------\n             --------\n     1881    *** An error occurred in MPI_Init\n     1882    *** on a NULL communicator\n     1883    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1884    ***    and potentially your MPI job)\n     1885    [borax27:49877] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1886    FAILED: Command exited with status 1:\n     1887        './ring_usempi'\n     1888    ------------------------------------------------------------------\n             --------\n     1889    The application appears to have been direct launched using \"srun\",\n     1890    but OMPI was not built with SLURM support. This usually happens\n     1891    when OMPI was not configured --with-slurm and we weren't able\n     1892    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax27:49953] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1953    ------------------------------------------------------------------\n             --------\n     1954    *** An error occurred in MPI_Init\n     1955    *** on a NULL communicator\n     1956    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1957    ***    and potentially your MPI job)\n     1958    [borax27:49953] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1959    FAILED: Command exited with status 1:\n     1960        './ring_usempif08'\n     1961    ------------------------------------------------------------------\n             --------\n     1962    The application appears to have been direct launched using \"srun\",\n     1963    but OMPI was not built with SLURM support. This usually happens\n     1964    when OMPI was not configured --with-slurm and we weren't able\n     1965    to discover a SLURM installation in the usual places.\n\n\n\n/p/vast1/build/spack/lib/spack/spack/build_environment.py:1061, in _setup_pkg_and_run:\n       1058        tb_string = traceback.format_exc()\n       1059\n       1060        # build up some context from the offending package so we can\n  >>   1061        # show that, too.\n       1062        package_context = get_package_context(tb)\n       1063\n       1064        logfile = None\n\nSee test log for details:\n  /g/g0/sochat1/.spack/test/3hutjcsrefi232cdtlufvumx3qkhe4rc/openmpi-3.1.0-qcuulae-test-out.txt\n\n==> Error: 1 test(s) in the suite failed.\n\n======================== 1 failed, 0 passed of 1 specs =========================\n",
                    "return_code": 1,
                    "prediction": false,
                    "command": "/p/vast1/build/spack/bin/spack test run /qcuula"
                }
            ],
            "symbolator": [
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                }
            ],
            "libabigail": [
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.0-qcuulaeuph4q4av2yhiyvxuka754gzdn/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                }
            ]
        },
        "libs": {
            "spliced": [
                {
                    "dep": "pkgconf@1.3.8%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0"
                    ]
                }
            ],
            "original": [
                {
                    "dep": "pkgconf@1.8.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ]
        },
        "experiment": "experiment",
        "result": "splice-success",
        "success": true,
        "splice": "pkgconf@1.3.8",
        "package": "openmpi@3.1.0"
    }
]