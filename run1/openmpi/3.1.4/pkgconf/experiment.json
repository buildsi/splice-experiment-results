[
    {
        "binaries": {
            "spliced": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper"
            ],
            "original": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orted"
            ]
        },
        "predictions": {
            "spack-test": [
                {
                    "message": "==> Spack test wjkcucjdda4pthihxgsnrwpoy4cqlmqe\n==> Testing package openmpi-3.1.4-xf5lntw\n==> Error: TestFailure: 18 tests failed.\n\n\nCommand exited with status 1:\n    './hello_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:38243] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     589    -------------------------------------------------------------------\n            -------\n     590    *** An error occurred in MPI_Init\n     591    *** on a NULL communicator\n     592    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     593    ***    and potentially your MPI job)\n     594    [borax38:38243] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 595    FAILED: Command exited with status 1:\n     596        './hello_c'\n     597    -------------------------------------------------------------------\n            -------\n     598    The application appears to have been direct launched using \"srun\",\n     599    but OMPI was not built with SLURM support. This usually happens\n     600    when OMPI was not configured --with-slurm and we weren't able\n     601    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:38244] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     662    -------------------------------------------------------------------\n            -------\n     663    *** An error occurred in MPI_Init\n     664    *** on a NULL communicator\n     665    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     666    ***    and potentially your MPI job)\n     667    [borax38:38244] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 668    FAILED: Command exited with status 1:\n     669        './hello_mpifh'\n     670    -------------------------------------------------------------------\n            -------\n     671    The application appears to have been direct launched using \"srun\",\n     672    but OMPI was not built with SLURM support. This usually happens\n     673    when OMPI was not configured --with-slurm and we weren't able\n     674    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 38245, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        38245\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     741    of its peer processes in the job will be killed properly.  You shou\n            ld\n     742    double check that everything has shut down cleanly.\n     743    \n     744    Local host: borax38\n     745    PID:        38245\n     746    -------------------------------------------------------------------\n            -------\n  >> 747    FAILED: Command exited with status 255:\n     748        './hello_oshmem'\n     749    -------------------------------------------------------------------\n            -------\n     750    The application appears to have been direct launched using \"srun\",\n     751    but OMPI was not built with SLURM support. This usually happens\n     752    when OMPI was not configured --with-slurm and we weren't able\n     753    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemcxx'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 38322, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        38322\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     826    of its peer processes in the job will be killed properly.  You shou\n            ld\n     827    double check that everything has shut down cleanly.\n     828    \n     829    Local host: borax38\n     830    PID:        38322\n     831    -------------------------------------------------------------------\n            -------\n  >> 832    FAILED: Command exited with status 255:\n     833        './hello_oshmemcxx'\n     834    -------------------------------------------------------------------\n            -------\n     835    The application appears to have been direct launched using \"srun\",\n     836    but OMPI was not built with SLURM support. This usually happens\n     837    when OMPI was not configured --with-slurm and we weren't able\n     838    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 38398, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        38398\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     911    of its peer processes in the job will be killed properly.  You shou\n            ld\n     912    double check that everything has shut down cleanly.\n     913    \n     914    Local host: borax38\n     915    PID:        38398\n     916    -------------------------------------------------------------------\n            -------\n  >> 917    FAILED: Command exited with status 255:\n     918        './hello_oshmemfh'\n     919    -------------------------------------------------------------------\n            -------\n     920    The application appears to have been direct launched using \"srun\",\n     921    but OMPI was not built with SLURM support. This usually happens\n     922    when OMPI was not configured --with-slurm and we weren't able\n     923    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:38475] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     990    -------------------------------------------------------------------\n            -------\n     991    *** An error occurred in MPI_Init\n     992    *** on a NULL communicator\n     993    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     994    ***    and potentially your MPI job)\n     995    [borax38:38475] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 996    FAILED: Command exited with status 1:\n     997        './hello_usempi'\n     998    -------------------------------------------------------------------\n            -------\n     999    The application appears to have been direct launched using \"srun\",\n     1000   but OMPI was not built with SLURM support. This usually happens\n     1001   when OMPI was not configured --with-slurm and we weren't able\n     1002   to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:38552] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1063    ------------------------------------------------------------------\n             --------\n     1064    *** An error occurred in MPI_Init\n     1065    *** on a NULL communicator\n     1066    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1067    ***    and potentially your MPI job)\n     1068    [borax38:38552] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1069    FAILED: Command exited with status 1:\n     1070        './hello_usempif08'\n     1071    ------------------------------------------------------------------\n             --------\n     1072    The application appears to have been direct launched using \"srun\",\n     1073    but OMPI was not built with SLURM support. This usually happens\n     1074    when OMPI was not configured --with-slurm and we weren't able\n     1075    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_circular_shift'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 38628, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        38628\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1142    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1143    double check that everything has shut down cleanly.\n     1144    \n     1145    Local host: borax38\n     1146    PID:        38628\n     1147    ------------------------------------------------------------------\n             --------\n  >> 1148    FAILED: Command exited with status 255:\n     1149        './oshmem_circular_shift'\n     1150    ------------------------------------------------------------------\n             --------\n     1151    The application appears to have been direct launched using \"srun\",\n     1152    but OMPI was not built with SLURM support. This usually happens\n     1153    when OMPI was not configured --with-slurm and we weren't able\n     1154    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_max_reduction'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 38704, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        38704\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1227    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1228    double check that everything has shut down cleanly.\n     1229    \n     1230    Local host: borax38\n     1231    PID:        38704\n     1232    ------------------------------------------------------------------\n             --------\n  >> 1233    FAILED: Command exited with status 255:\n     1234        './oshmem_max_reduction'\n     1235    ------------------------------------------------------------------\n             --------\n     1236    The application appears to have been direct launched using \"srun\",\n     1237    but OMPI was not built with SLURM support. This usually happens\n     1238    when OMPI was not configured --with-slurm and we weren't able\n     1239    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_shmalloc'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 38781, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        38781\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1312    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1313    double check that everything has shut down cleanly.\n     1314    \n     1315    Local host: borax38\n     1316    PID:        38781\n     1317    ------------------------------------------------------------------\n             --------\n  >> 1318    FAILED: Command exited with status 255:\n     1319        './oshmem_shmalloc'\n     1320    ------------------------------------------------------------------\n             --------\n     1321    The application appears to have been direct launched using \"srun\",\n     1322    but OMPI was not built with SLURM support. This usually happens\n     1323    when OMPI was not configured --with-slurm and we weren't able\n     1324    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_strided_puts'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 38863, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        38863\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1397    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1398    double check that everything has shut down cleanly.\n     1399    \n     1400    Local host: borax38\n     1401    PID:        38863\n     1402    ------------------------------------------------------------------\n             --------\n  >> 1403    FAILED: Command exited with status 255:\n     1404        './oshmem_strided_puts'\n     1405    ------------------------------------------------------------------\n             --------\n     1406    The application appears to have been direct launched using \"srun\",\n     1407    but OMPI was not built with SLURM support. This usually happens\n     1408    when OMPI was not configured --with-slurm and we weren't able\n     1409    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_symmetric_data'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 38944, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        38944\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1482    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1483    double check that everything has shut down cleanly.\n     1484    \n     1485    Local host: borax38\n     1486    PID:        38944\n     1487    ------------------------------------------------------------------\n             --------\n  >> 1488    FAILED: Command exited with status 255:\n     1489        './oshmem_symmetric_data'\n     1490    ------------------------------------------------------------------\n             --------\n     1491    The application appears to have been direct launched using \"srun\",\n     1492    but OMPI was not built with SLURM support. This usually happens\n     1493    when OMPI was not configured --with-slurm and we weren't able\n     1494    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:39021] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1561    ------------------------------------------------------------------\n             --------\n     1562    *** An error occurred in MPI_Init\n     1563    *** on a NULL communicator\n     1564    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1565    ***    and potentially your MPI job)\n     1566    [borax38:39021] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1567    FAILED: Command exited with status 1:\n     1568        './ring_c'\n     1569    ------------------------------------------------------------------\n             --------\n     1570    The application appears to have been direct launched using \"srun\",\n     1571    but OMPI was not built with SLURM support. This usually happens\n     1572    when OMPI was not configured --with-slurm and we weren't able\n     1573    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:39099] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1634    ------------------------------------------------------------------\n             --------\n     1635    *** An error occurred in MPI_Init\n     1636    *** on a NULL communicator\n     1637    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1638    ***    and potentially your MPI job)\n     1639    [borax38:39099] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1640    FAILED: Command exited with status 1:\n     1641        './ring_mpifh'\n     1642    ------------------------------------------------------------------\n             --------\n     1643    The application appears to have been direct launched using \"srun\",\n     1644    but OMPI was not built with SLURM support. This usually happens\n     1645    when OMPI was not configured --with-slurm and we weren't able\n     1646    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 39178, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        39178\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1713    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1714    double check that everything has shut down cleanly.\n     1715    \n     1716    Local host: borax38\n     1717    PID:        39178\n     1718    ------------------------------------------------------------------\n             --------\n  >> 1719    FAILED: Command exited with status 255:\n     1720        './ring_oshmem'\n     1721    ------------------------------------------------------------------\n             --------\n     1722    The application appears to have been direct launched using \"srun\",\n     1723    but OMPI was not built with SLURM support. This usually happens\n     1724    when OMPI was not configured --with-slurm and we weren't able\n     1725    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 39255, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        39255\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1798    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1799    double check that everything has shut down cleanly.\n     1800    \n     1801    Local host: borax38\n     1802    PID:        39255\n     1803    ------------------------------------------------------------------\n             --------\n  >> 1804    FAILED: Command exited with status 255:\n     1805        './ring_oshmemfh'\n     1806    ------------------------------------------------------------------\n             --------\n     1807    The application appears to have been direct launched using \"srun\",\n     1808    but OMPI was not built with SLURM support. This usually happens\n     1809    when OMPI was not configured --with-slurm and we weren't able\n     1810    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:39332] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1877    ------------------------------------------------------------------\n             --------\n     1878    *** An error occurred in MPI_Init\n     1879    *** on a NULL communicator\n     1880    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1881    ***    and potentially your MPI job)\n     1882    [borax38:39332] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1883    FAILED: Command exited with status 1:\n     1884        './ring_usempi'\n     1885    ------------------------------------------------------------------\n             --------\n     1886    The application appears to have been direct launched using \"srun\",\n     1887    but OMPI was not built with SLURM support. This usually happens\n     1888    when OMPI was not configured --with-slurm and we weren't able\n     1889    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:39409] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1950    ------------------------------------------------------------------\n             --------\n     1951    *** An error occurred in MPI_Init\n     1952    *** on a NULL communicator\n     1953    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1954    ***    and potentially your MPI job)\n     1955    [borax38:39409] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1956    FAILED: Command exited with status 1:\n     1957        './ring_usempif08'\n     1958    ------------------------------------------------------------------\n             --------\n     1959    The application appears to have been direct launched using \"srun\",\n     1960    but OMPI was not built with SLURM support. This usually happens\n     1961    when OMPI was not configured --with-slurm and we weren't able\n     1962    to discover a SLURM installation in the usual places.\n\n\n\n/p/vast1/build/spack/lib/spack/spack/build_environment.py:1061, in _setup_pkg_and_run:\n       1058        tb_string = traceback.format_exc()\n       1059\n       1060        # build up some context from the offending package so we can\n  >>   1061        # show that, too.\n       1062        package_context = get_package_context(tb)\n       1063\n       1064        logfile = None\n\nSee test log for details:\n  /g/g0/sochat1/.spack/test/wjkcucjdda4pthihxgsnrwpoy4cqlmqe/openmpi-3.1.4-xf5lntw-test-out.txt\n\n==> Error: 1 test(s) in the suite failed.\n\n======================== 1 failed, 0 passed of 1 specs =========================\n",
                    "return_code": 1,
                    "prediction": false,
                    "command": "/p/vast1/build/spack/bin/spack test run /xf5lnt"
                }
            ],
            "symbolator": [
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ],
            "libabigail": [
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ]
        },
        "libs": {
            "spliced": [
                {
                    "dep": "pkgconf@1.8.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ],
            "original": [
                {
                    "dep": "pkgconf@1.8.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ]
        },
        "experiment": "experiment",
        "result": "splice-success",
        "success": true,
        "splice": "pkgconf@1.8.0",
        "package": "openmpi@3.1.4"
    },
    {
        "binaries": {
            "spliced": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper"
            ],
            "original": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orted"
            ]
        },
        "predictions": {
            "spack-test": [
                {
                    "message": "==> Spack test wjkcucjdda4pthihxgsnrwpoy4cqlmqe\n==> Testing package openmpi-3.1.4-xf5lntw\n==> Error: TestFailure: 18 tests failed.\n\n\nCommand exited with status 1:\n    './hello_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:39790] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     592    -------------------------------------------------------------------\n            -------\n     593    *** An error occurred in MPI_Init\n     594    *** on a NULL communicator\n     595    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     596    ***    and potentially your MPI job)\n     597    [borax38:39790] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 598    FAILED: Command exited with status 1:\n     599        './hello_c'\n     600    -------------------------------------------------------------------\n            -------\n     601    The application appears to have been direct launched using \"srun\",\n     602    but OMPI was not built with SLURM support. This usually happens\n     603    when OMPI was not configured --with-slurm and we weren't able\n     604    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:39792] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     665    -------------------------------------------------------------------\n            -------\n     666    *** An error occurred in MPI_Init\n     667    *** on a NULL communicator\n     668    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     669    ***    and potentially your MPI job)\n     670    [borax38:39792] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 671    FAILED: Command exited with status 1:\n     672        './hello_mpifh'\n     673    -------------------------------------------------------------------\n            -------\n     674    The application appears to have been direct launched using \"srun\",\n     675    but OMPI was not built with SLURM support. This usually happens\n     676    when OMPI was not configured --with-slurm and we weren't able\n     677    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 39793, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        39793\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     744    of its peer processes in the job will be killed properly.  You shou\n            ld\n     745    double check that everything has shut down cleanly.\n     746    \n     747    Local host: borax38\n     748    PID:        39793\n     749    -------------------------------------------------------------------\n            -------\n  >> 750    FAILED: Command exited with status 255:\n     751        './hello_oshmem'\n     752    -------------------------------------------------------------------\n            -------\n     753    The application appears to have been direct launched using \"srun\",\n     754    but OMPI was not built with SLURM support. This usually happens\n     755    when OMPI was not configured --with-slurm and we weren't able\n     756    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemcxx'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 39869, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        39869\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     829    of its peer processes in the job will be killed properly.  You shou\n            ld\n     830    double check that everything has shut down cleanly.\n     831    \n     832    Local host: borax38\n     833    PID:        39869\n     834    -------------------------------------------------------------------\n            -------\n  >> 835    FAILED: Command exited with status 255:\n     836        './hello_oshmemcxx'\n     837    -------------------------------------------------------------------\n            -------\n     838    The application appears to have been direct launched using \"srun\",\n     839    but OMPI was not built with SLURM support. This usually happens\n     840    when OMPI was not configured --with-slurm and we weren't able\n     841    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 39945, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        39945\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     914    of its peer processes in the job will be killed properly.  You shou\n            ld\n     915    double check that everything has shut down cleanly.\n     916    \n     917    Local host: borax38\n     918    PID:        39945\n     919    -------------------------------------------------------------------\n            -------\n  >> 920    FAILED: Command exited with status 255:\n     921        './hello_oshmemfh'\n     922    -------------------------------------------------------------------\n            -------\n     923    The application appears to have been direct launched using \"srun\",\n     924    but OMPI was not built with SLURM support. This usually happens\n     925    when OMPI was not configured --with-slurm and we weren't able\n     926    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:40021] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     993    -------------------------------------------------------------------\n            -------\n     994    *** An error occurred in MPI_Init\n     995    *** on a NULL communicator\n     996    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     997    ***    and potentially your MPI job)\n     998    [borax38:40021] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 999    FAILED: Command exited with status 1:\n     1000       './hello_usempi'\n     1001   -------------------------------------------------------------------\n            -------\n     1002   The application appears to have been direct launched using \"srun\",\n     1003   but OMPI was not built with SLURM support. This usually happens\n     1004   when OMPI was not configured --with-slurm and we weren't able\n     1005   to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:40098] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1066    ------------------------------------------------------------------\n             --------\n     1067    *** An error occurred in MPI_Init\n     1068    *** on a NULL communicator\n     1069    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1070    ***    and potentially your MPI job)\n     1071    [borax38:40098] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1072    FAILED: Command exited with status 1:\n     1073        './hello_usempif08'\n     1074    ------------------------------------------------------------------\n             --------\n     1075    The application appears to have been direct launched using \"srun\",\n     1076    but OMPI was not built with SLURM support. This usually happens\n     1077    when OMPI was not configured --with-slurm and we weren't able\n     1078    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_circular_shift'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 40174, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        40174\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1145    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1146    double check that everything has shut down cleanly.\n     1147    \n     1148    Local host: borax38\n     1149    PID:        40174\n     1150    ------------------------------------------------------------------\n             --------\n  >> 1151    FAILED: Command exited with status 255:\n     1152        './oshmem_circular_shift'\n     1153    ------------------------------------------------------------------\n             --------\n     1154    The application appears to have been direct launched using \"srun\",\n     1155    but OMPI was not built with SLURM support. This usually happens\n     1156    when OMPI was not configured --with-slurm and we weren't able\n     1157    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_max_reduction'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 40250, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        40250\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1230    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1231    double check that everything has shut down cleanly.\n     1232    \n     1233    Local host: borax38\n     1234    PID:        40250\n     1235    ------------------------------------------------------------------\n             --------\n  >> 1236    FAILED: Command exited with status 255:\n     1237        './oshmem_max_reduction'\n     1238    ------------------------------------------------------------------\n             --------\n     1239    The application appears to have been direct launched using \"srun\",\n     1240    but OMPI was not built with SLURM support. This usually happens\n     1241    when OMPI was not configured --with-slurm and we weren't able\n     1242    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_shmalloc'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 40326, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        40326\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1315    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1316    double check that everything has shut down cleanly.\n     1317    \n     1318    Local host: borax38\n     1319    PID:        40326\n     1320    ------------------------------------------------------------------\n             --------\n  >> 1321    FAILED: Command exited with status 255:\n     1322        './oshmem_shmalloc'\n     1323    ------------------------------------------------------------------\n             --------\n     1324    The application appears to have been direct launched using \"srun\",\n     1325    but OMPI was not built with SLURM support. This usually happens\n     1326    when OMPI was not configured --with-slurm and we weren't able\n     1327    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_strided_puts'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 40403, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        40403\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1400    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1401    double check that everything has shut down cleanly.\n     1402    \n     1403    Local host: borax38\n     1404    PID:        40403\n     1405    ------------------------------------------------------------------\n             --------\n  >> 1406    FAILED: Command exited with status 255:\n     1407        './oshmem_strided_puts'\n     1408    ------------------------------------------------------------------\n             --------\n     1409    The application appears to have been direct launched using \"srun\",\n     1410    but OMPI was not built with SLURM support. This usually happens\n     1411    when OMPI was not configured --with-slurm and we weren't able\n     1412    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_symmetric_data'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 40479, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        40479\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1485    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1486    double check that everything has shut down cleanly.\n     1487    \n     1488    Local host: borax38\n     1489    PID:        40479\n     1490    ------------------------------------------------------------------\n             --------\n  >> 1491    FAILED: Command exited with status 255:\n     1492        './oshmem_symmetric_data'\n     1493    ------------------------------------------------------------------\n             --------\n     1494    The application appears to have been direct launched using \"srun\",\n     1495    but OMPI was not built with SLURM support. This usually happens\n     1496    when OMPI was not configured --with-slurm and we weren't able\n     1497    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:40555] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1564    ------------------------------------------------------------------\n             --------\n     1565    *** An error occurred in MPI_Init\n     1566    *** on a NULL communicator\n     1567    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1568    ***    and potentially your MPI job)\n     1569    [borax38:40555] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1570    FAILED: Command exited with status 1:\n     1571        './ring_c'\n     1572    ------------------------------------------------------------------\n             --------\n     1573    The application appears to have been direct launched using \"srun\",\n     1574    but OMPI was not built with SLURM support. This usually happens\n     1575    when OMPI was not configured --with-slurm and we weren't able\n     1576    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:40631] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1637    ------------------------------------------------------------------\n             --------\n     1638    *** An error occurred in MPI_Init\n     1639    *** on a NULL communicator\n     1640    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1641    ***    and potentially your MPI job)\n     1642    [borax38:40631] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1643    FAILED: Command exited with status 1:\n     1644        './ring_mpifh'\n     1645    ------------------------------------------------------------------\n             --------\n     1646    The application appears to have been direct launched using \"srun\",\n     1647    but OMPI was not built with SLURM support. This usually happens\n     1648    when OMPI was not configured --with-slurm and we weren't able\n     1649    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 40707, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        40707\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1716    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1717    double check that everything has shut down cleanly.\n     1718    \n     1719    Local host: borax38\n     1720    PID:        40707\n     1721    ------------------------------------------------------------------\n             --------\n  >> 1722    FAILED: Command exited with status 255:\n     1723        './ring_oshmem'\n     1724    ------------------------------------------------------------------\n             --------\n     1725    The application appears to have been direct launched using \"srun\",\n     1726    but OMPI was not built with SLURM support. This usually happens\n     1727    when OMPI was not configured --with-slurm and we weren't able\n     1728    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 40784, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        40784\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1801    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1802    double check that everything has shut down cleanly.\n     1803    \n     1804    Local host: borax38\n     1805    PID:        40784\n     1806    ------------------------------------------------------------------\n             --------\n  >> 1807    FAILED: Command exited with status 255:\n     1808        './ring_oshmemfh'\n     1809    ------------------------------------------------------------------\n             --------\n     1810    The application appears to have been direct launched using \"srun\",\n     1811    but OMPI was not built with SLURM support. This usually happens\n     1812    when OMPI was not configured --with-slurm and we weren't able\n     1813    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:40860] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1880    ------------------------------------------------------------------\n             --------\n     1881    *** An error occurred in MPI_Init\n     1882    *** on a NULL communicator\n     1883    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1884    ***    and potentially your MPI job)\n     1885    [borax38:40860] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1886    FAILED: Command exited with status 1:\n     1887        './ring_usempi'\n     1888    ------------------------------------------------------------------\n             --------\n     1889    The application appears to have been direct launched using \"srun\",\n     1890    but OMPI was not built with SLURM support. This usually happens\n     1891    when OMPI was not configured --with-slurm and we weren't able\n     1892    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:40936] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1953    ------------------------------------------------------------------\n             --------\n     1954    *** An error occurred in MPI_Init\n     1955    *** on a NULL communicator\n     1956    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1957    ***    and potentially your MPI job)\n     1958    [borax38:40936] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1959    FAILED: Command exited with status 1:\n     1960        './ring_usempif08'\n     1961    ------------------------------------------------------------------\n             --------\n     1962    The application appears to have been direct launched using \"srun\",\n     1963    but OMPI was not built with SLURM support. This usually happens\n     1964    when OMPI was not configured --with-slurm and we weren't able\n     1965    to discover a SLURM installation in the usual places.\n\n\n\n/p/vast1/build/spack/lib/spack/spack/build_environment.py:1061, in _setup_pkg_and_run:\n       1058        tb_string = traceback.format_exc()\n       1059\n       1060        # build up some context from the offending package so we can\n  >>   1061        # show that, too.\n       1062        package_context = get_package_context(tb)\n       1063\n       1064        logfile = None\n\nSee test log for details:\n  /g/g0/sochat1/.spack/test/wjkcucjdda4pthihxgsnrwpoy4cqlmqe/openmpi-3.1.4-xf5lntw-test-out.txt\n\n==> Error: 1 test(s) in the suite failed.\n\n======================== 1 failed, 0 passed of 1 specs =========================\n",
                    "return_code": 1,
                    "prediction": false,
                    "command": "/p/vast1/build/spack/bin/spack test run /xf5lnt"
                }
            ],
            "symbolator": [
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ],
            "libabigail": [
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ]
        },
        "libs": {
            "spliced": [
                {
                    "dep": "pkgconf@1.7.4%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.4-undwekwfnoadmtca2gklapyszqo6fbqc/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ],
            "original": [
                {
                    "dep": "pkgconf@1.8.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ]
        },
        "experiment": "experiment",
        "result": "splice-success",
        "success": true,
        "splice": "pkgconf@1.7.4",
        "package": "openmpi@3.1.4"
    },
    {
        "binaries": {
            "spliced": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper"
            ],
            "original": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orted"
            ]
        },
        "predictions": {
            "spack-test": [
                {
                    "message": "==> Spack test wjkcucjdda4pthihxgsnrwpoy4cqlmqe\n==> Testing package openmpi-3.1.4-xf5lntw\n==> Error: TestFailure: 18 tests failed.\n\n\nCommand exited with status 1:\n    './hello_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:41320] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     592    -------------------------------------------------------------------\n            -------\n     593    *** An error occurred in MPI_Init\n     594    *** on a NULL communicator\n     595    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     596    ***    and potentially your MPI job)\n     597    [borax38:41320] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 598    FAILED: Command exited with status 1:\n     599        './hello_c'\n     600    -------------------------------------------------------------------\n            -------\n     601    The application appears to have been direct launched using \"srun\",\n     602    but OMPI was not built with SLURM support. This usually happens\n     603    when OMPI was not configured --with-slurm and we weren't able\n     604    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:41321] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     665    -------------------------------------------------------------------\n            -------\n     666    *** An error occurred in MPI_Init\n     667    *** on a NULL communicator\n     668    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     669    ***    and potentially your MPI job)\n     670    [borax38:41321] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 671    FAILED: Command exited with status 1:\n     672        './hello_mpifh'\n     673    -------------------------------------------------------------------\n            -------\n     674    The application appears to have been direct launched using \"srun\",\n     675    but OMPI was not built with SLURM support. This usually happens\n     676    when OMPI was not configured --with-slurm and we weren't able\n     677    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 41322, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        41322\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     744    of its peer processes in the job will be killed properly.  You shou\n            ld\n     745    double check that everything has shut down cleanly.\n     746    \n     747    Local host: borax38\n     748    PID:        41322\n     749    -------------------------------------------------------------------\n            -------\n  >> 750    FAILED: Command exited with status 255:\n     751        './hello_oshmem'\n     752    -------------------------------------------------------------------\n            -------\n     753    The application appears to have been direct launched using \"srun\",\n     754    but OMPI was not built with SLURM support. This usually happens\n     755    when OMPI was not configured --with-slurm and we weren't able\n     756    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemcxx'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 41398, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        41398\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     829    of its peer processes in the job will be killed properly.  You shou\n            ld\n     830    double check that everything has shut down cleanly.\n     831    \n     832    Local host: borax38\n     833    PID:        41398\n     834    -------------------------------------------------------------------\n            -------\n  >> 835    FAILED: Command exited with status 255:\n     836        './hello_oshmemcxx'\n     837    -------------------------------------------------------------------\n            -------\n     838    The application appears to have been direct launched using \"srun\",\n     839    but OMPI was not built with SLURM support. This usually happens\n     840    when OMPI was not configured --with-slurm and we weren't able\n     841    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 41476, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        41476\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     914    of its peer processes in the job will be killed properly.  You shou\n            ld\n     915    double check that everything has shut down cleanly.\n     916    \n     917    Local host: borax38\n     918    PID:        41476\n     919    -------------------------------------------------------------------\n            -------\n  >> 920    FAILED: Command exited with status 255:\n     921        './hello_oshmemfh'\n     922    -------------------------------------------------------------------\n            -------\n     923    The application appears to have been direct launched using \"srun\",\n     924    but OMPI was not built with SLURM support. This usually happens\n     925    when OMPI was not configured --with-slurm and we weren't able\n     926    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:41553] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     993    -------------------------------------------------------------------\n            -------\n     994    *** An error occurred in MPI_Init\n     995    *** on a NULL communicator\n     996    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     997    ***    and potentially your MPI job)\n     998    [borax38:41553] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 999    FAILED: Command exited with status 1:\n     1000       './hello_usempi'\n     1001   -------------------------------------------------------------------\n            -------\n     1002   The application appears to have been direct launched using \"srun\",\n     1003   but OMPI was not built with SLURM support. This usually happens\n     1004   when OMPI was not configured --with-slurm and we weren't able\n     1005   to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:41631] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1066    ------------------------------------------------------------------\n             --------\n     1067    *** An error occurred in MPI_Init\n     1068    *** on a NULL communicator\n     1069    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1070    ***    and potentially your MPI job)\n     1071    [borax38:41631] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1072    FAILED: Command exited with status 1:\n     1073        './hello_usempif08'\n     1074    ------------------------------------------------------------------\n             --------\n     1075    The application appears to have been direct launched using \"srun\",\n     1076    but OMPI was not built with SLURM support. This usually happens\n     1077    when OMPI was not configured --with-slurm and we weren't able\n     1078    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_circular_shift'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 41708, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        41708\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1145    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1146    double check that everything has shut down cleanly.\n     1147    \n     1148    Local host: borax38\n     1149    PID:        41708\n     1150    ------------------------------------------------------------------\n             --------\n  >> 1151    FAILED: Command exited with status 255:\n     1152        './oshmem_circular_shift'\n     1153    ------------------------------------------------------------------\n             --------\n     1154    The application appears to have been direct launched using \"srun\",\n     1155    but OMPI was not built with SLURM support. This usually happens\n     1156    when OMPI was not configured --with-slurm and we weren't able\n     1157    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_max_reduction'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 41785, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        41785\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1230    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1231    double check that everything has shut down cleanly.\n     1232    \n     1233    Local host: borax38\n     1234    PID:        41785\n     1235    ------------------------------------------------------------------\n             --------\n  >> 1236    FAILED: Command exited with status 255:\n     1237        './oshmem_max_reduction'\n     1238    ------------------------------------------------------------------\n             --------\n     1239    The application appears to have been direct launched using \"srun\",\n     1240    but OMPI was not built with SLURM support. This usually happens\n     1241    when OMPI was not configured --with-slurm and we weren't able\n     1242    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_shmalloc'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 41862, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        41862\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1315    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1316    double check that everything has shut down cleanly.\n     1317    \n     1318    Local host: borax38\n     1319    PID:        41862\n     1320    ------------------------------------------------------------------\n             --------\n  >> 1321    FAILED: Command exited with status 255:\n     1322        './oshmem_shmalloc'\n     1323    ------------------------------------------------------------------\n             --------\n     1324    The application appears to have been direct launched using \"srun\",\n     1325    but OMPI was not built with SLURM support. This usually happens\n     1326    when OMPI was not configured --with-slurm and we weren't able\n     1327    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_strided_puts'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 41938, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        41938\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1400    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1401    double check that everything has shut down cleanly.\n     1402    \n     1403    Local host: borax38\n     1404    PID:        41938\n     1405    ------------------------------------------------------------------\n             --------\n  >> 1406    FAILED: Command exited with status 255:\n     1407        './oshmem_strided_puts'\n     1408    ------------------------------------------------------------------\n             --------\n     1409    The application appears to have been direct launched using \"srun\",\n     1410    but OMPI was not built with SLURM support. This usually happens\n     1411    when OMPI was not configured --with-slurm and we weren't able\n     1412    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_symmetric_data'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 42014, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        42014\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1485    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1486    double check that everything has shut down cleanly.\n     1487    \n     1488    Local host: borax38\n     1489    PID:        42014\n     1490    ------------------------------------------------------------------\n             --------\n  >> 1491    FAILED: Command exited with status 255:\n     1492        './oshmem_symmetric_data'\n     1493    ------------------------------------------------------------------\n             --------\n     1494    The application appears to have been direct launched using \"srun\",\n     1495    but OMPI was not built with SLURM support. This usually happens\n     1496    when OMPI was not configured --with-slurm and we weren't able\n     1497    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:42092] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1564    ------------------------------------------------------------------\n             --------\n     1565    *** An error occurred in MPI_Init\n     1566    *** on a NULL communicator\n     1567    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1568    ***    and potentially your MPI job)\n     1569    [borax38:42092] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1570    FAILED: Command exited with status 1:\n     1571        './ring_c'\n     1572    ------------------------------------------------------------------\n             --------\n     1573    The application appears to have been direct launched using \"srun\",\n     1574    but OMPI was not built with SLURM support. This usually happens\n     1575    when OMPI was not configured --with-slurm and we weren't able\n     1576    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:42168] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1637    ------------------------------------------------------------------\n             --------\n     1638    *** An error occurred in MPI_Init\n     1639    *** on a NULL communicator\n     1640    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1641    ***    and potentially your MPI job)\n     1642    [borax38:42168] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1643    FAILED: Command exited with status 1:\n     1644        './ring_mpifh'\n     1645    ------------------------------------------------------------------\n             --------\n     1646    The application appears to have been direct launched using \"srun\",\n     1647    but OMPI was not built with SLURM support. This usually happens\n     1648    when OMPI was not configured --with-slurm and we weren't able\n     1649    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 42244, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        42244\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1716    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1717    double check that everything has shut down cleanly.\n     1718    \n     1719    Local host: borax38\n     1720    PID:        42244\n     1721    ------------------------------------------------------------------\n             --------\n  >> 1722    FAILED: Command exited with status 255:\n     1723        './ring_oshmem'\n     1724    ------------------------------------------------------------------\n             --------\n     1725    The application appears to have been direct launched using \"srun\",\n     1726    but OMPI was not built with SLURM support. This usually happens\n     1727    when OMPI was not configured --with-slurm and we weren't able\n     1728    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 42320, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        42320\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1801    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1802    double check that everything has shut down cleanly.\n     1803    \n     1804    Local host: borax38\n     1805    PID:        42320\n     1806    ------------------------------------------------------------------\n             --------\n  >> 1807    FAILED: Command exited with status 255:\n     1808        './ring_oshmemfh'\n     1809    ------------------------------------------------------------------\n             --------\n     1810    The application appears to have been direct launched using \"srun\",\n     1811    but OMPI was not built with SLURM support. This usually happens\n     1812    when OMPI was not configured --with-slurm and we weren't able\n     1813    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:42399] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1880    ------------------------------------------------------------------\n             --------\n     1881    *** An error occurred in MPI_Init\n     1882    *** on a NULL communicator\n     1883    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1884    ***    and potentially your MPI job)\n     1885    [borax38:42399] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1886    FAILED: Command exited with status 1:\n     1887        './ring_usempi'\n     1888    ------------------------------------------------------------------\n             --------\n     1889    The application appears to have been direct launched using \"srun\",\n     1890    but OMPI was not built with SLURM support. This usually happens\n     1891    when OMPI was not configured --with-slurm and we weren't able\n     1892    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:42475] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1953    ------------------------------------------------------------------\n             --------\n     1954    *** An error occurred in MPI_Init\n     1955    *** on a NULL communicator\n     1956    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1957    ***    and potentially your MPI job)\n     1958    [borax38:42475] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1959    FAILED: Command exited with status 1:\n     1960        './ring_usempif08'\n     1961    ------------------------------------------------------------------\n             --------\n     1962    The application appears to have been direct launched using \"srun\",\n     1963    but OMPI was not built with SLURM support. This usually happens\n     1964    when OMPI was not configured --with-slurm and we weren't able\n     1965    to discover a SLURM installation in the usual places.\n\n\n\n/p/vast1/build/spack/lib/spack/spack/build_environment.py:1061, in _setup_pkg_and_run:\n       1058        tb_string = traceback.format_exc()\n       1059\n       1060        # build up some context from the offending package so we can\n  >>   1061        # show that, too.\n       1062        package_context = get_package_context(tb)\n       1063\n       1064        logfile = None\n\nSee test log for details:\n  /g/g0/sochat1/.spack/test/wjkcucjdda4pthihxgsnrwpoy4cqlmqe/openmpi-3.1.4-xf5lntw-test-out.txt\n\n==> Error: 1 test(s) in the suite failed.\n\n======================== 1 failed, 0 passed of 1 specs =========================\n",
                    "return_code": 1,
                    "prediction": false,
                    "command": "/p/vast1/build/spack/bin/spack test run /xf5lnt"
                }
            ],
            "symbolator": [
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ],
            "libabigail": [
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ]
        },
        "libs": {
            "spliced": [
                {
                    "dep": "pkgconf@1.7.3%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.7.3-7hgm7il7fkby77dpxqp72np3ihfyc2tn/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ],
            "original": [
                {
                    "dep": "pkgconf@1.8.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ]
        },
        "experiment": "experiment",
        "result": "splice-success",
        "success": true,
        "splice": "pkgconf@1.7.3",
        "package": "openmpi@3.1.4"
    },
    {
        "binaries": {
            "spliced": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper"
            ],
            "original": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orted"
            ]
        },
        "predictions": {
            "spack-test": [
                {
                    "message": "==> Spack test wjkcucjdda4pthihxgsnrwpoy4cqlmqe\n==> Testing package openmpi-3.1.4-xf5lntw\n==> Error: TestFailure: 18 tests failed.\n\n\nCommand exited with status 1:\n    './hello_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:42855] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     592    -------------------------------------------------------------------\n            -------\n     593    *** An error occurred in MPI_Init\n     594    *** on a NULL communicator\n     595    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     596    ***    and potentially your MPI job)\n     597    [borax38:42855] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 598    FAILED: Command exited with status 1:\n     599        './hello_c'\n     600    -------------------------------------------------------------------\n            -------\n     601    The application appears to have been direct launched using \"srun\",\n     602    but OMPI was not built with SLURM support. This usually happens\n     603    when OMPI was not configured --with-slurm and we weren't able\n     604    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:42856] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     665    -------------------------------------------------------------------\n            -------\n     666    *** An error occurred in MPI_Init\n     667    *** on a NULL communicator\n     668    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     669    ***    and potentially your MPI job)\n     670    [borax38:42856] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 671    FAILED: Command exited with status 1:\n     672        './hello_mpifh'\n     673    -------------------------------------------------------------------\n            -------\n     674    The application appears to have been direct launched using \"srun\",\n     675    but OMPI was not built with SLURM support. This usually happens\n     676    when OMPI was not configured --with-slurm and we weren't able\n     677    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 42932, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        42932\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     744    of its peer processes in the job will be killed properly.  You shou\n            ld\n     745    double check that everything has shut down cleanly.\n     746    \n     747    Local host: borax38\n     748    PID:        42932\n     749    -------------------------------------------------------------------\n            -------\n  >> 750    FAILED: Command exited with status 255:\n     751        './hello_oshmem'\n     752    -------------------------------------------------------------------\n            -------\n     753    The application appears to have been direct launched using \"srun\",\n     754    but OMPI was not built with SLURM support. This usually happens\n     755    when OMPI was not configured --with-slurm and we weren't able\n     756    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemcxx'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 43009, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        43009\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     829    of its peer processes in the job will be killed properly.  You shou\n            ld\n     830    double check that everything has shut down cleanly.\n     831    \n     832    Local host: borax38\n     833    PID:        43009\n     834    -------------------------------------------------------------------\n            -------\n  >> 835    FAILED: Command exited with status 255:\n     836        './hello_oshmemcxx'\n     837    -------------------------------------------------------------------\n            -------\n     838    The application appears to have been direct launched using \"srun\",\n     839    but OMPI was not built with SLURM support. This usually happens\n     840    when OMPI was not configured --with-slurm and we weren't able\n     841    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 43085, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        43085\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     914    of its peer processes in the job will be killed properly.  You shou\n            ld\n     915    double check that everything has shut down cleanly.\n     916    \n     917    Local host: borax38\n     918    PID:        43085\n     919    -------------------------------------------------------------------\n            -------\n  >> 920    FAILED: Command exited with status 255:\n     921        './hello_oshmemfh'\n     922    -------------------------------------------------------------------\n            -------\n     923    The application appears to have been direct launched using \"srun\",\n     924    but OMPI was not built with SLURM support. This usually happens\n     925    when OMPI was not configured --with-slurm and we weren't able\n     926    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:43161] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     993    -------------------------------------------------------------------\n            -------\n     994    *** An error occurred in MPI_Init\n     995    *** on a NULL communicator\n     996    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     997    ***    and potentially your MPI job)\n     998    [borax38:43161] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 999    FAILED: Command exited with status 1:\n     1000       './hello_usempi'\n     1001   -------------------------------------------------------------------\n            -------\n     1002   The application appears to have been direct launched using \"srun\",\n     1003   but OMPI was not built with SLURM support. This usually happens\n     1004   when OMPI was not configured --with-slurm and we weren't able\n     1005   to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:43238] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1066    ------------------------------------------------------------------\n             --------\n     1067    *** An error occurred in MPI_Init\n     1068    *** on a NULL communicator\n     1069    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1070    ***    and potentially your MPI job)\n     1071    [borax38:43238] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1072    FAILED: Command exited with status 1:\n     1073        './hello_usempif08'\n     1074    ------------------------------------------------------------------\n             --------\n     1075    The application appears to have been direct launched using \"srun\",\n     1076    but OMPI was not built with SLURM support. This usually happens\n     1077    when OMPI was not configured --with-slurm and we weren't able\n     1078    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_circular_shift'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 43314, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        43314\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1145    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1146    double check that everything has shut down cleanly.\n     1147    \n     1148    Local host: borax38\n     1149    PID:        43314\n     1150    ------------------------------------------------------------------\n             --------\n  >> 1151    FAILED: Command exited with status 255:\n     1152        './oshmem_circular_shift'\n     1153    ------------------------------------------------------------------\n             --------\n     1154    The application appears to have been direct launched using \"srun\",\n     1155    but OMPI was not built with SLURM support. This usually happens\n     1156    when OMPI was not configured --with-slurm and we weren't able\n     1157    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_max_reduction'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 43391, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        43391\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1230    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1231    double check that everything has shut down cleanly.\n     1232    \n     1233    Local host: borax38\n     1234    PID:        43391\n     1235    ------------------------------------------------------------------\n             --------\n  >> 1236    FAILED: Command exited with status 255:\n     1237        './oshmem_max_reduction'\n     1238    ------------------------------------------------------------------\n             --------\n     1239    The application appears to have been direct launched using \"srun\",\n     1240    but OMPI was not built with SLURM support. This usually happens\n     1241    when OMPI was not configured --with-slurm and we weren't able\n     1242    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_shmalloc'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 43467, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        43467\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1315    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1316    double check that everything has shut down cleanly.\n     1317    \n     1318    Local host: borax38\n     1319    PID:        43467\n     1320    ------------------------------------------------------------------\n             --------\n  >> 1321    FAILED: Command exited with status 255:\n     1322        './oshmem_shmalloc'\n     1323    ------------------------------------------------------------------\n             --------\n     1324    The application appears to have been direct launched using \"srun\",\n     1325    but OMPI was not built with SLURM support. This usually happens\n     1326    when OMPI was not configured --with-slurm and we weren't able\n     1327    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_strided_puts'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 43543, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        43543\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1400    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1401    double check that everything has shut down cleanly.\n     1402    \n     1403    Local host: borax38\n     1404    PID:        43543\n     1405    ------------------------------------------------------------------\n             --------\n  >> 1406    FAILED: Command exited with status 255:\n     1407        './oshmem_strided_puts'\n     1408    ------------------------------------------------------------------\n             --------\n     1409    The application appears to have been direct launched using \"srun\",\n     1410    but OMPI was not built with SLURM support. This usually happens\n     1411    when OMPI was not configured --with-slurm and we weren't able\n     1412    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_symmetric_data'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 43620, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        43620\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1485    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1486    double check that everything has shut down cleanly.\n     1487    \n     1488    Local host: borax38\n     1489    PID:        43620\n     1490    ------------------------------------------------------------------\n             --------\n  >> 1491    FAILED: Command exited with status 255:\n     1492        './oshmem_symmetric_data'\n     1493    ------------------------------------------------------------------\n             --------\n     1494    The application appears to have been direct launched using \"srun\",\n     1495    but OMPI was not built with SLURM support. This usually happens\n     1496    when OMPI was not configured --with-slurm and we weren't able\n     1497    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:43698] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1564    ------------------------------------------------------------------\n             --------\n     1565    *** An error occurred in MPI_Init\n     1566    *** on a NULL communicator\n     1567    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1568    ***    and potentially your MPI job)\n     1569    [borax38:43698] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1570    FAILED: Command exited with status 1:\n     1571        './ring_c'\n     1572    ------------------------------------------------------------------\n             --------\n     1573    The application appears to have been direct launched using \"srun\",\n     1574    but OMPI was not built with SLURM support. This usually happens\n     1575    when OMPI was not configured --with-slurm and we weren't able\n     1576    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:43775] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1637    ------------------------------------------------------------------\n             --------\n     1638    *** An error occurred in MPI_Init\n     1639    *** on a NULL communicator\n     1640    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1641    ***    and potentially your MPI job)\n     1642    [borax38:43775] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1643    FAILED: Command exited with status 1:\n     1644        './ring_mpifh'\n     1645    ------------------------------------------------------------------\n             --------\n     1646    The application appears to have been direct launched using \"srun\",\n     1647    but OMPI was not built with SLURM support. This usually happens\n     1648    when OMPI was not configured --with-slurm and we weren't able\n     1649    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 43851, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        43851\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1716    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1717    double check that everything has shut down cleanly.\n     1718    \n     1719    Local host: borax38\n     1720    PID:        43851\n     1721    ------------------------------------------------------------------\n             --------\n  >> 1722    FAILED: Command exited with status 255:\n     1723        './ring_oshmem'\n     1724    ------------------------------------------------------------------\n             --------\n     1725    The application appears to have been direct launched using \"srun\",\n     1726    but OMPI was not built with SLURM support. This usually happens\n     1727    when OMPI was not configured --with-slurm and we weren't able\n     1728    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 43931, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        43931\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1801    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1802    double check that everything has shut down cleanly.\n     1803    \n     1804    Local host: borax38\n     1805    PID:        43931\n     1806    ------------------------------------------------------------------\n             --------\n  >> 1807    FAILED: Command exited with status 255:\n     1808        './ring_oshmemfh'\n     1809    ------------------------------------------------------------------\n             --------\n     1810    The application appears to have been direct launched using \"srun\",\n     1811    but OMPI was not built with SLURM support. This usually happens\n     1812    when OMPI was not configured --with-slurm and we weren't able\n     1813    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:44010] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1880    ------------------------------------------------------------------\n             --------\n     1881    *** An error occurred in MPI_Init\n     1882    *** on a NULL communicator\n     1883    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1884    ***    and potentially your MPI job)\n     1885    [borax38:44010] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1886    FAILED: Command exited with status 1:\n     1887        './ring_usempi'\n     1888    ------------------------------------------------------------------\n             --------\n     1889    The application appears to have been direct launched using \"srun\",\n     1890    but OMPI was not built with SLURM support. This usually happens\n     1891    when OMPI was not configured --with-slurm and we weren't able\n     1892    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:44088] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1953    ------------------------------------------------------------------\n             --------\n     1954    *** An error occurred in MPI_Init\n     1955    *** on a NULL communicator\n     1956    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1957    ***    and potentially your MPI job)\n     1958    [borax38:44088] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1959    FAILED: Command exited with status 1:\n     1960        './ring_usempif08'\n     1961    ------------------------------------------------------------------\n             --------\n     1962    The application appears to have been direct launched using \"srun\",\n     1963    but OMPI was not built with SLURM support. This usually happens\n     1964    when OMPI was not configured --with-slurm and we weren't able\n     1965    to discover a SLURM installation in the usual places.\n\n\n\n/p/vast1/build/spack/lib/spack/spack/build_environment.py:1061, in _setup_pkg_and_run:\n       1058        tb_string = traceback.format_exc()\n       1059\n       1060        # build up some context from the offending package so we can\n  >>   1061        # show that, too.\n       1062        package_context = get_package_context(tb)\n       1063\n       1064        logfile = None\n\nSee test log for details:\n  /g/g0/sochat1/.spack/test/wjkcucjdda4pthihxgsnrwpoy4cqlmqe/openmpi-3.1.4-xf5lntw-test-out.txt\n\n==> Error: 1 test(s) in the suite failed.\n\n======================== 1 failed, 0 passed of 1 specs =========================\n",
                    "return_code": 1,
                    "prediction": false,
                    "command": "/p/vast1/build/spack/bin/spack test run /xf5lnt"
                }
            ],
            "symbolator": [
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ],
            "libabigail": [
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ]
        },
        "libs": {
            "spliced": [
                {
                    "dep": "pkgconf@1.6.3%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.3-wx7wxute6htyt7arhoksznfq56yoelvh/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ],
            "original": [
                {
                    "dep": "pkgconf@1.8.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ]
        },
        "experiment": "experiment",
        "result": "splice-success",
        "success": true,
        "splice": "pkgconf@1.6.3",
        "package": "openmpi@3.1.4"
    },
    {
        "binaries": {
            "spliced": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper"
            ],
            "original": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orted"
            ]
        },
        "predictions": {
            "spack-test": [
                {
                    "message": "==> Spack test wjkcucjdda4pthihxgsnrwpoy4cqlmqe\n==> Testing package openmpi-3.1.4-xf5lntw\n==> Error: TestFailure: 18 tests failed.\n\n\nCommand exited with status 1:\n    './hello_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:44472] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     592    -------------------------------------------------------------------\n            -------\n     593    *** An error occurred in MPI_Init\n     594    *** on a NULL communicator\n     595    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     596    ***    and potentially your MPI job)\n     597    [borax38:44472] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 598    FAILED: Command exited with status 1:\n     599        './hello_c'\n     600    -------------------------------------------------------------------\n            -------\n     601    The application appears to have been direct launched using \"srun\",\n     602    but OMPI was not built with SLURM support. This usually happens\n     603    when OMPI was not configured --with-slurm and we weren't able\n     604    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:44473] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     665    -------------------------------------------------------------------\n            -------\n     666    *** An error occurred in MPI_Init\n     667    *** on a NULL communicator\n     668    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     669    ***    and potentially your MPI job)\n     670    [borax38:44473] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 671    FAILED: Command exited with status 1:\n     672        './hello_mpifh'\n     673    -------------------------------------------------------------------\n            -------\n     674    The application appears to have been direct launched using \"srun\",\n     675    but OMPI was not built with SLURM support. This usually happens\n     676    when OMPI was not configured --with-slurm and we weren't able\n     677    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 44474, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        44474\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     744    of its peer processes in the job will be killed properly.  You shou\n            ld\n     745    double check that everything has shut down cleanly.\n     746    \n     747    Local host: borax38\n     748    PID:        44474\n     749    -------------------------------------------------------------------\n            -------\n  >> 750    FAILED: Command exited with status 255:\n     751        './hello_oshmem'\n     752    -------------------------------------------------------------------\n            -------\n     753    The application appears to have been direct launched using \"srun\",\n     754    but OMPI was not built with SLURM support. This usually happens\n     755    when OMPI was not configured --with-slurm and we weren't able\n     756    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemcxx'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 44550, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        44550\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     829    of its peer processes in the job will be killed properly.  You shou\n            ld\n     830    double check that everything has shut down cleanly.\n     831    \n     832    Local host: borax38\n     833    PID:        44550\n     834    -------------------------------------------------------------------\n            -------\n  >> 835    FAILED: Command exited with status 255:\n     836        './hello_oshmemcxx'\n     837    -------------------------------------------------------------------\n            -------\n     838    The application appears to have been direct launched using \"srun\",\n     839    but OMPI was not built with SLURM support. This usually happens\n     840    when OMPI was not configured --with-slurm and we weren't able\n     841    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 44626, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        44626\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     914    of its peer processes in the job will be killed properly.  You shou\n            ld\n     915    double check that everything has shut down cleanly.\n     916    \n     917    Local host: borax38\n     918    PID:        44626\n     919    -------------------------------------------------------------------\n            -------\n  >> 920    FAILED: Command exited with status 255:\n     921        './hello_oshmemfh'\n     922    -------------------------------------------------------------------\n            -------\n     923    The application appears to have been direct launched using \"srun\",\n     924    but OMPI was not built with SLURM support. This usually happens\n     925    when OMPI was not configured --with-slurm and we weren't able\n     926    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:44702] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     993    -------------------------------------------------------------------\n            -------\n     994    *** An error occurred in MPI_Init\n     995    *** on a NULL communicator\n     996    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     997    ***    and potentially your MPI job)\n     998    [borax38:44702] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 999    FAILED: Command exited with status 1:\n     1000       './hello_usempi'\n     1001   -------------------------------------------------------------------\n            -------\n     1002   The application appears to have been direct launched using \"srun\",\n     1003   but OMPI was not built with SLURM support. This usually happens\n     1004   when OMPI was not configured --with-slurm and we weren't able\n     1005   to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:44779] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1066    ------------------------------------------------------------------\n             --------\n     1067    *** An error occurred in MPI_Init\n     1068    *** on a NULL communicator\n     1069    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1070    ***    and potentially your MPI job)\n     1071    [borax38:44779] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1072    FAILED: Command exited with status 1:\n     1073        './hello_usempif08'\n     1074    ------------------------------------------------------------------\n             --------\n     1075    The application appears to have been direct launched using \"srun\",\n     1076    but OMPI was not built with SLURM support. This usually happens\n     1077    when OMPI was not configured --with-slurm and we weren't able\n     1078    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_circular_shift'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 44855, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        44855\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1145    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1146    double check that everything has shut down cleanly.\n     1147    \n     1148    Local host: borax38\n     1149    PID:        44855\n     1150    ------------------------------------------------------------------\n             --------\n  >> 1151    FAILED: Command exited with status 255:\n     1152        './oshmem_circular_shift'\n     1153    ------------------------------------------------------------------\n             --------\n     1154    The application appears to have been direct launched using \"srun\",\n     1155    but OMPI was not built with SLURM support. This usually happens\n     1156    when OMPI was not configured --with-slurm and we weren't able\n     1157    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_max_reduction'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 44931, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        44931\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1230    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1231    double check that everything has shut down cleanly.\n     1232    \n     1233    Local host: borax38\n     1234    PID:        44931\n     1235    ------------------------------------------------------------------\n             --------\n  >> 1236    FAILED: Command exited with status 255:\n     1237        './oshmem_max_reduction'\n     1238    ------------------------------------------------------------------\n             --------\n     1239    The application appears to have been direct launched using \"srun\",\n     1240    but OMPI was not built with SLURM support. This usually happens\n     1241    when OMPI was not configured --with-slurm and we weren't able\n     1242    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_shmalloc'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 45007, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        45007\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1315    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1316    double check that everything has shut down cleanly.\n     1317    \n     1318    Local host: borax38\n     1319    PID:        45007\n     1320    ------------------------------------------------------------------\n             --------\n  >> 1321    FAILED: Command exited with status 255:\n     1322        './oshmem_shmalloc'\n     1323    ------------------------------------------------------------------\n             --------\n     1324    The application appears to have been direct launched using \"srun\",\n     1325    but OMPI was not built with SLURM support. This usually happens\n     1326    when OMPI was not configured --with-slurm and we weren't able\n     1327    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_strided_puts'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 45084, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        45084\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1400    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1401    double check that everything has shut down cleanly.\n     1402    \n     1403    Local host: borax38\n     1404    PID:        45084\n     1405    ------------------------------------------------------------------\n             --------\n  >> 1406    FAILED: Command exited with status 255:\n     1407        './oshmem_strided_puts'\n     1408    ------------------------------------------------------------------\n             --------\n     1409    The application appears to have been direct launched using \"srun\",\n     1410    but OMPI was not built with SLURM support. This usually happens\n     1411    when OMPI was not configured --with-slurm and we weren't able\n     1412    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_symmetric_data'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 45160, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        45160\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1485    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1486    double check that everything has shut down cleanly.\n     1487    \n     1488    Local host: borax38\n     1489    PID:        45160\n     1490    ------------------------------------------------------------------\n             --------\n  >> 1491    FAILED: Command exited with status 255:\n     1492        './oshmem_symmetric_data'\n     1493    ------------------------------------------------------------------\n             --------\n     1494    The application appears to have been direct launched using \"srun\",\n     1495    but OMPI was not built with SLURM support. This usually happens\n     1496    when OMPI was not configured --with-slurm and we weren't able\n     1497    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:45236] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1564    ------------------------------------------------------------------\n             --------\n     1565    *** An error occurred in MPI_Init\n     1566    *** on a NULL communicator\n     1567    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1568    ***    and potentially your MPI job)\n     1569    [borax38:45236] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1570    FAILED: Command exited with status 1:\n     1571        './ring_c'\n     1572    ------------------------------------------------------------------\n             --------\n     1573    The application appears to have been direct launched using \"srun\",\n     1574    but OMPI was not built with SLURM support. This usually happens\n     1575    when OMPI was not configured --with-slurm and we weren't able\n     1576    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:45312] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1637    ------------------------------------------------------------------\n             --------\n     1638    *** An error occurred in MPI_Init\n     1639    *** on a NULL communicator\n     1640    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1641    ***    and potentially your MPI job)\n     1642    [borax38:45312] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1643    FAILED: Command exited with status 1:\n     1644        './ring_mpifh'\n     1645    ------------------------------------------------------------------\n             --------\n     1646    The application appears to have been direct launched using \"srun\",\n     1647    but OMPI was not built with SLURM support. This usually happens\n     1648    when OMPI was not configured --with-slurm and we weren't able\n     1649    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 45389, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        45389\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1716    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1717    double check that everything has shut down cleanly.\n     1718    \n     1719    Local host: borax38\n     1720    PID:        45389\n     1721    ------------------------------------------------------------------\n             --------\n  >> 1722    FAILED: Command exited with status 255:\n     1723        './ring_oshmem'\n     1724    ------------------------------------------------------------------\n             --------\n     1725    The application appears to have been direct launched using \"srun\",\n     1726    but OMPI was not built with SLURM support. This usually happens\n     1727    when OMPI was not configured --with-slurm and we weren't able\n     1728    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 45466, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        45466\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1801    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1802    double check that everything has shut down cleanly.\n     1803    \n     1804    Local host: borax38\n     1805    PID:        45466\n     1806    ------------------------------------------------------------------\n             --------\n  >> 1807    FAILED: Command exited with status 255:\n     1808        './ring_oshmemfh'\n     1809    ------------------------------------------------------------------\n             --------\n     1810    The application appears to have been direct launched using \"srun\",\n     1811    but OMPI was not built with SLURM support. This usually happens\n     1812    when OMPI was not configured --with-slurm and we weren't able\n     1813    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:45542] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1880    ------------------------------------------------------------------\n             --------\n     1881    *** An error occurred in MPI_Init\n     1882    *** on a NULL communicator\n     1883    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1884    ***    and potentially your MPI job)\n     1885    [borax38:45542] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1886    FAILED: Command exited with status 1:\n     1887        './ring_usempi'\n     1888    ------------------------------------------------------------------\n             --------\n     1889    The application appears to have been direct launched using \"srun\",\n     1890    but OMPI was not built with SLURM support. This usually happens\n     1891    when OMPI was not configured --with-slurm and we weren't able\n     1892    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:45618] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1953    ------------------------------------------------------------------\n             --------\n     1954    *** An error occurred in MPI_Init\n     1955    *** on a NULL communicator\n     1956    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1957    ***    and potentially your MPI job)\n     1958    [borax38:45618] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1959    FAILED: Command exited with status 1:\n     1960        './ring_usempif08'\n     1961    ------------------------------------------------------------------\n             --------\n     1962    The application appears to have been direct launched using \"srun\",\n     1963    but OMPI was not built with SLURM support. This usually happens\n     1964    when OMPI was not configured --with-slurm and we weren't able\n     1965    to discover a SLURM installation in the usual places.\n\n\n\n/p/vast1/build/spack/lib/spack/spack/build_environment.py:1061, in _setup_pkg_and_run:\n       1058        tb_string = traceback.format_exc()\n       1059\n       1060        # build up some context from the offending package so we can\n  >>   1061        # show that, too.\n       1062        package_context = get_package_context(tb)\n       1063\n       1064        logfile = None\n\nSee test log for details:\n  /g/g0/sochat1/.spack/test/wjkcucjdda4pthihxgsnrwpoy4cqlmqe/openmpi-3.1.4-xf5lntw-test-out.txt\n\n==> Error: 1 test(s) in the suite failed.\n\n======================== 1 failed, 0 passed of 1 specs =========================\n",
                    "return_code": 1,
                    "prediction": false,
                    "command": "/p/vast1/build/spack/bin/spack test run /xf5lnt"
                }
            ],
            "symbolator": [
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ],
            "libabigail": [
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ]
        },
        "libs": {
            "spliced": [
                {
                    "dep": "pkgconf@1.6.1%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.1-i2ayheogpfexopiqqg3qz7t4upgsmtba/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ],
            "original": [
                {
                    "dep": "pkgconf@1.8.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ]
        },
        "experiment": "experiment",
        "result": "splice-success",
        "success": true,
        "splice": "pkgconf@1.6.1",
        "package": "openmpi@3.1.4"
    },
    {
        "binaries": {
            "spliced": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper"
            ],
            "original": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orted"
            ]
        },
        "predictions": {
            "spack-test": [
                {
                    "message": "==> Spack test wjkcucjdda4pthihxgsnrwpoy4cqlmqe\n==> Testing package openmpi-3.1.4-xf5lntw\n==> Error: TestFailure: 18 tests failed.\n\n\nCommand exited with status 1:\n    './hello_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:45999] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     592    -------------------------------------------------------------------\n            -------\n     593    *** An error occurred in MPI_Init\n     594    *** on a NULL communicator\n     595    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     596    ***    and potentially your MPI job)\n     597    [borax38:45999] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 598    FAILED: Command exited with status 1:\n     599        './hello_c'\n     600    -------------------------------------------------------------------\n            -------\n     601    The application appears to have been direct launched using \"srun\",\n     602    but OMPI was not built with SLURM support. This usually happens\n     603    when OMPI was not configured --with-slurm and we weren't able\n     604    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:46000] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     665    -------------------------------------------------------------------\n            -------\n     666    *** An error occurred in MPI_Init\n     667    *** on a NULL communicator\n     668    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     669    ***    and potentially your MPI job)\n     670    [borax38:46000] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 671    FAILED: Command exited with status 1:\n     672        './hello_mpifh'\n     673    -------------------------------------------------------------------\n            -------\n     674    The application appears to have been direct launched using \"srun\",\n     675    but OMPI was not built with SLURM support. This usually happens\n     676    when OMPI was not configured --with-slurm and we weren't able\n     677    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 46001, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        46001\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     744    of its peer processes in the job will be killed properly.  You shou\n            ld\n     745    double check that everything has shut down cleanly.\n     746    \n     747    Local host: borax38\n     748    PID:        46001\n     749    -------------------------------------------------------------------\n            -------\n  >> 750    FAILED: Command exited with status 255:\n     751        './hello_oshmem'\n     752    -------------------------------------------------------------------\n            -------\n     753    The application appears to have been direct launched using \"srun\",\n     754    but OMPI was not built with SLURM support. This usually happens\n     755    when OMPI was not configured --with-slurm and we weren't able\n     756    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemcxx'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 46077, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        46077\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     829    of its peer processes in the job will be killed properly.  You shou\n            ld\n     830    double check that everything has shut down cleanly.\n     831    \n     832    Local host: borax38\n     833    PID:        46077\n     834    -------------------------------------------------------------------\n            -------\n  >> 835    FAILED: Command exited with status 255:\n     836        './hello_oshmemcxx'\n     837    -------------------------------------------------------------------\n            -------\n     838    The application appears to have been direct launched using \"srun\",\n     839    but OMPI was not built with SLURM support. This usually happens\n     840    when OMPI was not configured --with-slurm and we weren't able\n     841    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 46153, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        46153\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     914    of its peer processes in the job will be killed properly.  You shou\n            ld\n     915    double check that everything has shut down cleanly.\n     916    \n     917    Local host: borax38\n     918    PID:        46153\n     919    -------------------------------------------------------------------\n            -------\n  >> 920    FAILED: Command exited with status 255:\n     921        './hello_oshmemfh'\n     922    -------------------------------------------------------------------\n            -------\n     923    The application appears to have been direct launched using \"srun\",\n     924    but OMPI was not built with SLURM support. This usually happens\n     925    when OMPI was not configured --with-slurm and we weren't able\n     926    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:46233] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     993    -------------------------------------------------------------------\n            -------\n     994    *** An error occurred in MPI_Init\n     995    *** on a NULL communicator\n     996    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     997    ***    and potentially your MPI job)\n     998    [borax38:46233] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 999    FAILED: Command exited with status 1:\n     1000       './hello_usempi'\n     1001   -------------------------------------------------------------------\n            -------\n     1002   The application appears to have been direct launched using \"srun\",\n     1003   but OMPI was not built with SLURM support. This usually happens\n     1004   when OMPI was not configured --with-slurm and we weren't able\n     1005   to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:46309] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1066    ------------------------------------------------------------------\n             --------\n     1067    *** An error occurred in MPI_Init\n     1068    *** on a NULL communicator\n     1069    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1070    ***    and potentially your MPI job)\n     1071    [borax38:46309] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1072    FAILED: Command exited with status 1:\n     1073        './hello_usempif08'\n     1074    ------------------------------------------------------------------\n             --------\n     1075    The application appears to have been direct launched using \"srun\",\n     1076    but OMPI was not built with SLURM support. This usually happens\n     1077    when OMPI was not configured --with-slurm and we weren't able\n     1078    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_circular_shift'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 46386, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        46386\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1145    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1146    double check that everything has shut down cleanly.\n     1147    \n     1148    Local host: borax38\n     1149    PID:        46386\n     1150    ------------------------------------------------------------------\n             --------\n  >> 1151    FAILED: Command exited with status 255:\n     1152        './oshmem_circular_shift'\n     1153    ------------------------------------------------------------------\n             --------\n     1154    The application appears to have been direct launched using \"srun\",\n     1155    but OMPI was not built with SLURM support. This usually happens\n     1156    when OMPI was not configured --with-slurm and we weren't able\n     1157    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_max_reduction'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 46462, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        46462\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1230    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1231    double check that everything has shut down cleanly.\n     1232    \n     1233    Local host: borax38\n     1234    PID:        46462\n     1235    ------------------------------------------------------------------\n             --------\n  >> 1236    FAILED: Command exited with status 255:\n     1237        './oshmem_max_reduction'\n     1238    ------------------------------------------------------------------\n             --------\n     1239    The application appears to have been direct launched using \"srun\",\n     1240    but OMPI was not built with SLURM support. This usually happens\n     1241    when OMPI was not configured --with-slurm and we weren't able\n     1242    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_shmalloc'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 46539, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        46539\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1315    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1316    double check that everything has shut down cleanly.\n     1317    \n     1318    Local host: borax38\n     1319    PID:        46539\n     1320    ------------------------------------------------------------------\n             --------\n  >> 1321    FAILED: Command exited with status 255:\n     1322        './oshmem_shmalloc'\n     1323    ------------------------------------------------------------------\n             --------\n     1324    The application appears to have been direct launched using \"srun\",\n     1325    but OMPI was not built with SLURM support. This usually happens\n     1326    when OMPI was not configured --with-slurm and we weren't able\n     1327    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_strided_puts'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 46615, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        46615\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1400    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1401    double check that everything has shut down cleanly.\n     1402    \n     1403    Local host: borax38\n     1404    PID:        46615\n     1405    ------------------------------------------------------------------\n             --------\n  >> 1406    FAILED: Command exited with status 255:\n     1407        './oshmem_strided_puts'\n     1408    ------------------------------------------------------------------\n             --------\n     1409    The application appears to have been direct launched using \"srun\",\n     1410    but OMPI was not built with SLURM support. This usually happens\n     1411    when OMPI was not configured --with-slurm and we weren't able\n     1412    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_symmetric_data'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 46691, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        46691\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1485    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1486    double check that everything has shut down cleanly.\n     1487    \n     1488    Local host: borax38\n     1489    PID:        46691\n     1490    ------------------------------------------------------------------\n             --------\n  >> 1491    FAILED: Command exited with status 255:\n     1492        './oshmem_symmetric_data'\n     1493    ------------------------------------------------------------------\n             --------\n     1494    The application appears to have been direct launched using \"srun\",\n     1495    but OMPI was not built with SLURM support. This usually happens\n     1496    when OMPI was not configured --with-slurm and we weren't able\n     1497    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:46767] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1564    ------------------------------------------------------------------\n             --------\n     1565    *** An error occurred in MPI_Init\n     1566    *** on a NULL communicator\n     1567    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1568    ***    and potentially your MPI job)\n     1569    [borax38:46767] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1570    FAILED: Command exited with status 1:\n     1571        './ring_c'\n     1572    ------------------------------------------------------------------\n             --------\n     1573    The application appears to have been direct launched using \"srun\",\n     1574    but OMPI was not built with SLURM support. This usually happens\n     1575    when OMPI was not configured --with-slurm and we weren't able\n     1576    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:46844] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1637    ------------------------------------------------------------------\n             --------\n     1638    *** An error occurred in MPI_Init\n     1639    *** on a NULL communicator\n     1640    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1641    ***    and potentially your MPI job)\n     1642    [borax38:46844] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1643    FAILED: Command exited with status 1:\n     1644        './ring_mpifh'\n     1645    ------------------------------------------------------------------\n             --------\n     1646    The application appears to have been direct launched using \"srun\",\n     1647    but OMPI was not built with SLURM support. This usually happens\n     1648    when OMPI was not configured --with-slurm and we weren't able\n     1649    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 46920, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        46920\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1716    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1717    double check that everything has shut down cleanly.\n     1718    \n     1719    Local host: borax38\n     1720    PID:        46920\n     1721    ------------------------------------------------------------------\n             --------\n  >> 1722    FAILED: Command exited with status 255:\n     1723        './ring_oshmem'\n     1724    ------------------------------------------------------------------\n             --------\n     1725    The application appears to have been direct launched using \"srun\",\n     1726    but OMPI was not built with SLURM support. This usually happens\n     1727    when OMPI was not configured --with-slurm and we weren't able\n     1728    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 46996, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        46996\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1801    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1802    double check that everything has shut down cleanly.\n     1803    \n     1804    Local host: borax38\n     1805    PID:        46996\n     1806    ------------------------------------------------------------------\n             --------\n  >> 1807    FAILED: Command exited with status 255:\n     1808        './ring_oshmemfh'\n     1809    ------------------------------------------------------------------\n             --------\n     1810    The application appears to have been direct launched using \"srun\",\n     1811    but OMPI was not built with SLURM support. This usually happens\n     1812    when OMPI was not configured --with-slurm and we weren't able\n     1813    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:47072] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1880    ------------------------------------------------------------------\n             --------\n     1881    *** An error occurred in MPI_Init\n     1882    *** on a NULL communicator\n     1883    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1884    ***    and potentially your MPI job)\n     1885    [borax38:47072] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1886    FAILED: Command exited with status 1:\n     1887        './ring_usempi'\n     1888    ------------------------------------------------------------------\n             --------\n     1889    The application appears to have been direct launched using \"srun\",\n     1890    but OMPI was not built with SLURM support. This usually happens\n     1891    when OMPI was not configured --with-slurm and we weren't able\n     1892    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:47149] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1953    ------------------------------------------------------------------\n             --------\n     1954    *** An error occurred in MPI_Init\n     1955    *** on a NULL communicator\n     1956    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1957    ***    and potentially your MPI job)\n     1958    [borax38:47149] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1959    FAILED: Command exited with status 1:\n     1960        './ring_usempif08'\n     1961    ------------------------------------------------------------------\n             --------\n     1962    The application appears to have been direct launched using \"srun\",\n     1963    but OMPI was not built with SLURM support. This usually happens\n     1964    when OMPI was not configured --with-slurm and we weren't able\n     1965    to discover a SLURM installation in the usual places.\n\n\n\n/p/vast1/build/spack/lib/spack/spack/build_environment.py:1061, in _setup_pkg_and_run:\n       1058        tb_string = traceback.format_exc()\n       1059\n       1060        # build up some context from the offending package so we can\n  >>   1061        # show that, too.\n       1062        package_context = get_package_context(tb)\n       1063\n       1064        logfile = None\n\nSee test log for details:\n  /g/g0/sochat1/.spack/test/wjkcucjdda4pthihxgsnrwpoy4cqlmqe/openmpi-3.1.4-xf5lntw-test-out.txt\n\n==> Error: 1 test(s) in the suite failed.\n\n======================== 1 failed, 0 passed of 1 specs =========================\n",
                    "return_code": 1,
                    "prediction": false,
                    "command": "/p/vast1/build/spack/bin/spack test run /xf5lnt"
                }
            ],
            "symbolator": [
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ],
            "libabigail": [
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ]
        },
        "libs": {
            "spliced": [
                {
                    "dep": "pkgconf@1.6.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.6.0-euka5rjqvhk7nehbjmm22b3fnf4n3brf/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ],
            "original": [
                {
                    "dep": "pkgconf@1.8.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ]
        },
        "experiment": "experiment",
        "result": "splice-success",
        "success": true,
        "splice": "pkgconf@1.6.0",
        "package": "openmpi@3.1.4"
    },
    {
        "binaries": {
            "spliced": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper"
            ],
            "original": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orted"
            ]
        },
        "predictions": {
            "spack-test": [
                {
                    "message": "==> Spack test wjkcucjdda4pthihxgsnrwpoy4cqlmqe\n==> Testing package openmpi-3.1.4-xf5lntw\n==> Error: TestFailure: 18 tests failed.\n\n\nCommand exited with status 1:\n    './hello_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:47530] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     592    -------------------------------------------------------------------\n            -------\n     593    *** An error occurred in MPI_Init\n     594    *** on a NULL communicator\n     595    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     596    ***    and potentially your MPI job)\n     597    [borax38:47530] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 598    FAILED: Command exited with status 1:\n     599        './hello_c'\n     600    -------------------------------------------------------------------\n            -------\n     601    The application appears to have been direct launched using \"srun\",\n     602    but OMPI was not built with SLURM support. This usually happens\n     603    when OMPI was not configured --with-slurm and we weren't able\n     604    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:47532] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     665    -------------------------------------------------------------------\n            -------\n     666    *** An error occurred in MPI_Init\n     667    *** on a NULL communicator\n     668    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     669    ***    and potentially your MPI job)\n     670    [borax38:47532] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 671    FAILED: Command exited with status 1:\n     672        './hello_mpifh'\n     673    -------------------------------------------------------------------\n            -------\n     674    The application appears to have been direct launched using \"srun\",\n     675    but OMPI was not built with SLURM support. This usually happens\n     676    when OMPI was not configured --with-slurm and we weren't able\n     677    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 47533, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        47533\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     744    of its peer processes in the job will be killed properly.  You shou\n            ld\n     745    double check that everything has shut down cleanly.\n     746    \n     747    Local host: borax38\n     748    PID:        47533\n     749    -------------------------------------------------------------------\n            -------\n  >> 750    FAILED: Command exited with status 255:\n     751        './hello_oshmem'\n     752    -------------------------------------------------------------------\n            -------\n     753    The application appears to have been direct launched using \"srun\",\n     754    but OMPI was not built with SLURM support. This usually happens\n     755    when OMPI was not configured --with-slurm and we weren't able\n     756    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemcxx'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 47609, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        47609\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     829    of its peer processes in the job will be killed properly.  You shou\n            ld\n     830    double check that everything has shut down cleanly.\n     831    \n     832    Local host: borax38\n     833    PID:        47609\n     834    -------------------------------------------------------------------\n            -------\n  >> 835    FAILED: Command exited with status 255:\n     836        './hello_oshmemcxx'\n     837    -------------------------------------------------------------------\n            -------\n     838    The application appears to have been direct launched using \"srun\",\n     839    but OMPI was not built with SLURM support. This usually happens\n     840    when OMPI was not configured --with-slurm and we weren't able\n     841    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 47685, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        47685\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     914    of its peer processes in the job will be killed properly.  You shou\n            ld\n     915    double check that everything has shut down cleanly.\n     916    \n     917    Local host: borax38\n     918    PID:        47685\n     919    -------------------------------------------------------------------\n            -------\n  >> 920    FAILED: Command exited with status 255:\n     921        './hello_oshmemfh'\n     922    -------------------------------------------------------------------\n            -------\n     923    The application appears to have been direct launched using \"srun\",\n     924    but OMPI was not built with SLURM support. This usually happens\n     925    when OMPI was not configured --with-slurm and we weren't able\n     926    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:47761] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     993    -------------------------------------------------------------------\n            -------\n     994    *** An error occurred in MPI_Init\n     995    *** on a NULL communicator\n     996    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     997    ***    and potentially your MPI job)\n     998    [borax38:47761] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 999    FAILED: Command exited with status 1:\n     1000       './hello_usempi'\n     1001   -------------------------------------------------------------------\n            -------\n     1002   The application appears to have been direct launched using \"srun\",\n     1003   but OMPI was not built with SLURM support. This usually happens\n     1004   when OMPI was not configured --with-slurm and we weren't able\n     1005   to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:47838] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1066    ------------------------------------------------------------------\n             --------\n     1067    *** An error occurred in MPI_Init\n     1068    *** on a NULL communicator\n     1069    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1070    ***    and potentially your MPI job)\n     1071    [borax38:47838] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1072    FAILED: Command exited with status 1:\n     1073        './hello_usempif08'\n     1074    ------------------------------------------------------------------\n             --------\n     1075    The application appears to have been direct launched using \"srun\",\n     1076    but OMPI was not built with SLURM support. This usually happens\n     1077    when OMPI was not configured --with-slurm and we weren't able\n     1078    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_circular_shift'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 47914, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        47914\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1145    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1146    double check that everything has shut down cleanly.\n     1147    \n     1148    Local host: borax38\n     1149    PID:        47914\n     1150    ------------------------------------------------------------------\n             --------\n  >> 1151    FAILED: Command exited with status 255:\n     1152        './oshmem_circular_shift'\n     1153    ------------------------------------------------------------------\n             --------\n     1154    The application appears to have been direct launched using \"srun\",\n     1155    but OMPI was not built with SLURM support. This usually happens\n     1156    when OMPI was not configured --with-slurm and we weren't able\n     1157    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_max_reduction'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 47990, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        47990\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1230    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1231    double check that everything has shut down cleanly.\n     1232    \n     1233    Local host: borax38\n     1234    PID:        47990\n     1235    ------------------------------------------------------------------\n             --------\n  >> 1236    FAILED: Command exited with status 255:\n     1237        './oshmem_max_reduction'\n     1238    ------------------------------------------------------------------\n             --------\n     1239    The application appears to have been direct launched using \"srun\",\n     1240    but OMPI was not built with SLURM support. This usually happens\n     1241    when OMPI was not configured --with-slurm and we weren't able\n     1242    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_shmalloc'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 48066, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        48066\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1315    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1316    double check that everything has shut down cleanly.\n     1317    \n     1318    Local host: borax38\n     1319    PID:        48066\n     1320    ------------------------------------------------------------------\n             --------\n  >> 1321    FAILED: Command exited with status 255:\n     1322        './oshmem_shmalloc'\n     1323    ------------------------------------------------------------------\n             --------\n     1324    The application appears to have been direct launched using \"srun\",\n     1325    but OMPI was not built with SLURM support. This usually happens\n     1326    when OMPI was not configured --with-slurm and we weren't able\n     1327    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_strided_puts'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 48144, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        48144\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1400    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1401    double check that everything has shut down cleanly.\n     1402    \n     1403    Local host: borax38\n     1404    PID:        48144\n     1405    ------------------------------------------------------------------\n             --------\n  >> 1406    FAILED: Command exited with status 255:\n     1407        './oshmem_strided_puts'\n     1408    ------------------------------------------------------------------\n             --------\n     1409    The application appears to have been direct launched using \"srun\",\n     1410    but OMPI was not built with SLURM support. This usually happens\n     1411    when OMPI was not configured --with-slurm and we weren't able\n     1412    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_symmetric_data'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 48221, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        48221\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1485    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1486    double check that everything has shut down cleanly.\n     1487    \n     1488    Local host: borax38\n     1489    PID:        48221\n     1490    ------------------------------------------------------------------\n             --------\n  >> 1491    FAILED: Command exited with status 255:\n     1492        './oshmem_symmetric_data'\n     1493    ------------------------------------------------------------------\n             --------\n     1494    The application appears to have been direct launched using \"srun\",\n     1495    but OMPI was not built with SLURM support. This usually happens\n     1496    when OMPI was not configured --with-slurm and we weren't able\n     1497    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:48298] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1564    ------------------------------------------------------------------\n             --------\n     1565    *** An error occurred in MPI_Init\n     1566    *** on a NULL communicator\n     1567    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1568    ***    and potentially your MPI job)\n     1569    [borax38:48298] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1570    FAILED: Command exited with status 1:\n     1571        './ring_c'\n     1572    ------------------------------------------------------------------\n             --------\n     1573    The application appears to have been direct launched using \"srun\",\n     1574    but OMPI was not built with SLURM support. This usually happens\n     1575    when OMPI was not configured --with-slurm and we weren't able\n     1576    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:48374] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1637    ------------------------------------------------------------------\n             --------\n     1638    *** An error occurred in MPI_Init\n     1639    *** on a NULL communicator\n     1640    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1641    ***    and potentially your MPI job)\n     1642    [borax38:48374] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1643    FAILED: Command exited with status 1:\n     1644        './ring_mpifh'\n     1645    ------------------------------------------------------------------\n             --------\n     1646    The application appears to have been direct launched using \"srun\",\n     1647    but OMPI was not built with SLURM support. This usually happens\n     1648    when OMPI was not configured --with-slurm and we weren't able\n     1649    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 48450, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        48450\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1716    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1717    double check that everything has shut down cleanly.\n     1718    \n     1719    Local host: borax38\n     1720    PID:        48450\n     1721    ------------------------------------------------------------------\n             --------\n  >> 1722    FAILED: Command exited with status 255:\n     1723        './ring_oshmem'\n     1724    ------------------------------------------------------------------\n             --------\n     1725    The application appears to have been direct launched using \"srun\",\n     1726    but OMPI was not built with SLURM support. This usually happens\n     1727    when OMPI was not configured --with-slurm and we weren't able\n     1728    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 48527, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        48527\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1801    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1802    double check that everything has shut down cleanly.\n     1803    \n     1804    Local host: borax38\n     1805    PID:        48527\n     1806    ------------------------------------------------------------------\n             --------\n  >> 1807    FAILED: Command exited with status 255:\n     1808        './ring_oshmemfh'\n     1809    ------------------------------------------------------------------\n             --------\n     1810    The application appears to have been direct launched using \"srun\",\n     1811    but OMPI was not built with SLURM support. This usually happens\n     1812    when OMPI was not configured --with-slurm and we weren't able\n     1813    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:48603] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1880    ------------------------------------------------------------------\n             --------\n     1881    *** An error occurred in MPI_Init\n     1882    *** on a NULL communicator\n     1883    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1884    ***    and potentially your MPI job)\n     1885    [borax38:48603] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1886    FAILED: Command exited with status 1:\n     1887        './ring_usempi'\n     1888    ------------------------------------------------------------------\n             --------\n     1889    The application appears to have been direct launched using \"srun\",\n     1890    but OMPI was not built with SLURM support. This usually happens\n     1891    when OMPI was not configured --with-slurm and we weren't able\n     1892    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:48679] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1953    ------------------------------------------------------------------\n             --------\n     1954    *** An error occurred in MPI_Init\n     1955    *** on a NULL communicator\n     1956    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1957    ***    and potentially your MPI job)\n     1958    [borax38:48679] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1959    FAILED: Command exited with status 1:\n     1960        './ring_usempif08'\n     1961    ------------------------------------------------------------------\n             --------\n     1962    The application appears to have been direct launched using \"srun\",\n     1963    but OMPI was not built with SLURM support. This usually happens\n     1964    when OMPI was not configured --with-slurm and we weren't able\n     1965    to discover a SLURM installation in the usual places.\n\n\n\n/p/vast1/build/spack/lib/spack/spack/build_environment.py:1061, in _setup_pkg_and_run:\n       1058        tb_string = traceback.format_exc()\n       1059\n       1060        # build up some context from the offending package so we can\n  >>   1061        # show that, too.\n       1062        package_context = get_package_context(tb)\n       1063\n       1064        logfile = None\n\nSee test log for details:\n  /g/g0/sochat1/.spack/test/wjkcucjdda4pthihxgsnrwpoy4cqlmqe/openmpi-3.1.4-xf5lntw-test-out.txt\n\n==> Error: 1 test(s) in the suite failed.\n\n======================== 1 failed, 0 passed of 1 specs =========================\n",
                    "return_code": 1,
                    "prediction": false,
                    "command": "/p/vast1/build/spack/bin/spack test run /xf5lnt"
                }
            ],
            "symbolator": [
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ],
            "libabigail": [
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ]
        },
        "libs": {
            "spliced": [
                {
                    "dep": "pkgconf@1.5.4%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.5.4-5bz4ozzvn3jbawxtzq6hvkfckuo2szic/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ],
            "original": [
                {
                    "dep": "pkgconf@1.8.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ]
        },
        "experiment": "experiment",
        "result": "splice-success",
        "success": true,
        "splice": "pkgconf@1.5.4",
        "package": "openmpi@3.1.4"
    },
    {
        "binaries": {
            "spliced": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper"
            ],
            "original": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orted"
            ]
        },
        "predictions": {
            "spack-test": [
                {
                    "message": "==> Spack test wjkcucjdda4pthihxgsnrwpoy4cqlmqe\n==> Testing package openmpi-3.1.4-xf5lntw\n==> Error: TestFailure: 18 tests failed.\n\n\nCommand exited with status 1:\n    './hello_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:49059] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     592    -------------------------------------------------------------------\n            -------\n     593    *** An error occurred in MPI_Init\n     594    *** on a NULL communicator\n     595    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     596    ***    and potentially your MPI job)\n     597    [borax38:49059] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 598    FAILED: Command exited with status 1:\n     599        './hello_c'\n     600    -------------------------------------------------------------------\n            -------\n     601    The application appears to have been direct launched using \"srun\",\n     602    but OMPI was not built with SLURM support. This usually happens\n     603    when OMPI was not configured --with-slurm and we weren't able\n     604    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:49060] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     665    -------------------------------------------------------------------\n            -------\n     666    *** An error occurred in MPI_Init\n     667    *** on a NULL communicator\n     668    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     669    ***    and potentially your MPI job)\n     670    [borax38:49060] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 671    FAILED: Command exited with status 1:\n     672        './hello_mpifh'\n     673    -------------------------------------------------------------------\n            -------\n     674    The application appears to have been direct launched using \"srun\",\n     675    but OMPI was not built with SLURM support. This usually happens\n     676    when OMPI was not configured --with-slurm and we weren't able\n     677    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 49061, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        49061\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     744    of its peer processes in the job will be killed properly.  You shou\n            ld\n     745    double check that everything has shut down cleanly.\n     746    \n     747    Local host: borax38\n     748    PID:        49061\n     749    -------------------------------------------------------------------\n            -------\n  >> 750    FAILED: Command exited with status 255:\n     751        './hello_oshmem'\n     752    -------------------------------------------------------------------\n            -------\n     753    The application appears to have been direct launched using \"srun\",\n     754    but OMPI was not built with SLURM support. This usually happens\n     755    when OMPI was not configured --with-slurm and we weren't able\n     756    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemcxx'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 49138, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        49138\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     829    of its peer processes in the job will be killed properly.  You shou\n            ld\n     830    double check that everything has shut down cleanly.\n     831    \n     832    Local host: borax38\n     833    PID:        49138\n     834    -------------------------------------------------------------------\n            -------\n  >> 835    FAILED: Command exited with status 255:\n     836        './hello_oshmemcxx'\n     837    -------------------------------------------------------------------\n            -------\n     838    The application appears to have been direct launched using \"srun\",\n     839    but OMPI was not built with SLURM support. This usually happens\n     840    when OMPI was not configured --with-slurm and we weren't able\n     841    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 49215, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        49215\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     914    of its peer processes in the job will be killed properly.  You shou\n            ld\n     915    double check that everything has shut down cleanly.\n     916    \n     917    Local host: borax38\n     918    PID:        49215\n     919    -------------------------------------------------------------------\n            -------\n  >> 920    FAILED: Command exited with status 255:\n     921        './hello_oshmemfh'\n     922    -------------------------------------------------------------------\n            -------\n     923    The application appears to have been direct launched using \"srun\",\n     924    but OMPI was not built with SLURM support. This usually happens\n     925    when OMPI was not configured --with-slurm and we weren't able\n     926    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:49291] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     993    -------------------------------------------------------------------\n            -------\n     994    *** An error occurred in MPI_Init\n     995    *** on a NULL communicator\n     996    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     997    ***    and potentially your MPI job)\n     998    [borax38:49291] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 999    FAILED: Command exited with status 1:\n     1000       './hello_usempi'\n     1001   -------------------------------------------------------------------\n            -------\n     1002   The application appears to have been direct launched using \"srun\",\n     1003   but OMPI was not built with SLURM support. This usually happens\n     1004   when OMPI was not configured --with-slurm and we weren't able\n     1005   to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:49367] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1066    ------------------------------------------------------------------\n             --------\n     1067    *** An error occurred in MPI_Init\n     1068    *** on a NULL communicator\n     1069    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1070    ***    and potentially your MPI job)\n     1071    [borax38:49367] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1072    FAILED: Command exited with status 1:\n     1073        './hello_usempif08'\n     1074    ------------------------------------------------------------------\n             --------\n     1075    The application appears to have been direct launched using \"srun\",\n     1076    but OMPI was not built with SLURM support. This usually happens\n     1077    when OMPI was not configured --with-slurm and we weren't able\n     1078    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_circular_shift'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 49444, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        49444\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1145    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1146    double check that everything has shut down cleanly.\n     1147    \n     1148    Local host: borax38\n     1149    PID:        49444\n     1150    ------------------------------------------------------------------\n             --------\n  >> 1151    FAILED: Command exited with status 255:\n     1152        './oshmem_circular_shift'\n     1153    ------------------------------------------------------------------\n             --------\n     1154    The application appears to have been direct launched using \"srun\",\n     1155    but OMPI was not built with SLURM support. This usually happens\n     1156    when OMPI was not configured --with-slurm and we weren't able\n     1157    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_max_reduction'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 49520, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        49520\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1230    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1231    double check that everything has shut down cleanly.\n     1232    \n     1233    Local host: borax38\n     1234    PID:        49520\n     1235    ------------------------------------------------------------------\n             --------\n  >> 1236    FAILED: Command exited with status 255:\n     1237        './oshmem_max_reduction'\n     1238    ------------------------------------------------------------------\n             --------\n     1239    The application appears to have been direct launched using \"srun\",\n     1240    but OMPI was not built with SLURM support. This usually happens\n     1241    when OMPI was not configured --with-slurm and we weren't able\n     1242    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_shmalloc'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 49596, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        49596\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1315    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1316    double check that everything has shut down cleanly.\n     1317    \n     1318    Local host: borax38\n     1319    PID:        49596\n     1320    ------------------------------------------------------------------\n             --------\n  >> 1321    FAILED: Command exited with status 255:\n     1322        './oshmem_shmalloc'\n     1323    ------------------------------------------------------------------\n             --------\n     1324    The application appears to have been direct launched using \"srun\",\n     1325    but OMPI was not built with SLURM support. This usually happens\n     1326    when OMPI was not configured --with-slurm and we weren't able\n     1327    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_strided_puts'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 49672, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        49672\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1400    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1401    double check that everything has shut down cleanly.\n     1402    \n     1403    Local host: borax38\n     1404    PID:        49672\n     1405    ------------------------------------------------------------------\n             --------\n  >> 1406    FAILED: Command exited with status 255:\n     1407        './oshmem_strided_puts'\n     1408    ------------------------------------------------------------------\n             --------\n     1409    The application appears to have been direct launched using \"srun\",\n     1410    but OMPI was not built with SLURM support. This usually happens\n     1411    when OMPI was not configured --with-slurm and we weren't able\n     1412    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_symmetric_data'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 49749, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        49749\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1485    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1486    double check that everything has shut down cleanly.\n     1487    \n     1488    Local host: borax38\n     1489    PID:        49749\n     1490    ------------------------------------------------------------------\n             --------\n  >> 1491    FAILED: Command exited with status 255:\n     1492        './oshmem_symmetric_data'\n     1493    ------------------------------------------------------------------\n             --------\n     1494    The application appears to have been direct launched using \"srun\",\n     1495    but OMPI was not built with SLURM support. This usually happens\n     1496    when OMPI was not configured --with-slurm and we weren't able\n     1497    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:49826] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1564    ------------------------------------------------------------------\n             --------\n     1565    *** An error occurred in MPI_Init\n     1566    *** on a NULL communicator\n     1567    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1568    ***    and potentially your MPI job)\n     1569    [borax38:49826] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1570    FAILED: Command exited with status 1:\n     1571        './ring_c'\n     1572    ------------------------------------------------------------------\n             --------\n     1573    The application appears to have been direct launched using \"srun\",\n     1574    but OMPI was not built with SLURM support. This usually happens\n     1575    when OMPI was not configured --with-slurm and we weren't able\n     1576    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:49902] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1637    ------------------------------------------------------------------\n             --------\n     1638    *** An error occurred in MPI_Init\n     1639    *** on a NULL communicator\n     1640    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1641    ***    and potentially your MPI job)\n     1642    [borax38:49902] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1643    FAILED: Command exited with status 1:\n     1644        './ring_mpifh'\n     1645    ------------------------------------------------------------------\n             --------\n     1646    The application appears to have been direct launched using \"srun\",\n     1647    but OMPI was not built with SLURM support. This usually happens\n     1648    when OMPI was not configured --with-slurm and we weren't able\n     1649    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 49978, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        49978\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1716    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1717    double check that everything has shut down cleanly.\n     1718    \n     1719    Local host: borax38\n     1720    PID:        49978\n     1721    ------------------------------------------------------------------\n             --------\n  >> 1722    FAILED: Command exited with status 255:\n     1723        './ring_oshmem'\n     1724    ------------------------------------------------------------------\n             --------\n     1725    The application appears to have been direct launched using \"srun\",\n     1726    but OMPI was not built with SLURM support. This usually happens\n     1727    when OMPI was not configured --with-slurm and we weren't able\n     1728    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 50056, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        50056\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1801    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1802    double check that everything has shut down cleanly.\n     1803    \n     1804    Local host: borax38\n     1805    PID:        50056\n     1806    ------------------------------------------------------------------\n             --------\n  >> 1807    FAILED: Command exited with status 255:\n     1808        './ring_oshmemfh'\n     1809    ------------------------------------------------------------------\n             --------\n     1810    The application appears to have been direct launched using \"srun\",\n     1811    but OMPI was not built with SLURM support. This usually happens\n     1812    when OMPI was not configured --with-slurm and we weren't able\n     1813    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:50132] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1880    ------------------------------------------------------------------\n             --------\n     1881    *** An error occurred in MPI_Init\n     1882    *** on a NULL communicator\n     1883    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1884    ***    and potentially your MPI job)\n     1885    [borax38:50132] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1886    FAILED: Command exited with status 1:\n     1887        './ring_usempi'\n     1888    ------------------------------------------------------------------\n             --------\n     1889    The application appears to have been direct launched using \"srun\",\n     1890    but OMPI was not built with SLURM support. This usually happens\n     1891    when OMPI was not configured --with-slurm and we weren't able\n     1892    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:50209] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1953    ------------------------------------------------------------------\n             --------\n     1954    *** An error occurred in MPI_Init\n     1955    *** on a NULL communicator\n     1956    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1957    ***    and potentially your MPI job)\n     1958    [borax38:50209] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1959    FAILED: Command exited with status 1:\n     1960        './ring_usempif08'\n     1961    ------------------------------------------------------------------\n             --------\n     1962    The application appears to have been direct launched using \"srun\",\n     1963    but OMPI was not built with SLURM support. This usually happens\n     1964    when OMPI was not configured --with-slurm and we weren't able\n     1965    to discover a SLURM installation in the usual places.\n\n\n\n/p/vast1/build/spack/lib/spack/spack/build_environment.py:1061, in _setup_pkg_and_run:\n       1058        tb_string = traceback.format_exc()\n       1059\n       1060        # build up some context from the offending package so we can\n  >>   1061        # show that, too.\n       1062        package_context = get_package_context(tb)\n       1063\n       1064        logfile = None\n\nSee test log for details:\n  /g/g0/sochat1/.spack/test/wjkcucjdda4pthihxgsnrwpoy4cqlmqe/openmpi-3.1.4-xf5lntw-test-out.txt\n\n==> Error: 1 test(s) in the suite failed.\n\n======================== 1 failed, 0 passed of 1 specs =========================\n",
                    "return_code": 1,
                    "prediction": false,
                    "command": "/p/vast1/build/spack/bin/spack test run /xf5lnt"
                }
            ],
            "symbolator": [
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ],
            "libabigail": [
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ]
        },
        "libs": {
            "spliced": [
                {
                    "dep": "pkgconf@1.4.2%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.2-jpd7blzfco2pa72yl2jvfxu5xs3vucqf/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ],
            "original": [
                {
                    "dep": "pkgconf@1.8.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ]
        },
        "experiment": "experiment",
        "result": "splice-success",
        "success": true,
        "splice": "pkgconf@1.4.2",
        "package": "openmpi@3.1.4"
    },
    {
        "binaries": {
            "spliced": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper"
            ],
            "original": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orted"
            ]
        },
        "predictions": {
            "spack-test": [
                {
                    "message": "==> Spack test wjkcucjdda4pthihxgsnrwpoy4cqlmqe\n==> Testing package openmpi-3.1.4-xf5lntw\n==> Error: TestFailure: 18 tests failed.\n\n\nCommand exited with status 1:\n    './hello_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:50590] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     592    -------------------------------------------------------------------\n            -------\n     593    *** An error occurred in MPI_Init\n     594    *** on a NULL communicator\n     595    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     596    ***    and potentially your MPI job)\n     597    [borax38:50590] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 598    FAILED: Command exited with status 1:\n     599        './hello_c'\n     600    -------------------------------------------------------------------\n            -------\n     601    The application appears to have been direct launched using \"srun\",\n     602    but OMPI was not built with SLURM support. This usually happens\n     603    when OMPI was not configured --with-slurm and we weren't able\n     604    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:50591] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     665    -------------------------------------------------------------------\n            -------\n     666    *** An error occurred in MPI_Init\n     667    *** on a NULL communicator\n     668    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     669    ***    and potentially your MPI job)\n     670    [borax38:50591] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 671    FAILED: Command exited with status 1:\n     672        './hello_mpifh'\n     673    -------------------------------------------------------------------\n            -------\n     674    The application appears to have been direct launched using \"srun\",\n     675    but OMPI was not built with SLURM support. This usually happens\n     676    when OMPI was not configured --with-slurm and we weren't able\n     677    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 50592, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        50592\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     744    of its peer processes in the job will be killed properly.  You shou\n            ld\n     745    double check that everything has shut down cleanly.\n     746    \n     747    Local host: borax38\n     748    PID:        50592\n     749    -------------------------------------------------------------------\n            -------\n  >> 750    FAILED: Command exited with status 255:\n     751        './hello_oshmem'\n     752    -------------------------------------------------------------------\n            -------\n     753    The application appears to have been direct launched using \"srun\",\n     754    but OMPI was not built with SLURM support. This usually happens\n     755    when OMPI was not configured --with-slurm and we weren't able\n     756    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemcxx'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 50669, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        50669\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     829    of its peer processes in the job will be killed properly.  You shou\n            ld\n     830    double check that everything has shut down cleanly.\n     831    \n     832    Local host: borax38\n     833    PID:        50669\n     834    -------------------------------------------------------------------\n            -------\n  >> 835    FAILED: Command exited with status 255:\n     836        './hello_oshmemcxx'\n     837    -------------------------------------------------------------------\n            -------\n     838    The application appears to have been direct launched using \"srun\",\n     839    but OMPI was not built with SLURM support. This usually happens\n     840    when OMPI was not configured --with-slurm and we weren't able\n     841    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 50745, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        50745\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     914    of its peer processes in the job will be killed properly.  You shou\n            ld\n     915    double check that everything has shut down cleanly.\n     916    \n     917    Local host: borax38\n     918    PID:        50745\n     919    -------------------------------------------------------------------\n            -------\n  >> 920    FAILED: Command exited with status 255:\n     921        './hello_oshmemfh'\n     922    -------------------------------------------------------------------\n            -------\n     923    The application appears to have been direct launched using \"srun\",\n     924    but OMPI was not built with SLURM support. This usually happens\n     925    when OMPI was not configured --with-slurm and we weren't able\n     926    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:50821] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     993    -------------------------------------------------------------------\n            -------\n     994    *** An error occurred in MPI_Init\n     995    *** on a NULL communicator\n     996    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     997    ***    and potentially your MPI job)\n     998    [borax38:50821] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 999    FAILED: Command exited with status 1:\n     1000       './hello_usempi'\n     1001   -------------------------------------------------------------------\n            -------\n     1002   The application appears to have been direct launched using \"srun\",\n     1003   but OMPI was not built with SLURM support. This usually happens\n     1004   when OMPI was not configured --with-slurm and we weren't able\n     1005   to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:50898] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1066    ------------------------------------------------------------------\n             --------\n     1067    *** An error occurred in MPI_Init\n     1068    *** on a NULL communicator\n     1069    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1070    ***    and potentially your MPI job)\n     1071    [borax38:50898] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1072    FAILED: Command exited with status 1:\n     1073        './hello_usempif08'\n     1074    ------------------------------------------------------------------\n             --------\n     1075    The application appears to have been direct launched using \"srun\",\n     1076    but OMPI was not built with SLURM support. This usually happens\n     1077    when OMPI was not configured --with-slurm and we weren't able\n     1078    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_circular_shift'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 50975, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        50975\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1145    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1146    double check that everything has shut down cleanly.\n     1147    \n     1148    Local host: borax38\n     1149    PID:        50975\n     1150    ------------------------------------------------------------------\n             --------\n  >> 1151    FAILED: Command exited with status 255:\n     1152        './oshmem_circular_shift'\n     1153    ------------------------------------------------------------------\n             --------\n     1154    The application appears to have been direct launched using \"srun\",\n     1155    but OMPI was not built with SLURM support. This usually happens\n     1156    when OMPI was not configured --with-slurm and we weren't able\n     1157    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_max_reduction'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 51051, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        51051\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1230    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1231    double check that everything has shut down cleanly.\n     1232    \n     1233    Local host: borax38\n     1234    PID:        51051\n     1235    ------------------------------------------------------------------\n             --------\n  >> 1236    FAILED: Command exited with status 255:\n     1237        './oshmem_max_reduction'\n     1238    ------------------------------------------------------------------\n             --------\n     1239    The application appears to have been direct launched using \"srun\",\n     1240    but OMPI was not built with SLURM support. This usually happens\n     1241    when OMPI was not configured --with-slurm and we weren't able\n     1242    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_shmalloc'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 51127, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        51127\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1315    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1316    double check that everything has shut down cleanly.\n     1317    \n     1318    Local host: borax38\n     1319    PID:        51127\n     1320    ------------------------------------------------------------------\n             --------\n  >> 1321    FAILED: Command exited with status 255:\n     1322        './oshmem_shmalloc'\n     1323    ------------------------------------------------------------------\n             --------\n     1324    The application appears to have been direct launched using \"srun\",\n     1325    but OMPI was not built with SLURM support. This usually happens\n     1326    when OMPI was not configured --with-slurm and we weren't able\n     1327    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_strided_puts'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 51203, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        51203\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1400    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1401    double check that everything has shut down cleanly.\n     1402    \n     1403    Local host: borax38\n     1404    PID:        51203\n     1405    ------------------------------------------------------------------\n             --------\n  >> 1406    FAILED: Command exited with status 255:\n     1407        './oshmem_strided_puts'\n     1408    ------------------------------------------------------------------\n             --------\n     1409    The application appears to have been direct launched using \"srun\",\n     1410    but OMPI was not built with SLURM support. This usually happens\n     1411    when OMPI was not configured --with-slurm and we weren't able\n     1412    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_symmetric_data'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 51279, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        51279\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1485    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1486    double check that everything has shut down cleanly.\n     1487    \n     1488    Local host: borax38\n     1489    PID:        51279\n     1490    ------------------------------------------------------------------\n             --------\n  >> 1491    FAILED: Command exited with status 255:\n     1492        './oshmem_symmetric_data'\n     1493    ------------------------------------------------------------------\n             --------\n     1494    The application appears to have been direct launched using \"srun\",\n     1495    but OMPI was not built with SLURM support. This usually happens\n     1496    when OMPI was not configured --with-slurm and we weren't able\n     1497    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:51356] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1564    ------------------------------------------------------------------\n             --------\n     1565    *** An error occurred in MPI_Init\n     1566    *** on a NULL communicator\n     1567    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1568    ***    and potentially your MPI job)\n     1569    [borax38:51356] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1570    FAILED: Command exited with status 1:\n     1571        './ring_c'\n     1572    ------------------------------------------------------------------\n             --------\n     1573    The application appears to have been direct launched using \"srun\",\n     1574    but OMPI was not built with SLURM support. This usually happens\n     1575    when OMPI was not configured --with-slurm and we weren't able\n     1576    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:51432] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1637    ------------------------------------------------------------------\n             --------\n     1638    *** An error occurred in MPI_Init\n     1639    *** on a NULL communicator\n     1640    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1641    ***    and potentially your MPI job)\n     1642    [borax38:51432] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1643    FAILED: Command exited with status 1:\n     1644        './ring_mpifh'\n     1645    ------------------------------------------------------------------\n             --------\n     1646    The application appears to have been direct launched using \"srun\",\n     1647    but OMPI was not built with SLURM support. This usually happens\n     1648    when OMPI was not configured --with-slurm and we weren't able\n     1649    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 51508, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        51508\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1716    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1717    double check that everything has shut down cleanly.\n     1718    \n     1719    Local host: borax38\n     1720    PID:        51508\n     1721    ------------------------------------------------------------------\n             --------\n  >> 1722    FAILED: Command exited with status 255:\n     1723        './ring_oshmem'\n     1724    ------------------------------------------------------------------\n             --------\n     1725    The application appears to have been direct launched using \"srun\",\n     1726    but OMPI was not built with SLURM support. This usually happens\n     1727    when OMPI was not configured --with-slurm and we weren't able\n     1728    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 51584, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        51584\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1801    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1802    double check that everything has shut down cleanly.\n     1803    \n     1804    Local host: borax38\n     1805    PID:        51584\n     1806    ------------------------------------------------------------------\n             --------\n  >> 1807    FAILED: Command exited with status 255:\n     1808        './ring_oshmemfh'\n     1809    ------------------------------------------------------------------\n             --------\n     1810    The application appears to have been direct launched using \"srun\",\n     1811    but OMPI was not built with SLURM support. This usually happens\n     1812    when OMPI was not configured --with-slurm and we weren't able\n     1813    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:51662] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1880    ------------------------------------------------------------------\n             --------\n     1881    *** An error occurred in MPI_Init\n     1882    *** on a NULL communicator\n     1883    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1884    ***    and potentially your MPI job)\n     1885    [borax38:51662] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1886    FAILED: Command exited with status 1:\n     1887        './ring_usempi'\n     1888    ------------------------------------------------------------------\n             --------\n     1889    The application appears to have been direct launched using \"srun\",\n     1890    but OMPI was not built with SLURM support. This usually happens\n     1891    when OMPI was not configured --with-slurm and we weren't able\n     1892    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:51738] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1953    ------------------------------------------------------------------\n             --------\n     1954    *** An error occurred in MPI_Init\n     1955    *** on a NULL communicator\n     1956    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1957    ***    and potentially your MPI job)\n     1958    [borax38:51738] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1959    FAILED: Command exited with status 1:\n     1960        './ring_usempif08'\n     1961    ------------------------------------------------------------------\n             --------\n     1962    The application appears to have been direct launched using \"srun\",\n     1963    but OMPI was not built with SLURM support. This usually happens\n     1964    when OMPI was not configured --with-slurm and we weren't able\n     1965    to discover a SLURM installation in the usual places.\n\n\n\n/p/vast1/build/spack/lib/spack/spack/build_environment.py:1061, in _setup_pkg_and_run:\n       1058        tb_string = traceback.format_exc()\n       1059\n       1060        # build up some context from the offending package so we can\n  >>   1061        # show that, too.\n       1062        package_context = get_package_context(tb)\n       1063\n       1064        logfile = None\n\nSee test log for details:\n  /g/g0/sochat1/.spack/test/wjkcucjdda4pthihxgsnrwpoy4cqlmqe/openmpi-3.1.4-xf5lntw-test-out.txt\n\n==> Error: 1 test(s) in the suite failed.\n\n======================== 1 failed, 0 passed of 1 specs =========================\n",
                    "return_code": 1,
                    "prediction": false,
                    "command": "/p/vast1/build/spack/bin/spack test run /xf5lnt"
                }
            ],
            "symbolator": [
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ],
            "libabigail": [
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                },
                {
                    "message": "",
                    "return_code": 0,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0",
                    "prediction": true
                }
            ]
        },
        "libs": {
            "spliced": [
                {
                    "dep": "pkgconf@1.4.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.4.0-e3inpigv7qvo7xyvjfpg6j2yfvggdhfa/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ],
            "original": [
                {
                    "dep": "pkgconf@1.8.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ]
        },
        "experiment": "experiment",
        "result": "splice-success",
        "success": true,
        "splice": "pkgconf@1.4.0",
        "package": "openmpi@3.1.4"
    },
    {
        "binaries": {
            "spliced": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper"
            ],
            "original": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orted"
            ]
        },
        "predictions": {
            "spack-test": [
                {
                    "message": "==> Spack test wjkcucjdda4pthihxgsnrwpoy4cqlmqe\n==> Testing package openmpi-3.1.4-xf5lntw\n==> Error: TestFailure: 18 tests failed.\n\n\nCommand exited with status 1:\n    './hello_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:52119] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     592    -------------------------------------------------------------------\n            -------\n     593    *** An error occurred in MPI_Init\n     594    *** on a NULL communicator\n     595    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     596    ***    and potentially your MPI job)\n     597    [borax38:52119] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 598    FAILED: Command exited with status 1:\n     599        './hello_c'\n     600    -------------------------------------------------------------------\n            -------\n     601    The application appears to have been direct launched using \"srun\",\n     602    but OMPI was not built with SLURM support. This usually happens\n     603    when OMPI was not configured --with-slurm and we weren't able\n     604    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:52120] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     665    -------------------------------------------------------------------\n            -------\n     666    *** An error occurred in MPI_Init\n     667    *** on a NULL communicator\n     668    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     669    ***    and potentially your MPI job)\n     670    [borax38:52120] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 671    FAILED: Command exited with status 1:\n     672        './hello_mpifh'\n     673    -------------------------------------------------------------------\n            -------\n     674    The application appears to have been direct launched using \"srun\",\n     675    but OMPI was not built with SLURM support. This usually happens\n     676    when OMPI was not configured --with-slurm and we weren't able\n     677    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 52196, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        52196\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     744    of its peer processes in the job will be killed properly.  You shou\n            ld\n     745    double check that everything has shut down cleanly.\n     746    \n     747    Local host: borax38\n     748    PID:        52196\n     749    -------------------------------------------------------------------\n            -------\n  >> 750    FAILED: Command exited with status 255:\n     751        './hello_oshmem'\n     752    -------------------------------------------------------------------\n            -------\n     753    The application appears to have been direct launched using \"srun\",\n     754    but OMPI was not built with SLURM support. This usually happens\n     755    when OMPI was not configured --with-slurm and we weren't able\n     756    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemcxx'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 52274, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        52274\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     829    of its peer processes in the job will be killed properly.  You shou\n            ld\n     830    double check that everything has shut down cleanly.\n     831    \n     832    Local host: borax38\n     833    PID:        52274\n     834    -------------------------------------------------------------------\n            -------\n  >> 835    FAILED: Command exited with status 255:\n     836        './hello_oshmemcxx'\n     837    -------------------------------------------------------------------\n            -------\n     838    The application appears to have been direct launched using \"srun\",\n     839    but OMPI was not built with SLURM support. This usually happens\n     840    when OMPI was not configured --with-slurm and we weren't able\n     841    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 52350, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        52350\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     914    of its peer processes in the job will be killed properly.  You shou\n            ld\n     915    double check that everything has shut down cleanly.\n     916    \n     917    Local host: borax38\n     918    PID:        52350\n     919    -------------------------------------------------------------------\n            -------\n  >> 920    FAILED: Command exited with status 255:\n     921        './hello_oshmemfh'\n     922    -------------------------------------------------------------------\n            -------\n     923    The application appears to have been direct launched using \"srun\",\n     924    but OMPI was not built with SLURM support. This usually happens\n     925    when OMPI was not configured --with-slurm and we weren't able\n     926    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:52426] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     993    -------------------------------------------------------------------\n            -------\n     994    *** An error occurred in MPI_Init\n     995    *** on a NULL communicator\n     996    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     997    ***    and potentially your MPI job)\n     998    [borax38:52426] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 999    FAILED: Command exited with status 1:\n     1000       './hello_usempi'\n     1001   -------------------------------------------------------------------\n            -------\n     1002   The application appears to have been direct launched using \"srun\",\n     1003   but OMPI was not built with SLURM support. This usually happens\n     1004   when OMPI was not configured --with-slurm and we weren't able\n     1005   to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:52503] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1066    ------------------------------------------------------------------\n             --------\n     1067    *** An error occurred in MPI_Init\n     1068    *** on a NULL communicator\n     1069    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1070    ***    and potentially your MPI job)\n     1071    [borax38:52503] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1072    FAILED: Command exited with status 1:\n     1073        './hello_usempif08'\n     1074    ------------------------------------------------------------------\n             --------\n     1075    The application appears to have been direct launched using \"srun\",\n     1076    but OMPI was not built with SLURM support. This usually happens\n     1077    when OMPI was not configured --with-slurm and we weren't able\n     1078    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_circular_shift'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 52580, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        52580\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1145    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1146    double check that everything has shut down cleanly.\n     1147    \n     1148    Local host: borax38\n     1149    PID:        52580\n     1150    ------------------------------------------------------------------\n             --------\n  >> 1151    FAILED: Command exited with status 255:\n     1152        './oshmem_circular_shift'\n     1153    ------------------------------------------------------------------\n             --------\n     1154    The application appears to have been direct launched using \"srun\",\n     1155    but OMPI was not built with SLURM support. This usually happens\n     1156    when OMPI was not configured --with-slurm and we weren't able\n     1157    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_max_reduction'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 52656, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        52656\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1230    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1231    double check that everything has shut down cleanly.\n     1232    \n     1233    Local host: borax38\n     1234    PID:        52656\n     1235    ------------------------------------------------------------------\n             --------\n  >> 1236    FAILED: Command exited with status 255:\n     1237        './oshmem_max_reduction'\n     1238    ------------------------------------------------------------------\n             --------\n     1239    The application appears to have been direct launched using \"srun\",\n     1240    but OMPI was not built with SLURM support. This usually happens\n     1241    when OMPI was not configured --with-slurm and we weren't able\n     1242    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_shmalloc'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 52732, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        52732\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1315    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1316    double check that everything has shut down cleanly.\n     1317    \n     1318    Local host: borax38\n     1319    PID:        52732\n     1320    ------------------------------------------------------------------\n             --------\n  >> 1321    FAILED: Command exited with status 255:\n     1322        './oshmem_shmalloc'\n     1323    ------------------------------------------------------------------\n             --------\n     1324    The application appears to have been direct launched using \"srun\",\n     1325    but OMPI was not built with SLURM support. This usually happens\n     1326    when OMPI was not configured --with-slurm and we weren't able\n     1327    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_strided_puts'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 52808, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        52808\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1400    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1401    double check that everything has shut down cleanly.\n     1402    \n     1403    Local host: borax38\n     1404    PID:        52808\n     1405    ------------------------------------------------------------------\n             --------\n  >> 1406    FAILED: Command exited with status 255:\n     1407        './oshmem_strided_puts'\n     1408    ------------------------------------------------------------------\n             --------\n     1409    The application appears to have been direct launched using \"srun\",\n     1410    but OMPI was not built with SLURM support. This usually happens\n     1411    when OMPI was not configured --with-slurm and we weren't able\n     1412    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_symmetric_data'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 52884, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        52884\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1485    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1486    double check that everything has shut down cleanly.\n     1487    \n     1488    Local host: borax38\n     1489    PID:        52884\n     1490    ------------------------------------------------------------------\n             --------\n  >> 1491    FAILED: Command exited with status 255:\n     1492        './oshmem_symmetric_data'\n     1493    ------------------------------------------------------------------\n             --------\n     1494    The application appears to have been direct launched using \"srun\",\n     1495    but OMPI was not built with SLURM support. This usually happens\n     1496    when OMPI was not configured --with-slurm and we weren't able\n     1497    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:52962] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1564    ------------------------------------------------------------------\n             --------\n     1565    *** An error occurred in MPI_Init\n     1566    *** on a NULL communicator\n     1567    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1568    ***    and potentially your MPI job)\n     1569    [borax38:52962] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1570    FAILED: Command exited with status 1:\n     1571        './ring_c'\n     1572    ------------------------------------------------------------------\n             --------\n     1573    The application appears to have been direct launched using \"srun\",\n     1574    but OMPI was not built with SLURM support. This usually happens\n     1575    when OMPI was not configured --with-slurm and we weren't able\n     1576    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:53038] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1637    ------------------------------------------------------------------\n             --------\n     1638    *** An error occurred in MPI_Init\n     1639    *** on a NULL communicator\n     1640    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1641    ***    and potentially your MPI job)\n     1642    [borax38:53038] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1643    FAILED: Command exited with status 1:\n     1644        './ring_mpifh'\n     1645    ------------------------------------------------------------------\n             --------\n     1646    The application appears to have been direct launched using \"srun\",\n     1647    but OMPI was not built with SLURM support. This usually happens\n     1648    when OMPI was not configured --with-slurm and we weren't able\n     1649    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 53114, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        53114\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1716    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1717    double check that everything has shut down cleanly.\n     1718    \n     1719    Local host: borax38\n     1720    PID:        53114\n     1721    ------------------------------------------------------------------\n             --------\n  >> 1722    FAILED: Command exited with status 255:\n     1723        './ring_oshmem'\n     1724    ------------------------------------------------------------------\n             --------\n     1725    The application appears to have been direct launched using \"srun\",\n     1726    but OMPI was not built with SLURM support. This usually happens\n     1727    when OMPI was not configured --with-slurm and we weren't able\n     1728    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 53191, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        53191\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1801    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1802    double check that everything has shut down cleanly.\n     1803    \n     1804    Local host: borax38\n     1805    PID:        53191\n     1806    ------------------------------------------------------------------\n             --------\n  >> 1807    FAILED: Command exited with status 255:\n     1808        './ring_oshmemfh'\n     1809    ------------------------------------------------------------------\n             --------\n     1810    The application appears to have been direct launched using \"srun\",\n     1811    but OMPI was not built with SLURM support. This usually happens\n     1812    when OMPI was not configured --with-slurm and we weren't able\n     1813    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:53268] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1880    ------------------------------------------------------------------\n             --------\n     1881    *** An error occurred in MPI_Init\n     1882    *** on a NULL communicator\n     1883    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1884    ***    and potentially your MPI job)\n     1885    [borax38:53268] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1886    FAILED: Command exited with status 1:\n     1887        './ring_usempi'\n     1888    ------------------------------------------------------------------\n             --------\n     1889    The application appears to have been direct launched using \"srun\",\n     1890    but OMPI was not built with SLURM support. This usually happens\n     1891    when OMPI was not configured --with-slurm and we weren't able\n     1892    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:53344] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1953    ------------------------------------------------------------------\n             --------\n     1954    *** An error occurred in MPI_Init\n     1955    *** on a NULL communicator\n     1956    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1957    ***    and potentially your MPI job)\n     1958    [borax38:53344] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1959    FAILED: Command exited with status 1:\n     1960        './ring_usempif08'\n     1961    ------------------------------------------------------------------\n             --------\n     1962    The application appears to have been direct launched using \"srun\",\n     1963    but OMPI was not built with SLURM support. This usually happens\n     1964    when OMPI was not configured --with-slurm and we weren't able\n     1965    to discover a SLURM installation in the usual places.\n\n\n\n/p/vast1/build/spack/lib/spack/spack/build_environment.py:1061, in _setup_pkg_and_run:\n       1058        tb_string = traceback.format_exc()\n       1059\n       1060        # build up some context from the offending package so we can\n  >>   1061        # show that, too.\n       1062        package_context = get_package_context(tb)\n       1063\n       1064        logfile = None\n\nSee test log for details:\n  /g/g0/sochat1/.spack/test/wjkcucjdda4pthihxgsnrwpoy4cqlmqe/openmpi-3.1.4-xf5lntw-test-out.txt\n\n==> Error: 1 test(s) in the suite failed.\n\n======================== 1 failed, 0 passed of 1 specs =========================\n",
                    "return_code": 1,
                    "prediction": false,
                    "command": "/p/vast1/build/spack/bin/spack test run /xf5lnt"
                }
            ],
            "symbolator": [
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                }
            ],
            "libabigail": [
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                }
            ]
        },
        "libs": {
            "spliced": [
                {
                    "dep": "pkgconf@1.3.10%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.10-jtx7kucfxrepgtxdyx5nrgfammidiou6/lib/libpkgconf.so.2.0.0"
                    ]
                }
            ],
            "original": [
                {
                    "dep": "pkgconf@1.8.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ]
        },
        "experiment": "experiment",
        "result": "splice-success",
        "success": true,
        "splice": "pkgconf@1.3.10",
        "package": "openmpi@3.1.4"
    },
    {
        "binaries": {
            "spliced": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper"
            ],
            "original": [
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orterun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-dvm",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/oshmem_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/prun",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-server",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/opal_wrapper",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-top",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-clean",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/ompi_info",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orte-ps",
                "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-6ti3h5pyepzrzfgylibpnifcxkjn3hda/bin/orted"
            ]
        },
        "predictions": {
            "spack-test": [
                {
                    "message": "==> Spack test wjkcucjdda4pthihxgsnrwpoy4cqlmqe\n==> Testing package openmpi-3.1.4-xf5lntw\n==> Error: TestFailure: 18 tests failed.\n\n\nCommand exited with status 1:\n    './hello_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:53726] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     592    -------------------------------------------------------------------\n            -------\n     593    *** An error occurred in MPI_Init\n     594    *** on a NULL communicator\n     595    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     596    ***    and potentially your MPI job)\n     597    [borax38:53726] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 598    FAILED: Command exited with status 1:\n     599        './hello_c'\n     600    -------------------------------------------------------------------\n            -------\n     601    The application appears to have been direct launched using \"srun\",\n     602    but OMPI was not built with SLURM support. This usually happens\n     603    when OMPI was not configured --with-slurm and we weren't able\n     604    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:53727] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     665    -------------------------------------------------------------------\n            -------\n     666    *** An error occurred in MPI_Init\n     667    *** on a NULL communicator\n     668    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     669    ***    and potentially your MPI job)\n     670    [borax38:53727] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 671    FAILED: Command exited with status 1:\n     672        './hello_mpifh'\n     673    -------------------------------------------------------------------\n            -------\n     674    The application appears to have been direct launched using \"srun\",\n     675    but OMPI was not built with SLURM support. This usually happens\n     676    when OMPI was not configured --with-slurm and we weren't able\n     677    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 53803, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        53803\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     744    of its peer processes in the job will be killed properly.  You shou\n            ld\n     745    double check that everything has shut down cleanly.\n     746    \n     747    Local host: borax38\n     748    PID:        53803\n     749    -------------------------------------------------------------------\n            -------\n  >> 750    FAILED: Command exited with status 255:\n     751        './hello_oshmem'\n     752    -------------------------------------------------------------------\n            -------\n     753    The application appears to have been direct launched using \"srun\",\n     754    but OMPI was not built with SLURM support. This usually happens\n     755    when OMPI was not configured --with-slurm and we weren't able\n     756    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemcxx'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 53880, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        53880\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     829    of its peer processes in the job will be killed properly.  You shou\n            ld\n     830    double check that everything has shut down cleanly.\n     831    \n     832    Local host: borax38\n     833    PID:        53880\n     834    -------------------------------------------------------------------\n            -------\n  >> 835    FAILED: Command exited with status 255:\n     836        './hello_oshmemcxx'\n     837    -------------------------------------------------------------------\n            -------\n     838    The application appears to have been direct launched using \"srun\",\n     839    but OMPI was not built with SLURM support. This usually happens\n     840    when OMPI was not configured --with-slurm and we weren't able\n     841    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './hello_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 53956, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        53956\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     914    of its peer processes in the job will be killed properly.  You shou\n            ld\n     915    double check that everything has shut down cleanly.\n     916    \n     917    Local host: borax38\n     918    PID:        53956\n     919    -------------------------------------------------------------------\n            -------\n  >> 920    FAILED: Command exited with status 255:\n     921        './hello_oshmemfh'\n     922    -------------------------------------------------------------------\n            -------\n     923    The application appears to have been direct launched using \"srun\",\n     924    but OMPI was not built with SLURM support. This usually happens\n     925    when OMPI was not configured --with-slurm and we weren't able\n     926    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:54032] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     993    -------------------------------------------------------------------\n            -------\n     994    *** An error occurred in MPI_Init\n     995    *** on a NULL communicator\n     996    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now a\n            bort,\n     997    ***    and potentially your MPI job)\n     998    [borax38:54032] Local abort before MPI_INIT completed completed suc\n            cessfully, but am not able to aggregate error messages, and not abl\n            e to guarantee that all other processes were killed!\n  >> 999    FAILED: Command exited with status 1:\n     1000       './hello_usempi'\n     1001   -------------------------------------------------------------------\n            -------\n     1002   The application appears to have been direct launched using \"srun\",\n     1003   but OMPI was not built with SLURM support. This usually happens\n     1004   when OMPI was not configured --with-slurm and we weren't able\n     1005   to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './hello_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:54109] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1066    ------------------------------------------------------------------\n             --------\n     1067    *** An error occurred in MPI_Init\n     1068    *** on a NULL communicator\n     1069    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1070    ***    and potentially your MPI job)\n     1071    [borax38:54109] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1072    FAILED: Command exited with status 1:\n     1073        './hello_usempif08'\n     1074    ------------------------------------------------------------------\n             --------\n     1075    The application appears to have been direct launched using \"srun\",\n     1076    but OMPI was not built with SLURM support. This usually happens\n     1077    when OMPI was not configured --with-slurm and we weren't able\n     1078    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_circular_shift'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 54186, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        54186\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1145    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1146    double check that everything has shut down cleanly.\n     1147    \n     1148    Local host: borax38\n     1149    PID:        54186\n     1150    ------------------------------------------------------------------\n             --------\n  >> 1151    FAILED: Command exited with status 255:\n     1152        './oshmem_circular_shift'\n     1153    ------------------------------------------------------------------\n             --------\n     1154    The application appears to have been direct launched using \"srun\",\n     1155    but OMPI was not built with SLURM support. This usually happens\n     1156    when OMPI was not configured --with-slurm and we weren't able\n     1157    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_max_reduction'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 54262, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        54262\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1230    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1231    double check that everything has shut down cleanly.\n     1232    \n     1233    Local host: borax38\n     1234    PID:        54262\n     1235    ------------------------------------------------------------------\n             --------\n  >> 1236    FAILED: Command exited with status 255:\n     1237        './oshmem_max_reduction'\n     1238    ------------------------------------------------------------------\n             --------\n     1239    The application appears to have been direct launched using \"srun\",\n     1240    but OMPI was not built with SLURM support. This usually happens\n     1241    when OMPI was not configured --with-slurm and we weren't able\n     1242    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_shmalloc'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 54338, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        54338\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1315    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1316    double check that everything has shut down cleanly.\n     1317    \n     1318    Local host: borax38\n     1319    PID:        54338\n     1320    ------------------------------------------------------------------\n             --------\n  >> 1321    FAILED: Command exited with status 255:\n     1322        './oshmem_shmalloc'\n     1323    ------------------------------------------------------------------\n             --------\n     1324    The application appears to have been direct launched using \"srun\",\n     1325    but OMPI was not built with SLURM support. This usually happens\n     1326    when OMPI was not configured --with-slurm and we weren't able\n     1327    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_strided_puts'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 54424, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        54424\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1400    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1401    double check that everything has shut down cleanly.\n     1402    \n     1403    Local host: borax38\n     1404    PID:        54424\n     1405    ------------------------------------------------------------------\n             --------\n  >> 1406    FAILED: Command exited with status 255:\n     1407        './oshmem_strided_puts'\n     1408    ------------------------------------------------------------------\n             --------\n     1409    The application appears to have been direct launched using \"srun\",\n     1410    but OMPI was not built with SLURM support. This usually happens\n     1411    when OMPI was not configured --with-slurm and we weren't able\n     1412    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './oshmem_symmetric_data'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 54502, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        54502\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1485    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1486    double check that everything has shut down cleanly.\n     1487    \n     1488    Local host: borax38\n     1489    PID:        54502\n     1490    ------------------------------------------------------------------\n             --------\n  >> 1491    FAILED: Command exited with status 255:\n     1492        './oshmem_symmetric_data'\n     1493    ------------------------------------------------------------------\n             --------\n     1494    The application appears to have been direct launched using \"srun\",\n     1495    but OMPI was not built with SLURM support. This usually happens\n     1496    when OMPI was not configured --with-slurm and we weren't able\n     1497    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_c'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:54578] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1564    ------------------------------------------------------------------\n             --------\n     1565    *** An error occurred in MPI_Init\n     1566    *** on a NULL communicator\n     1567    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1568    ***    and potentially your MPI job)\n     1569    [borax38:54578] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1570    FAILED: Command exited with status 1:\n     1571        './ring_c'\n     1572    ------------------------------------------------------------------\n             --------\n     1573    The application appears to have been direct launched using \"srun\",\n     1574    but OMPI was not built with SLURM support. This usually happens\n     1575    when OMPI was not configured --with-slurm and we weren't able\n     1576    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_mpifh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:54654] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1637    ------------------------------------------------------------------\n             --------\n     1638    *** An error occurred in MPI_Init\n     1639    *** on a NULL communicator\n     1640    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1641    ***    and potentially your MPI job)\n     1642    [borax38:54654] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1643    FAILED: Command exited with status 1:\n     1644        './ring_mpifh'\n     1645    ------------------------------------------------------------------\n             --------\n     1646    The application appears to have been direct launched using \"srun\",\n     1647    but OMPI was not built with SLURM support. This usually happens\n     1648    when OMPI was not configured --with-slurm and we weren't able\n     1649    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmem'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 54731, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        54731\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1716    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1717    double check that everything has shut down cleanly.\n     1718    \n     1719    Local host: borax38\n     1720    PID:        54731\n     1721    ------------------------------------------------------------------\n             --------\n  >> 1722    FAILED: Command exited with status 255:\n     1723        './ring_oshmem'\n     1724    ------------------------------------------------------------------\n             --------\n     1725    The application appears to have been direct launched using \"srun\",\n     1726    but OMPI was not built with SLURM support. This usually happens\n     1727    when OMPI was not configured --with-slurm and we weren't able\n     1728    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 255:\n    './ring_oshmemfh'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nSHMEM_ABORT was invoked on rank -1 (pid 54811, host=borax38) with errorcode -1.\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nA SHMEM process is aborting at a time when it cannot guarantee that all\nof its peer processes in the job will be killed properly.  You should\ndouble check that everything has shut down cleanly.\n\nLocal host: borax38\nPID:        54811\n--------------------------------------------------------------------------\n\n\n\n1 error found in test log:\n     1801    of its peer processes in the job will be killed properly.  You sho\n             uld\n     1802    double check that everything has shut down cleanly.\n     1803    \n     1804    Local host: borax38\n     1805    PID:        54811\n     1806    ------------------------------------------------------------------\n             --------\n  >> 1807    FAILED: Command exited with status 255:\n     1808        './ring_oshmemfh'\n     1809    ------------------------------------------------------------------\n             --------\n     1810    The application appears to have been direct launched using \"srun\",\n     1811    but OMPI was not built with SLURM support. This usually happens\n     1812    when OMPI was not configured --with-slurm and we weren't able\n     1813    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempi'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:54888] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1880    ------------------------------------------------------------------\n             --------\n     1881    *** An error occurred in MPI_Init\n     1882    *** on a NULL communicator\n     1883    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1884    ***    and potentially your MPI job)\n     1885    [borax38:54888] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1886    FAILED: Command exited with status 1:\n     1887        './ring_usempi'\n     1888    ------------------------------------------------------------------\n             --------\n     1889    The application appears to have been direct launched using \"srun\",\n     1890    but OMPI was not built with SLURM support. This usually happens\n     1891    when OMPI was not configured --with-slurm and we weren't able\n     1892    to discover a SLURM installation in the usual places.\n\n\n\nCommand exited with status 1:\n    './ring_usempif08'\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM support. This usually happens\nwhen OMPI was not configured --with-slurm and we weren't able\nto discover a SLURM installation in the usual places.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)\n[borax38:54964] Local abort before MPI_INIT completed completed successfully, but am not able to aggregate error messages, and not able to guarantee that all other processes were killed!\n\n\n\n1 error found in test log:\n     1953    ------------------------------------------------------------------\n             --------\n     1954    *** An error occurred in MPI_Init\n     1955    *** on a NULL communicator\n     1956    *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now \n             abort,\n     1957    ***    and potentially your MPI job)\n     1958    [borax38:54964] Local abort before MPI_INIT completed completed su\n             ccessfully, but am not able to aggregate error messages, and not a\n             ble to guarantee that all other processes were killed!\n  >> 1959    FAILED: Command exited with status 1:\n     1960        './ring_usempif08'\n     1961    ------------------------------------------------------------------\n             --------\n     1962    The application appears to have been direct launched using \"srun\",\n     1963    but OMPI was not built with SLURM support. This usually happens\n     1964    when OMPI was not configured --with-slurm and we weren't able\n     1965    to discover a SLURM installation in the usual places.\n\n\n\n/p/vast1/build/spack/lib/spack/spack/build_environment.py:1061, in _setup_pkg_and_run:\n       1058        tb_string = traceback.format_exc()\n       1059\n       1060        # build up some context from the offending package so we can\n  >>   1061        # show that, too.\n       1062        package_context = get_package_context(tb)\n       1063\n       1064        logfile = None\n\nSee test log for details:\n  /g/g0/sochat1/.spack/test/wjkcucjdda4pthihxgsnrwpoy4cqlmqe/openmpi-3.1.4-xf5lntw-test-out.txt\n\n==> Error: 1 test(s) in the suite failed.\n\n======================== 1 failed, 0 passed of 1 specs =========================\n",
                    "return_code": 1,
                    "prediction": false,
                    "command": "/p/vast1/build/spack/bin/spack test run /xf5lnt"
                }
            ],
            "symbolator": [
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                },
                {
                    "missing": [],
                    "selected": [
                        [
                            "libc.so.6",
                            "libc.so.6"
                        ]
                    ],
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": true
                }
            ],
            "libabigail": [
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/oshmem_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-server",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-clean",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-dvm",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/prun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orted",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-ps",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orterun",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/ompi_info",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/orte-top",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                },
                {
                    "message": "ELF file '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper' is not ABI compatible with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0' due to differences with '/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0' below:\nELF SONAME changed\nFunctions changes summary: 0 Removed, 0 Changed, 0 Added function\nVariables changes summary: 0 Removed, 0 Changed, 0 Added variable\n\nSONAME changed from 'libpkgconf.so.3' to 'libpkgconf.so.2'\n\n",
                    "return_code": 12,
                    "binary": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/openmpi-3.1.4-xf5lntwxmagso3ylq5z2etoqrgo7rcux/bin/opal_wrapper",
                    "splice_type": "same_lib",
                    "lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "original_lib": "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0",
                    "prediction": false
                }
            ]
        },
        "libs": {
            "spliced": [
                {
                    "dep": "pkgconf@1.3.8%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.3.8-umai66ztule5uqv26ykigysqrhsgoiyd/lib/libpkgconf.so.2.0.0"
                    ]
                }
            ],
            "original": [
                {
                    "dep": "pkgconf@1.8.0%gcc@10.2.1 arch=linux-rhel7-broadwell",
                    "paths": [
                        "/p/vast1/build/spack/opt/spack/linux-rhel7-broadwell/gcc-10.2.1/pkgconf-1.8.0-gp2xp6yplvubgic6eunbnl7emqdq3dye/lib/libpkgconf.so.3.0.0"
                    ]
                }
            ]
        },
        "experiment": "experiment",
        "result": "splice-success",
        "success": true,
        "splice": "pkgconf@1.3.8",
        "package": "openmpi@3.1.4"
    }
]